llm: 
  "api": "sambastudio"  # set either sambastudio or sambaverse
  # "api": "sambaverse"  # set either sambastudio or sambaverse
  # "api": "fastapi"
  "temperature": 0
  "max_tokens_to_generate": 1024
  "sambaverse_model_name": "Meta/Meta-Llama-3-70B-Instruct"
  "coe": True #set as true if using Sambastudio CoE endpoint
  "select_expert": "Meta-Llama-3-70B-Instruct" #set if using Sambaverse or SambaStudio CoE llm expert
  # "select_expert": "llama3-405b"
  # "select_expert": "llama3-70b"
  "do_sample": False
  "type": "sambastudio" # set either sambastudio or cpu
  "batch_size": 1 #set depending of your endpoint configuration (1 if CoE embedding expert)

tools:
  rag:
    llm:
      "api": "sambastudio"  # set either sambastudio or sambaverse
      "temperature": 0.01 
      "max_tokens_to_generate": 1024
      "sambaverse_model_name": "Meta/Meta-Llama-3-8B-Instruct"
      "coe": True #set as true if using Sambastudio CoE endpoint
      "select_expert": "Meta-Llama-3-8B-Instruct" #set if using Sambaverse or SambaStudio CoE llm expert
    embedding_model: 
      "type": "sambastudio" # set either sambastudio or cpu
      "batch_size": 1 #set depending of your endpoint configuration (1 if CoE embedding expert)
      "coe": True #set true if using Sambastudio embeddings in a CoE endpoint 
      "select_expert": "e5-mistral-7b-instruct" #set if using SambaStudio CoE embedding expert
    vector_db:
      "path": "data/my-vector-db" # path to your previously created chroma vdb
    retrieval:
      "k_retrieved_documents": 5
      "score_threshold": 0.7

  query_db:
    llm: 
      "api": "sambastudio"  # set either sambastudio or sambaverse
      "temperature": 0.01 
      "max_tokens_to_generate": 1024
      "sambaverse_model_name": "Meta/Meta-Llama-3-8B-Instruct"
      "coe": True #set as true if using Sambastudio CoE endpoint
      "select_expert": "Meta-Llama-3-8B-Instruct" #set if using Sambaverse or SambaStudio CoE llm expert
    db:
      "path": "data/chinook.db" 