{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.sambanova_endpoint import SambaNovaEndpoint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(repo_dir,'.env'))\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_binary = image_file.read()\n",
    "        base64_image = base64.b64encode(image_binary).decode()\n",
    "        return base64_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sambastudio call\n",
    "def llava_call(prompt, image_path):\n",
    "    image=image_to_base64(image_path)\n",
    "    endpoint_url = f\"{os.environ.get('BASE_URL')}/api/predict/generic/{os.environ.get('PROJECT_ID')}/{os.environ.get('ENDPOINT_ID')}\"\n",
    "    endpoint_key = os.environ.get('API_KEY')\n",
    "    # Define the data payload\n",
    "    data = {\n",
    "        \"instances\": [{\n",
    "            \"prompt\": prompt,\n",
    "            \"image_content\": f\"{image}\"\n",
    "        }],\n",
    "        \"params\": {\n",
    "            \"do_sample\": {\"type\": \"bool\", \"value\": \"false\"},\n",
    "            \"max_tokens_to_generate\": {\"type\": \"int\", \"value\": \"512\"},\n",
    "            \"temperature\": {\"type\": \"float\", \"value\": \"1\"},\n",
    "            \"top_k\": {\"type\": \"int\", \"value\": \"50\"},\n",
    "            \"top_logprobs\": {\"type\": \"int\", \"value\": \"0\"},\n",
    "            \"top_p\": {\"type\": \"float\", \"value\": \"1\"}\n",
    "        }\n",
    "    }\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"key\": endpoint_key\n",
    "    }\n",
    "    response = requests.post(endpoint_url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()[\"predictions\"][0]['completion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Llava Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At 4 pm, you could find approximately 10 birds on the tree.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans question. USER: <image>\\nhow many birds could you find at 4pm?. ASSISTANT:\"\n",
    "image_path = os.path.join(kit_dir,\"data\",\"sample_docs\",\"sample.png\")\n",
    "llava_call(prompt, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Llava call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image displays a graph showing the number of birds on a tree at different times of the day. The graph is a combination of a bar plot and a scatter plot, with the bar plot showing the number of birds at various times of the day, and the scatter plot showing the number of birds on a tree at a specific time.\\n\\nThe graph is divided into two main sections. The first section, which is the bar plot, shows the number of birds on a tree at different times of the day, with the bars extending from 10 am to 11 pm. The second section, which is the scatter plot, shows the number of birds on a tree at a specific time, with the x-axis representing the time and the y-axis representing the number of birds.\\n\\nThe graph is labeled with the time of the day, and the number of birds is represented by the number of orange dots on the graph. The dots are scattered throughout the graph, with some appearing closer to the bars and others appearing closer to the scatter plot.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans question. USER: <image>\\nDescribe the image in detail. Be specific about graphs, such as bar plots, scatter plots, or others. ASSISTANT:\"\n",
    "llava_call(prompt, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Path to save images\n",
    "file_path=os.path.join(kit_dir, \"data\", \"sample_docs\", \"sample.pdf\")\n",
    "output_path=os.path.splitext(file_path)[0]\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=file_path,\n",
    "    extract_images_in_pdf=True,\n",
    "    strategy='hi_res',\n",
    "     hi_res_model_name=\"yolox\",\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=1000,\n",
    "    new_after_n_chars=800,\n",
    "    combine_text_under_n_chars=500,\n",
    "    extract_image_block_output_dir=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m ELEMENT 0\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 1, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 4 2 0 2\n",
      "\n",
      "r p A 8 ] L C . s c [ 1 v 9 2 8 5 0 . 4 0 4 2 : v i X r a\n",
      "\n",
      "SambaLingo: Teaching Large Language Models New Languages\n",
      "\n",
      "Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu & Urmish Thakker SambaNova Systems zoltan.csaki@sambanovasystems.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 1\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 1, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best prac- tices and methodology have not been covered. In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages. Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. We scale these experi- ments across 9 languages and 2 parameter scales (7B and 70B). We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing lan- guage experts, outperforming all prior published baselines. Additionally, all evaluation code1 and checkpoints2 are made public to\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 2\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 1, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: facilitate future research.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 3\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 1, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 1 Introduction\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 4\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 1, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: New state of the art large language models are being released at a breakneck speed, yet their training data, tokenizer, and evaluations remain primarily centered around a few popular languages such as English, Chinese, French and Arabic. In principle, the way to create large language models for specific languages is to pre-train models from scratch (Sengupta et al., 2023; Zhang et al., 2020). However, it is difficult to obtain a large amount of compute resources and a vast quantity of data in diverse languages. Researchers have tackled this problem by training monolithic multi-lingual models that cover a wide range of languages (Workshop et al., 2023; Lin et al., 2022; Shliazhko et al., 2023; Xue et al., 2021). These models can still struggle to achieve uniformly good results across all languages due to various factors such as the curse of multilinguality (Chang et al., 2023; Conneau et al., 2020) and the scarcity of pre-training data in many languages (Chung et al., 2023).\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 5\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 1, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Recently, adapting English centric models to new languages has gained prominence (Blevins et al., 2024; Yong et al., 2023; Ebrahimi & Kann, 2021; Pires et al., 2023; Pipatanakul et al., 2023; Lin et al., 2024). The resulting models can outperform large multilingual models and even language specific models pre-trained from scratch. Adaptation requires various design choices around the tokenizer, data, alignment and evaluation strategies. This paper aims to provide a comprehensive study to help inform these decisions, outlining a clear protocol to adapt a pre-trained model to a new language. We show that our methodology works by training models across 9 languages and 2 parameter scales (7B and 70B) and comparing them against publicly available models. Figure 1 and 2 show that our methodology can\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 6\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 1, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 1Fork of lm-evaluation-harness (Gao et al., 2023) with new multilingual benchmarks: https: //github.com/sambanova/lm-evaluation-harness\n",
      "\n",
      "2All SambaLingo Checkpoints: https://huggingface.co/collections/sambanovasystems/ sambalingo-65e25770f2037c85ad35ca77\n",
      "\n",
      "1\n",
      "\n",
      "lead to better models than existing state of the art models in these languages.\n",
      "\n",
      "The key studies and contributions include:\n",
      "\n",
      "• Best practices for continuous pretraining in new languages scaled across 9 typologi- cally diverse languages including Arabic, Bulgarian, Hungarian, Japanese, Russian, Serbian, Slovenian, Thai, and Turkish\n",
      "\n",
      "– Expanding the vocabulary for the target language improves the tokenizer fertility (5), but does not have a siginficant impact on downstream accuracy (5.1.1)\n",
      "\n",
      "– Various embedding initialization methods have minimal impact on accuracy, but sub word averaging accelerates training loss convergence (5.1.2)\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 7\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 2, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: – The quality of the base checkpoint on English benchmarks can improve down- stream language adaptation results (5.3)\n",
      "\n",
      "A recipe for human preference alignment in any language using open source data – Aligning the adapted model requires minimal data from the target language,\n",
      "\n",
      "reducing the need of gathering expensive alignment data (5.2.1)\n",
      "\n",
      "– The choice of translated versus human written alignment data does not have a large impact on win rates (5.2.2)\n",
      "\n",
      "• Open sourcing code and checkpoints to promote future research\n",
      "\n",
      "– State of the art models adapted from Llama 2 in 9 languages and 2 parameter scales (7B, 70B)2\n",
      "\n",
      "– Integration of FLORES-200, SIB-200, EXAMS and multilingual perplexity bench- marks with lm-eval-harness1 (Gao et al., 2023)\n",
      "\n",
      "Perplexity (lower is better)\n",
      "\n",
      "2.0 : MN | Mt l thn Wit MI | In in Wt in WI Japanese Thai Hungarian Arabic Russian Turkish Bulgarian Serbian Slovenian\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 8\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 2, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: @ Llama 7b @ bloom-7b ™ xglm-7.58 lM Best open source expert M™ SambaLingo 7B ™ SambaLingo 70B\n",
      "\n",
      "Figure 1: Comparing the perplexity on hold out dataset of models trained with our methodol- ogy (our 7B and our 70B) with open source language experts and other multilingual models. Open source expert baselines: Japanese - Swallow-7b-hf (TokyoTech, 2023), Thai: typhoon-7b (Pipatanakul et al., 2023), Arabic: jais-13b (Sengupta et al., 2023), Hungarian: PULI-GPTrio (Yang et al., 2023), Russian: saiga-7b (Gusev, 2023), Bulgarian: mGPT-bulgarian(Shliazhko et al., 2023). We could not find Serbian, Slovenian and Turkish languages models with low enough perplexity that would fit the graph so we chose to omit them here to ensure readability.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 9\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 2, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 2 Related Work\n",
      "\n",
      "While prior work has explored adapting pre-trained LLMs to new languages, they do not extensively study the methodology to do so. None of these works explore the design choices around aligning models in new languages, for example the mixture of data in the base models language and the new language or the impact of translated data on qualitative evaluation. Pires et al. (2023) and Cui et al. (2023b) adapt Llama models to Portuguese and\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 10\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 3, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Chinese respectively, but they do not explore the impact of vocabulary extension and/or initialization. Blevins et al. (2024) explores training language experts to break the curse of multilinguality starting from a pre-trained model, but they do not explore the impact of vocabulary extension, initialization and quality of the base model. Extension of vocabulary was discussed in Zhao et al., 2024b; Tikhomirov & Chernyshev, 2023, however they do not explore token embedding initialization strategies or impact of quality of base model. Lin et al. (2024) studies simultaneous language adaptation to 500 languages. Nevertheless, they also do not answer questions around alignment or token initialization strategy. Ye et al. (2023) studies language adaptation of a wide variety of English-centric and multilingual models, however they only focus on fine-tuning XNLI tasks.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 11\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 3, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: There has been a significant body of work around open-source multi-lingual models (Work- shop et al., 2023; Lin et al., 2022; Shliazhko et al., 2023). Our work differs from the aforemen- tioned studies as we solely focus on adapting pre-trained LLMs to new languages and not on pre-training from scratch. Notably, these multilingual open-source models tend to be pretrained on significantly fewer tokens than the base models we adapt from. As the models in this work tend to outperform these multilingual models, this presents a promising path forward for obtaining the state of the art in new languages.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 12\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 3, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 3 Adaptation Methodology\n",
      "\n",
      "We present our methodology to adapt large languages models to a new language, with state of the art results in 9 target languages: Arabic, Thai, Turkish, Japanese, Hungarian, Russian, Bulgarian, Serbian and Slovenian. See Section 4 for evaluation results on the final checkpoints produced by this methodology, and Section 5 for ablations justifying our methods.\n",
      "\n",
      "We use the term initial language to describe the original language that the base model was trained on (in this case, English) and the term target language as the new language this model is being adapted to.\n",
      "\n",
      "3.1 Selecting a Base Model\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 13\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 3, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Our methodology starts with an existing base checkpoint instead of pre-training from scratch. Previous work has shown that starting from an existing checkpoint leads to faster training convergence, better downstream evaluation accuracy and lower compute/data requirements (Pires et al., 2023; Lin et al., 2024; Csaki et al., 2023). Section 5.3 demonstrates that it is important to select a starting checkpoint with the best results for the initial language, as that will improve the downstream results for the target language. Based on these observations, we chose Llama2 7B as our base model to adapt to target languages, the best open source model available at the time of the experiments.\n",
      "\n",
      "We additionally scale this methodology to Llama 2 70B. Given compute restrictions, we only do this for 3 languages - Arabic, Thai and Hungarian. See Section 4.2 for in-depth comparisons of our 7B and 70B models.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 14\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 3, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 3.2 Extending Model Vocabulary\n",
      "\n",
      "Llama 2 (Touvron et al., 2023) was trained predominantly on English text, and has poor tokenizer efficiency for other languages (see Section 5.1). To address this inefficiency, we chose to extend the vocabulary of the Llama 2 tokenizer by adding non overlapping tokens from the target language and initializing them using sub-word embeddings from the original tokenizer. See Section 5.1 for experiments that justify our approach.\n",
      "\n",
      "3.3 Continual Pre-training\n",
      "\n",
      "Our pretraining data consists of a 1:3 mixture of English and target language web data biased towards the target language. Pretraining data for all languages, including English, is sourced from CulturaX (Nguyen et al., 2023). These decisions are grounded in results from\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 15\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 4, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: previous works: Zhao et al. (2024b); Csaki et al. (2023) show that mixing in data from the base model domain helps downstream accuracy and training stability, Gupta et al. (2023) find that including a higher proportion of data from the target distribution helps improve the convergence in the target distribution, Almazrouei et al. (2023) showed the importance of cleaned web data. Additionally, hyperparameters used for training can be found in Appendix A.1.\n",
      "\n",
      "3.4 Aligning To Human Preferences In Other Languages\n",
      "\n",
      "To train a chat-aligned version of the model, we follow the two-stage approach from Tunstall et al. (2023) - supervised finetuning (SFT) followed by direct preference optimization (DPO). More details about hyperparameters for each of these phases used can be found in Appendix A.1.\n",
      "\n",
      "• For SFT, we use ultrachat-200k (Tunstall et al., 2023), in a 1:1 ratio with a Google translated version of ultrachat-200k.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 16\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 4, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: • For alignment, we use the ultrafeedback (Cui et al., 2023a) and cai-conversation-harmless dataset (Huang et al., 2024). We mix these datasets with a 10:1 ratio of English to machine translated data. Section 5.2.1 shows that this ratio of data performs almost as well as other ratios and section 5.2.2 shows that machine-translated data can perform as well as human written data.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 17\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 4, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 4 Evaluation\n",
      "\n",
      "4.1 Quantitative Evaluation\n",
      "\n",
      "We use a wide variety of benchmarks to quantitatively evaluate the performance of our models and compare them to prior work. See Table 1 for the full list of quantitative benchmarks. In summary, we evaluate language modeling with perplexity on a holdout set of CulturaX (Nguyen et al., 2023), translation with CHRF (Popovi´c, 2015) on FLORES-200 (Goyal et al., 2021; Zhu et al., 2023), text classification accuracy on SIB-200 (Adelani et al., 2024; Lin et al., 2024), open-book question answering on BELEBELE (Bandarkar et al., 2023), closed-book question answering on EXAMS (Hardalov et al., 2020), and a variety of natural language understanding benchmarks (XNLI (Conneau et al., 2018), XStoryCloze (Lin et al., 2022), XCOPA (Ponti et al., 2020), XWinograd (Emelin & Sennrich, 2021), and PAWS-X (Yang et al., 2019)).\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 18\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.9222691655158997, 'coordinates': {'points': ((342.4435119628906, 1407.0518798828125), (342.4435119628906, 1787.8878173828125), (1358.035400390625, 1787.8878173828125), (1358.035400390625, 1407.0518798828125)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th>Datasets</th><th>LESS Category</th><th>NEen Few-Shot</th><th>NEE! | Languages</th><th>Metric</th></thead><tr><td>mc4 , Wikipedia |</td><td>Perplexity</td><td>-</td><td></td><td>Perplexity</td></tr><tr><td>FLORES-200</td><td>Translation</td><td>8</td><td>200</td><td>CHRF</td></tr><tr><td>SIB-200</td><td>Text Classification</td><td>3</td><td>200</td><td>Accuracy</td></tr><tr><td>BELEBELE</td><td>Question Answering</td><td>| 3</td><td>122</td><td>Accuracy</td></tr><tr><td>Exams</td><td>Knowledge</td><td>3</td><td>11</td><td>Accuracy</td></tr><tr><td>XNLI XStoryCloze XCOPA XWinograd PAWS-X</td><td>De mBuage 8</td><td>0</td><td>25+</td><td>Accuracy</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Datasets mc4 , Wikipedia FLORES-200 SIB-200 BELEBELE Exams XNLI XStoryCloze XCOPA XWinograd PAWS-X Task Category Perplexity Translation Text Classification Question Answering Knowledge Natural Language Understanding Num Few-Shot - 8 3 3 3 0 Number Of Languages 323 200 200 122 11 25+ Metric Perplexity CHRF Accuracy Accuracy Accuracy Accuracy\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 19\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 4, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 1: Multi-lingual evaluation suite\n",
      "\n",
      "All quantitative evaluations are performed on our adapted models after continuous pre- training, but before the alignment stage. We note that not all of our target languages are covered across all benchmarks. However, each language we examine has evaluations in at least 4 of these benchmarks. We ensured that perplexity measurements were done on a held out set in the target language.\n",
      "\n",
      "4\n",
      "\n",
      "0.8000 0.6000 0.4000 0.2000 0.0000 Russian Arabic Japanese Thai Turkish Bulgarian Hungarian Serbian —‘Slovenian AYA-101 Llama2-7b Best 7B 70B\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 20\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 5, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: ‘Average Evaluation Score\n",
      "\n",
      "BH AYA-101 @ Llama2-7b @ bloom-7b l™@ xgim-7.5B_ M™ Best open source expert —™ SambaLingo 7B m™ SambaLingo 70B\n",
      "\n",
      "Figure 2: Quantitative evaluation results. The “best open source experts” are the same as ones specified in Figure 1. See Appendix A.5 for the full breakdown.\n",
      "\n",
      "4.1.1 Quantitative Results\n",
      "\n",
      "We evaluate our continuously pretrained models against the best open source models available in each target language and previous state of the art multilingual models. Figure 1 shows that our SambaLingo models have a lower perplexity across all existing baselines on a heldout set from our training data. Figure 2 shows the average evaluation score across the evaluation benchmarks introduced in Section 4.1, where we see our models outperform all other models in 7/9 languages.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 21\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 5, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 4.2 Scaling to 70B\n",
      "\n",
      "Scaling to 70B consistently leads to better results as seen in table 2. The 70B models in the table have trained on fewer tokens than the 7B models.\n",
      "\n",
      "Additionally, we evaluate compute-matched checkpoints of our 7B and 70B Llama 2 models in table 2. The compute-matched 70B checkpoints are trained for 10x fewer steps (4B tokens vs 40B tokens) and perform as well as or better than 7B variants trained over 40B tokens in every benchmark across Arabic and Hungarian.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 22\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.8819336891174316, 'coordinates': {'points': ((298.2753601074219, 1321.3704833984375), (298.2753601074219, 1422.6759033203125), (1401.8385009765625, 1422.6759033203125), (1401.8385009765625, 1321.3704833984375)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th>\"Language</th><th>Checkpoint</th><th>ppl (J)</th><th>FLORESEN-X(7)</th><th>FLORESX-EN(7)</th><th>Belebele (7)</th><th>SIB-200(7)</th><th>XNLI(T)</th><th>XStoryCloze (7)</th></thead><tr><td rowspan=\"2\">Arabic</td><td>70B</td><td>1.44</td><td>54.25</td><td>65.60</td><td>0.78</td><td>0.69</td><td>0.33</td><td>0.68</td></tr><tr><td></td><td>7B</td><td>1.44</td><td>53.67</td><td>61.66</td><td>0.29</td><td>0.26</td><td>0.34</td><td>0.65</td></tr><tr><td rowspan=\"2\">Hungarian</td><td>70B</td><td>1.57</td><td>58.81</td><td>64.03</td><td>0.82</td><td>0.64</td><td>-</td><td>-</td></tr><tr><td></td><td>7B</td><td>1.63,</td><td>52.70</td><td>58.31</td><td>0.33</td><td>0.25</td><td>-</td><td>-</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Language Checkpoint ppl (↓) 1.44 Arabic 1.44 1.57 1.63 70B 7B 70B 7B Hungarian FLORES EN→X (↑) 54.25 53.67 58.81 52.70 FLORES X→EN (↑) Belebele (↑) SIB-200 (↑) XNLI (↑) XStoryCloze (↑) 65.60 61.66 64.03 58.31 0.78 0.29 0.82 0.33 0.69 0.26 0.64 0.25 0.33 0.34 - - 0.68 0.65 - -\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 23\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 5, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 2: This table compares compute matched 7B and 70B checkpoints. We look at interme- diate checkpoint results and compare 7B models trained for 40B tokens with 70B models trained for 4B tokens.\n",
      "\n",
      "4.3 Evaluating Human Aligned Checkpoints\n",
      "\n",
      "4.3.1 GPT-4 as a Judge\n",
      "\n",
      "To test our human aligned models’ ability to generate high quality responses to real user prompts, we use GPT-4 (OpenAI et al., 2024) as a judge. This method was first introduced by Zheng et al. (2023) to evaluate English models, and then used by ¨Ust ¨un et al. (2024) as a method to evaluate multilingual models. The resulting model generations are shuffled and fit to the prompting style suggested by (Zheng et al., 2023) before being fed to GPT-4. See Appendix A.3 for the manually collected prompts and section 4.3.2 for the evaluation results.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 24\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 5, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: To verify the validity of GPT-4 as a judge, we manually examine GPT-4’s judgments on 5 examples in each language. We find that for almost all of these examples, we agree with GPT-4’s preference and that it gives logical explanations for its decisions. In appendix A.6\n",
      "\n",
      "5\n",
      "\n",
      "we include example model generations along with GPT-4’s corresponding preferences and explanations. Further work is needed to do a large scale study to see how GPT-4 preferences align with human preferences in other languages.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 25\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 6, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 4.3.2 Qualitative Results\n",
      "\n",
      "Measuring win-rate using GPT-4 as a judge only works in scenarios where a human aligned or instruction tuned model is available in a language. Given this constraint, we were only able to find relevant comparisons for Arabic, Japanese and Turkish, and do not have qualitative evaluations for our models in the other 6 languages. The results of our evalutation are shown in Figure 3. Our SambaLingo models consistently out-perform other models in the same language. For details about the native speaker-curated prompts, see Appendix A.3.\n",
      "\n",
      "(a) SambaLingo vs Aya101  (b) SambaLingo vs BloomChat-v1 \n",
      "\n",
      "(c) SambaLingo vs ELEYZA-7b-instruct\n",
      "\n",
      "(d) SambaLingo vs Jais-13b-chat\n",
      "\n",
      "Figure 3: GPT4 evaluation result\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 26\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 6, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 5 Ablations\n",
      "\n",
      "In this section, we present ablations of our design decisions in Section 3. Section 5.1 presents experiments motivating the modifications we make to the base model’s tokenizer and how we initialize its new embeddings. Section 5.2 ablates the amount of target language data and use of machine translated data in the DPO phase of our methodology. Finally, section 5.3 looks at the impact of the quality of the base model.\n",
      "\n",
      "5.1 Vocabulary Expansion\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 27\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 6, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: The Llama2 tokenizer is centered towards English. While this tokenizer can encode char- acters in any language, it will be very inefficient for non-English text. In fact, the BPE tokenizer may tokenize non-Latin characters as multiple independent bytes. One way to mitigate this problem is to extend the vocabulary of the base model by adding new tokens that represent the target language to it, and start adaptation training with this expanded vocabulary. This method also helps improve the inference efficiency in the target language. We explore different sizes for the expanded vocabulary and their impacts on fertility ´Acs (2019) in Table 3 and Figure 5. We chose to expand the vocabulary by 25,000 tokens for all languages.\n",
      "\n",
      "5.1.1 Vocabulary Expansion vs Original Tokenizer\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 28\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 6, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: To measure the impact of vocabulary expansion on accuracy, we train two models, one with an expanded vocabulary and one with original vocabulary across two languages - Hungarian and Arabic. We find that expanding the vocabulary does not have significant impact on the downstream accuracy. Nonetheless, given the benefit that the expanded\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 29\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.9181614518165588, 'coordinates': {'points': ((309.5831604003906, 226.56671142578125), (309.5831604003906, 374.870361328125), (1381.875244140625, 374.870361328125), (1381.875244140625, 226.56671142578125)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th>Added Tokens</th><th>Hungarian</th><th>Russian</th><th>‘Turkish</th><th></th><th>Bulgarian</th><th>Arabic</th><th>Japanese</th><th>Thai</th></thead><tr><td>0</td><td>2.70</td><td>2.28</td><td>3.28</td><td></td><td>2.36</td><td>4.23</td><td>2.07</td><td>4.84</td></tr><tr><td>1000</td><td>2.52</td><td>2.25</td><td>2.56</td><td></td><td>2.19</td><td>2.11</td><td>1.75</td><td>2.10</td></tr><tr><td>4000</td><td>2.14</td><td>2.05</td><td>2.20</td><td></td><td>1.92</td><td>1.67</td><td>1.23</td><td>1.50</td></tr><tr><td>25000</td><td>1.78</td><td>1.78</td><td>1.77</td><td></td><td>1.66</td><td>1.26</td><td>0.93</td><td>1.10</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Added Tokens Hungarian Russian Turkish Bulgarian Arabic 4.23 0 2.11 1000 1.67 4000 1.26 25000\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 30\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 7, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 3: Number of added tokens vs fertility (average number of tokens per ”word”)\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 31\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.8271790742874146, 'coordinates': {'points': ((293.96234130859375, 472.4879150390625), (293.96234130859375, 576.0731201171875), (1399.9244384765625, 576.0731201171875), (1399.9244384765625, 472.4879150390625)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th>Language</th><th>Tokenizer</th><th>ppl (j)</th><th>FLORESEN-X(f)</th><th>FLORES X—EN (7?)</th><th>Belebele ({)</th><th>SIB-200(7)</th><th>XNLI(f)</th><th>XStoryCloze (f°)</th></thead><tr><td rowspan=\"2\">Arabic</td><td>Original</td><td>1.50</td><td>48.27</td><td>57.35</td><td>0.27</td><td>0.27</td><td>(0.34</td><td>0.63</td></tr><tr><td></td><td>Expanded</td><td>1.46</td><td>52.66</td><td>61.05</td><td>0.32</td><td>0.35</td><td>0.34</td><td>0.64</td></tr><tr><td rowspan=\"2\">Hungarian</td><td>Original</td><td>1.61</td><td>52.70</td><td>58.31</td><td>0.33</td><td>0.26</td><td>-</td><td>-</td></tr><tr><td></td><td>Expanded</td><td>1.63</td><td>51.82</td><td>57.12</td><td>0.30</td><td>0.34</td><td>-</td><td>-</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Tokenizer ppl (↓) 1.50 Original 1.46 Expanded 1.61 1.63 Language Arabic Hungarian Original Expanded FLORES EN→X (↑) 48.27 52.66 52.70 51.82 FLORES X→EN (↑) Belebele (↑) SIB-200 (↑) XNLI (↑) XStoryCloze (↑) 57.35 61.05 58.31 57.12 0.27 0.32 0.33 0.30 0.27 0.35 0.26 0.34 0.34 0.34 - - 0.63 0.64 - -\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 32\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 7, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 4: Accuracy after training with expanded vocabulary vs original tokenizer\n",
      "\n",
      "vocabulary has for inference and sequence length utilization in the target language, we chose to expand the vocabulary of the base model.\n",
      "\n",
      "5.1.2 Initializing new token embeddings\n",
      "\n",
      "We experiment with 4 different token initialization strategies for the new tokens added to the vocabulary across 2 languages - Hungarian and Arabic. For each experiment, we train the model for 10 billion tokens and compare the loss values. Let V be the set of tokens in the original vocabulary, and E(t) the embedding vector of a token t ∈ V. The four token initialization methods we consider are as follows:\n",
      "\n",
      "• gaussian: N (0, 0.02)\n",
      "\n",
      "xavier uniform • avg all Hewitt (2021): For each new token t′, initialize E(t′) = mean({E(t)∀t ∈\n",
      "\n",
      "V})\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 33\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 7, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: • avg subwords (Liu et al., 2024; Koto et al., 2021): For each new token t′, let Lt′ = [t1, ..., tk] be the list of k tokens that t′ would have been tokenized as under the origi- nal tokenizer. Initialize the new embedding with E(t′) = mean([E(t1), ..., E(tk)]).\n",
      "\n",
      "g s 2 = £\n",
      "\n",
      "(a) Arabic  (b) Hungarian \n",
      "\n",
      "Figure 4: Training loss for different token initialization methods\n",
      "\n",
      "Figure 4 shows that after continuous pretraining for 10B tokens, all methods converge to similar loss values, with avg subwords showing faster convergence. Table 5 shows the impact on downstream benchmarks. For Arabic, avg subwords achieves marginally better scores on 5/7 benchmarks while for Hungarian the results are mixed. Given these results, we chose to initialize using avg subwords.\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 34\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.TableChunk'>\n",
      "META: {'detection_class_prob': 0.9082021713256836, 'coordinates': {'points': ((306.4409484863281, 224.68592834472656), (306.4409484863281, 388.76275634765625), (1390.186279296875, 388.76275634765625), (1390.186279296875, 224.68592834472656)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th>Language</th><th>_ Initialization Met!</th><th>ppl({)</th><th>FLORES EN-X (ft)</th><th>FLORESX-—-EN (Tt)</th><th>Belebele (7)</th><th>SIB-200(7)</th><th>XNLI({)</th><th>XStoryCloze (7</th></thead><tr><td rowspan=\"4\">Arabic</td><td>gaussian</td><td>1.50</td><td>48.48</td><td>57.31</td><td>0.34</td><td>0.25</td><td>0.34</td><td>0.61</td></tr><tr><td></td><td>xavier_uniform</td><td>1.49</td><td>50.46</td><td>58.90</td><td>0.36</td><td>0.26</td><td>0.33</td><td>0.62</td></tr><tr><td></td><td>avg_all</td><td>1.48</td><td>50.54</td><td>58.29</td><td>0.34</td><td>0.25</td><td>0.35</td><td>0.63</td></tr><tr><td></td><td>avg_subwords</td><td>1.48</td><td>50.87</td><td>59.62</td><td>0.38</td><td>0.27</td><td>0.34</td><td>0.64</td></tr><tr><td rowspan=\"4\">Hungarian</td><td></td><td>gaussian</td><td>1.65</td><td>51.42</td><td>56.92</td><td>0.32</td><td>0.50</td><td>-</td><td>-</td></tr><tr><td></td><td>xavier_uniform</td><td>1.65</td><td>49.52</td><td>55.81</td><td>0.34</td><td>0', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Language Arabic Hungarian Initialization Method ppl (↓) gaussian 1.50 xavier uniform 1.49 avg all 1.48 avg subwords 1.48 gaussian 1.65 xavier uniform 1.65 avg all 1.76 avg subwords 1.65 FLORES EN→X (↑) 48.48 50.46 50.54 50.87 51.42 49.52 51.39 50.79 FLORES X→EN (↑) Belebele (↑) SIB-200 (↑) XNLI (↑) XStoryCloze (↑) 57.31 58.90 58.29 59.62 56.92 55.81 56.86 56.77 0.34 0.36 0.34 0.38 0.32 0.34 0.34 0.33 0.25 0.26 0.25 0.27 0.50 0.42 0.45 0.30 0.34 0.33 0.35 0.34 - - - - 0.61 0.62 0.63 0.64 - - - -\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 35\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.TableChunk'>\n",
      "META: {'detection_class_prob': 0.9082021713256836, 'coordinates': {'points': ((306.4409484863281, 224.68592834472656), (306.4409484863281, 388.76275634765625), (1390.186279296875, 388.76275634765625), (1390.186279296875, 224.68592834472656)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '.42</td><td>-</td><td>-</td></tr><tr><td></td><td>avg_all</td><td>1.76</td><td>51.39</td><td>56.86</td><td>0.34</td><td>0.45</td><td>-</td><td>-</td></tr><tr><td>avg_subwords</td><td>1.65</td><td>50.79</td><td>56.77</td><td>0.33</td><td>0.30</td><td>-</td><td>-</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'is_continuation': True, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: \n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 36\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 5: Multilingual evaluations across token embedding initialization methods\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 37\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.9143213629722595, 'coordinates': {'points': ((306.1767578125, 495.0862731933594), (306.1767578125, 623.7979125976562), (1413.8929443359375, 623.7979125976562), (1413.8929443359375, 495.0862731933594)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th>Language</th><th>Initial/Target data ratio</th><th>100:1</th><th>10:1</th><th>10:3</th><th>1:1</th></thead><tr><td>Arabic</td><td></td><td>39.29% |</td><td>45.18% |</td><td>45.78% |</td><td>50.00%</td></tr><tr><td>Hungarian</td><td></td><td>30.39% |</td><td>35.00% |</td><td>34.62% |</td><td>50.00%</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Language Arabic Hungarian Initial/Target data ratio 100:1 10:1 10:3 1:1 39.29% 45.18% 45.78% 50.00% 30.39% 35.00% 34.62% 50.00%\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 38\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 6: DPO data mixture result (win-rate compared with 1:1 data mixture)\n",
      "\n",
      "5.2 Direct Preference Optimization\n",
      "\n",
      "5.2.1 DPO Data Mixture\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 39\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: There is a lack of supervised finetuning and human alignment data across different lan- guages. Collecting such data can be difficult and expensive. Given that the models obtained from our methodology are bilingual, we explore the question of how much of the human alignment data can be English and how much of it has to be from the target language. We run DPO on data mixtures of the English/Target language data ratio across 100:1, 10:1, 10:3 and 1:1, and observe the resulting win-rate in pairwise comparisons with other models. We run these experiments on two languages: Hungarian and Arabic, with results in Table 6. We show that a 10:1 data ratio can perform almost as well as 1:1 data ratio for Hungarian. For Arabic, even a 10:3 data ratio still falls behind the performance of 1:1. One hypothesis is that Hungarian is more linguistically similar to English than Hungarian, but further research is needed to understand how the language impacts optimal alignment data mixture ratio.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 40\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 5.2.2 Impact of Translated Human Preference Data\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 41\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Results in Table 6 are based on translated data from the target language. ¨Ust ¨un et al. (2024) emphasized the importance of human written prompt completion pairs and claim that translated data does not perform as well. However, their work does not start with a high quality pretrained base model, nor do they use DPO. In order to understand whether machine translated data is a viable option for human alignment, we explore the impact of alignment using both approaches. We do not ablate the use of machine-translated data in the SFT portion due to a lack of human-written data, which we leave to future work. We use Google translated ultrafeedback-200k data for one run and human-written data from Open Assistant Conversations (OASST1) (K ¨opf et al., 2023) for the other. We run this study on Russian, as it is has the most human written data from OASST1 (K ¨opf et al., 2023). The model trained using translated data attains a 50.47% win rate compared to the model trained with OASST1. This\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 42\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: comparison does not control for the diversity and quality of the question answer pairs, so this comparison is not meant to illustrate that translated data is as good or better than native data, but rather to show that human written data is not a silver bullet required to obtain good quality aligned models in other languages.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 43\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.8826079368591309, 'coordinates': {'points': ((301.2372741699219, 1856.4595947265625), (301.2372741699219, 1935.42236328125), (1396.3116455078125, 1935.42236328125), (1396.3116455078125, 1856.4595947265625)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th></th><th>HellaSwag(T)</th><th></th><th>ARC- KD</th><th>ARC-C(f)</th><th>PiQA(ft)</th><th>Winogrande(t)</th></thead><tr><td>GPT-13B</td><td>0.60</td><td>OpenBookQA(T) 0.36</td><td></td><td>0.30</td><td>0.76</td><td>0.60</td></tr><tr><td>Llama-2-7B</td><td>0.76</td><td>0.57</td><td>073</td><td>0.48</td><td>0.80</td><td>0.70</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: HellaSwag(↑) OpenBookQA(↑) ARC-E(↑) ARC-C(↑) PiQA(↑) Winogrande(↑) 0.60 0.53 0.60 GPT-13B 0.70 0.73 Llama-2-7B 0.76\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 44\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 8, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 7: Performance of GPT-13B and Llama-2-7B on English NLU benchmarks\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 45\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.8944143056869507, 'coordinates': {'points': ((301.76226806640625, 226.8310089111328), (301.76226806640625, 314.39898681640625), (1398.1224365234375, 314.39898681640625), (1398.1224365234375, 226.8310089111328)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><tr><td>Base Model GPT-13B</td><td>_ppl({) 1.80</td><td>FLORES EN-—&gt;X(t) FLORES 37.94</td><td>X-en(f) 48.99</td><td>Belebele({) 0.28</td><td>SIB-200(*) 0.25</td></tr><tr><td>Llama-2-7b</td><td>1.61</td><td>53.72</td><td>58.65</td><td>0.34</td><td>0.25</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Base Model ppl(↓) GPT-13B Llama-2-7b 1.80 1.61 FLORES EN→X(↑) 37.94 53.72 FLORES X→en(↑) Belebele(↑) SIB-200(↑) 0.25 0.25 48.99 58.65 0.28 0.34\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 46\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 9, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 8: Performance of GPT-13B and Llama 2 7B on Hungarian benchmarks after adaptation\n",
      "\n",
      "Importance Of Base Model Quality\n",
      "\n",
      "To explore the relationship between the quality of the base model employed for language adaptation and its subsequent impact on accuracy in the target language, we ablate using two different base models - Llama 2 7B and GPT-13B (Srinivasan et al., 2023). The GPT-13B model is clearly a lower quality models, as it lags behind Llama 2 7B in every English evaluation tasks we measured in Table 7.\n",
      "\n",
      "We adapt both of these models to Hungarian. We find that using a higher quality base model (Llama 2 7B) leads to better downstream performance in the target language. These results also indicate that as newer higher quality models become available, there is value in applying our proposed adaptation methodology on new base models.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 47\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 9, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 6 Conclusion\n",
      "\n",
      "We present a methodology to adapt pretrained LLMs to new languages. The methodology encompasses both continuous pretraining and alignment to human preferences in the target language. We present experimental results to motivate vocabulary expansion during pretraining and the use of machine translated data for alignment. We scale our methodology to 9 typologically diverse languages and 2 parameter scales, resulting in state of the art models in each language. We make our evaluation scripts and final checkpoints publically available to facilitate future research, and we hope this work outlines a clearer path towards attaining state of the art language models in every language.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 48\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 9, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: References\n",
      "\n",
      "David Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, and En-Shiun Lee. SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 226–245, St. Julian’s, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. eacl-long.14.\n",
      "\n",
      "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxan- dra Cojocaru, M´erouane Debbah, ´Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 49\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 9, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The belebele benchmark: a parallel reading comprehension dataset in 122 language variants. arXiv preprint arXiv:2308.16884, 2023.\n",
      "\n",
      "Terra Blevins, Tomasz Limisiewicz, Suchin Gururangan, Margaret Li, Hila Gonen, Noah A. Smith, and Luke Zettlemoyer. Breaking the curse of multilinguality with cross-lingual expert language models, 2024.\n",
      "\n",
      "Tyler A. Chang, Catherine Arnett, Zhuowen Tu, and Benjamin K. Bergen. When is multilin- guality a curse? language modeling for 250 high- and low-resource languages, 2023.\n",
      "\n",
      "Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 50\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 9, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 9\n",
      "\n",
      "Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2475–2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269.\n",
      "\n",
      "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale, 2020.\n",
      "\n",
      "Zoltan Csaki, Pian Pawakapan, Urmish Thakker, and Qiantong Xu. Efficiently adapting pretrained language models to new languages, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 51\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 10, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high- quality feedback, 2023a.\n",
      "\n",
      "Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca, 2023b.\n",
      "\n",
      "Abteen Ebrahimi and Katharina Kann. How to adapt your pretrained multilingual model to 1600 languages, 2021.\n",
      "\n",
      "Denis Emelin and Rico Sennrich. Wino-X: Multilingual Winograd schemas for common- sense reasoning and coreference resolution. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8517–8532, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.670. URL https://aclanthology.org/2021.emnlp-main.670.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 52\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 10, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.\n",
      "\n",
      "Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzm´an, and Angela Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\n",
      "\n",
      "Kshitij Gupta, Benjamin Th´erien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timoth´ee Lesort. Continual pre-training of large language models: How to (re)warm your model?, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 53\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 10, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Ilya Gusev. Saiga 7b, 2023. URL https://huggingface.co/IlyaGusev/saiga_mistral_ 7b_merged.\n",
      "\n",
      "Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav Nakov. EXAMS: A multi-subject high school examinations dataset for cross- lingual and multilingual question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5427–5444, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.438. URL https:// aclanthology.org/2020.emnlp-main.438.\n",
      "\n",
      "John Hewitt. Initializing new word embeddings for pretrained language models, 2021. URL https://nlp.stanford.edu/~johnhew//vocab-expansion.html.\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 54\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 11, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Edward J Hu, Yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=nZeVKeeFYf9.\n",
      "\n",
      "Shengyi Huang, Lewis Tunstall, Edward Beeching, Leandro von Werra, Omar Sanseviero, Kashif Rasul, and Thomas Wolf. Constitutional ai recipe. Hugging Face Blog, 2024. URL https://huggingface.co/blog/constitutional_ai.\n",
      "\n",
      "Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. OPT-IML: scaling language model instruction meta learning through the lens of generalization. CoRR, abs/2212.12017, 2022. doi: 10.48550/ARXIV.2212.12017. URL https://doi.org/10.48550/arXiv.2212.12017.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 55\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 11, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Fajri Koto, Jey Han Lau, and Timothy Baldwin. Indobertweet: A pretrained language model for indonesian twitter with effective domain-specific vocabulary initialization, 2021.\n",
      "\n",
      "Andreas K ¨opf, Yannic Kilcher, Dimitri von R ¨utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich´ard Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuh- mann, Huu Nguyen, and Alexander Mattick. Openassistant conversations – democratiz- ing large language model alignment, 2023.\n",
      "\n",
      "Peiqin Lin, Shaoxiong Ji, J ¨org Tiedemann, Andr´e F. T. Martins, and Hinrich Sch ¨utze. Mala- 500: Massive language adaptation of large language models, 2024.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 56\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 11, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual language models, 2022.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 57\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 11, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay, Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi, Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain, Ankit Jindal, Brucek Khailany, George Kokai, Kishor Kunal, Xiaowei Li, Charley Lind, Hao Liu, Stuart Oberman, Sujeet Omar, Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik P Suthar, Varun Tej, Walker Turner, Kaizhe Xu, and Haoxing Ren. Chipnemo: Domain-adapted llms for chip design, 2024.\n",
      "\n",
      "Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 58\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 11, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 59\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 11, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren- cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brak- man, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 60\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 11, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 11\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 61\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 12, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim ´on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 62\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 12, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M´ely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 63\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 12, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Fran- cis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thomp- son, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer ´on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 64\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 12, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Hannah Wong, Lauren Workman, Sher- win Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 65\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 12, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarn- mongkol, Ruangsak Patomwong, Pathomporn Chokchainant, and Kasima Tharnpipitchai. Typhoon: Thai large language models, 2023.\n",
      "\n",
      "Ramon Pires, Hugo Abonizio, Thales Sales Almeida, and Rodrigo Nogueira. Sabi´a: Por- tuguese Large Language Models, pp. 226–240. Springer Nature Switzerland, 2023. ISBN 9783031453922. doi: 10.1007/978-3-031-45392-2 15. URL http://dx.doi.org/10.1007/ 978-3-031-45392-2_15.\n",
      "\n",
      "Edoardo Maria Ponti, Goran Glavaˇs, Olga Majewska, Qianchu Liu, Ivan Vuli´c, and Anna Korhonen. XCOPA: A multilingual dataset for causal commonsense reasoning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2362–2376, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.185. URL https://aclanthology.org/2020.emnlp-main.185.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 66\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 12, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Maja Popovi´c. chrF: character n-gram F-score for automatic MT evaluation. In Ondˇrej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina (eds.), Proceedings of the Tenth Workshop\n",
      "\n",
      "12\n",
      "\n",
      "on Statistical Machine Translation, pp. 392–395, Lisbon, Portugal, September 2015. As- sociation for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https: //aclanthology.org/W15-3049.\n",
      "\n",
      "Raghu Prabhakar, Sumti Jairath, and Jinuk Luke Shin. Sambanova sn10 rdu: A 7nm dataflow In 2022 IEEE International Solid-State Circuits architecture to accelerate software 2.0. Conference (ISSCC), volume 65, pp. 350–352, 2022. doi: 10.1109/ISSCC42614.2022.9731612.\n",
      "\n",
      "Together Computer SambaNova Systems. BLOOMChat: a New Open Multilingual Chat LLM, 5 2023. URL https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 67\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 13, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Akira Sasaki, Masato Hirakawa, Shintaro Horie, and Tomoaki Nakamura. Elyza-japanese- llama-2-7b, 2023. URL https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b.\n",
      "\n",
      "Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mu- jahid, Massa Baali, Xudong Han, Sondos Mahmoud Bsharat, Alham Fikri Aji, Zhiqiang Shen, Zhengzhong Liu, Natalia Vassilieva, Joel Hestness, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Hector Xuguang Ren, Preslav Nakov, Timothy Baldwin, and Eric Xing. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models, 2023.\n",
      "\n",
      "Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. mgpt: Few-shot learners go multilingual, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 68\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 13, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Venkat Srinivasan, Darshan Gandhi, Urmish Thakker, and Raghu Prabhakar. Training large language models efficiently with sparsity and dataflow, 2023.\n",
      "\n",
      "Xianghui Sun, Yunjie Ji, Baochang Ma, and Xiangang Li. A comparative study between full-parameter and lora-based fine-tuning on chinese instruction data for instruction following large language model. arXiv preprint arXiv:2304.08109, 2023.\n",
      "\n",
      "SambaNova Systems. x-self-instruct-seed-32, 5 2023a. URL https://huggingface.co/ datasets/sambanovasystems/x-self-instruct-seed-32.\n",
      "\n",
      "SambaNova Systems. xoa22, 5 2023b. URL https://huggingface.co/datasets/ sambanovasystems/xOA22.\n",
      "\n",
      "Mikhail Tikhomirov and Daniil Chernyshev. Impact of tokenization on llama russian adaptation, 2023.\n",
      "\n",
      "TokyoTech. Swallow 7b, 2023. URL https://huggingface.co/tokyotech-llm/ Swallow-7b-hf.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 69\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 13, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 70\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 13, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 71\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 13, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl´ementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023.\n",
      "\n",
      "13\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 72\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 14, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagn´e, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Gall´e, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoˆıt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Belt- agy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenc¸on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz´alez Ponfer- rada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G´erard Dupont, Germ´an Kruszewski, Giada Pistilli, Hady Elsahar,\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 73\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 14, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J ¨org Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu ˜noz, Maraim Masoud, Mar´ıa Grandury, Mario ˇSaˇsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espe- jel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L ´opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick,\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 74\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 14, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Tas¸ar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 75\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 14, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franc¸ois Lavall´ee, R´emi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St´ephane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne- Laure Ligozat, Arjun Subramonian, Aur´elie N´ev´eol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bog- danov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Os- kar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenˇek Kasner, Alice\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 76\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 14, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu ˜noz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olan- rewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang,\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 77\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 14, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 14\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 78\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 15, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl´ementine Fourrier, Daniel Le ´on Peri ˜n´an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P`amies, Maria A Castillo, Marianna Nezhurina, Mario S¨anger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 79\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 15, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th´eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 80\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 15, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer, 2021.\n",
      "\n",
      "Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Kentaro Inui, Jing Jiang, Vin- cent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3687–3692, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL https://aclanthology.org/D19-1382.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 81\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 15, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Zijian Gy˝oz˝o Yang, L´aszl ´o J´anos Laki, Tam´as V´aradi, and G´abor Pr ´osz´eky. Mono- and multilingual gpt-3 models for hungarian. In Text, Speech, and Dialogue, Lecture Notes in Computer Science, pp. 94–104, Plze ˇn, Czech Republic, 2023. Springer Nature Switzerland. ISBN 978-3-031-40498-6.\n",
      "\n",
      "Jiacheng Ye, Xijia Tao, and Lingpeng Kong. Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability, 2023.\n",
      "\n",
      "Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vas- silina Nikoulina. Bloom+1: Adding language support to bloom for zero-shot prompting, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 82\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 15, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, and Maosong Sun. Cpm: A large-scale generative chinese pre-trained language model, 2020.\n",
      "\n",
      "Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024a.\n",
      "\n",
      "Jun Zhao, Zhihao Zhang, Luhui Gao, Qi Zhang, Tao Gui, and Xuanjing Huang. Llama beyond english: An empirical study on language capability transfer, 2024b.\n",
      "\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 83\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 15, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 15\n",
      "\n",
      "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis, 2023.\n",
      "\n",
      "Judit ´Acs. Exploring bert’s vocabulary, February 2019. URL https://juditacs.github. io/2019/02/19/bert-tokenization-stats.html.\n",
      "\n",
      "Ahmet ¨Ust ¨un, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An instruction finetuned open-access multilingual language model, 2024.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 84\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 16, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: A Appendix\n",
      "\n",
      "A.1 Hyperparameters\n",
      "\n",
      "• Continuous Pre-training: We pack the pretraining mixture into sequences of length 4096 and pretrain with document attention as described in Section 3.2 of Iyer et al. (2022) to ensure we only attend to tokens in the context of the corresponding text document. We train with a global batch size of 1024, sequence length of 4096, maximum learning rate of 1e-4 with cosine decay, warm-up ratio of 0.01 and a weight decay of 0.1. Each expert is trained for a maximum of 4 epochs, following (Muennighoff et al., 2023). Notably, we train all model parameters, foregoing use of PEFT methods such as LoRA (Hu et al., 2022), which are known to be inferior to full parameter training (Zhao et al., 2024a)(Sun et al., 2023).\n",
      "\n",
      "• Hardware: All experiments were run on Reconfigurable Dataflow Units (RDUs) Prabhakar et al. (2022).\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 85\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 16, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: • Supervised Finetuning: We use a global batch size of 512 and a maximum sequence length of 2048 tokens. We used a linear decay learning rate of 2e-5 with 10% warm up\n",
      "\n",
      "• Direct Preference Optimization: We train with a global batch size 32 for 3 epochs, a linear decay learning rate of 5e-7, 10% warmup and β = 0.1 as the regularization factor for DPO\n",
      "\n",
      "A.2 Language Experts vs Monolith Multilingual Model\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 86\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 16, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: “The Curse Of Multilinguality” (Chang et al., 2023; Conneau et al., 2020) is the idea that LLMs have a fixed capacity with which to learn various languages. This theory claims that as one expands the number of languages a model is trained on, the various languages compete for the capacity of the model, therefore degrading the models performance across all languages. Blevins et al. (2024) attempt to address this phenomenon by adapting multiple small-scale language experts from XGLM-1.7B (Lin et al., 2022), one for each language, and show that each expert outperforms training a single monolithic model trained simultaneously on one language. We build on these results by scaling this study to 7B parameters and use more comprehensive evaluation metrics than just perplexity. We compare our 9 Llama 2 7B language experts against a monolith Llama 2 7B model continuously pretrained on all 9 languages. We ensure that each language is represented equally in the monolith’s training data and the\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 87\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 16, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: vocabulary is expanded to represent all 9 languages evenly.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 88\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 16, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: For comparison’s sake, we select intermediate model checkpoints such that each individual language expert has used the same amount of compute as the monolith multilingual model. This means that the experts required 9x more compute to train then the monolith. Table 9 averages the evaluation results across all 9 languages and finds that the monolith model and language experts have very similar performance. This implies that if one wants to adapt to many languages at once, it may be more compute-efficient to continuously train a multi- linugal model rather then independent experts. Further work is warranted to determine how this result scales with an increasing number of target languages.\n",
      "\n",
      "16\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 89\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.Table'>\n",
      "META: {'detection_class_prob': 0.9300399422645569, 'coordinates': {'points': ((342.850341796875, 226.88214111328125), (342.850341796875, 589.9151000976562), (1345.0238037109375, 589.9151000976562), (1345.0238037109375, 226.88214111328125)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2024-05-06T12:02:48', 'text_as_html': '<table><thead><th></th><th>Llama2-7b Avg. Score</th><th>Multilingual Mono. Avg. Score</th><th>Language Expert Avg. Score</th></thead><tr><td>{ Holdout PPL</td><td>175</td><td>1.55</td><td>1.50</td></tr><tr><td>+ FLORES X-&gt;en (8 shot, CHRF)</td><td>40.42%</td><td>50.69%</td><td>51.71%</td></tr><tr><td>+ FLORES X-&gt;en (8 shot, BLEU)</td><td>53.95%</td><td>58.23%</td><td>59.37%</td></tr><tr><td>+ Belebele (3 shot)</td><td>36.24%</td><td>33.36%</td><td>32.09%</td></tr><tr><td>+ SIB-200 (3 shot)</td><td>26.67%</td><td>38.04%</td><td>33.43%</td></tr><tr><td>+ XNLI (0 shot)</td><td>39.00%</td><td>43.44%</td><td>43.04%</td></tr><tr><td>+ XStoryCloze (0 shot)</td><td>56.35%</td><td>65.75%</td><td>68.03%</td></tr><tr><td>+ XWinograd (0 shot)</td><td>69.48%</td><td>72.39%</td><td>71.97%</td></tr><tr><td>+ PAWS-X (0 shot)</td><td>51.00%</td><td>54.40%</td><td>53.50%</td></tr><tr><td>+ MGSM (3 shot)</td><td>5.40%</td><td>4.00%</td><td>4.20%</td></tr></table>', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 17, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Llama2-7b Avg. Score Mono. Avg. Score Multilingual Language Expert Avg. Score ↓ Holdout PPL ↑ FLORES X->en (8 shot, CHRF) ↑ FLORES X->en (8 shot, BLEU) ↑ Belebele (3 shot) ↑ SIB-200 (3 shot) ↑ XNLI (0 shot) ↑ XStoryCloze (0 shot) ↑ XWinograd (0 shot) ↑ PAWS-X (0 shot) ↑ MGSM (3 shot) 1.75 40.42% 53.95% 36.24% 26.67% 39.00% 56.35% 69.48% 51.00% 5.40% 1.55 50.69% 58.23% 33.36% 38.04% 43.44% 65.75% 72.39% 54.40% 4.00% 1.50 51.71% 59.37% 32.09% 33.43% 43.04% 68.03% 71.97% 53.50% 4.20%\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 90\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 17, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Table 9: Monolith multilingual continuous training vs language experts, averaged over all 9 languages.\n",
      "\n",
      "A.3 Qualitative Results\n",
      "\n",
      "For Arabic, we compare our 7B arabic expert with aya-101 ( ¨Ust ¨un et al., 2024), Jais-13b-chat (Sengupta et al., 2023), and Bloomchat-v1 (SambaNova Systems, 2023) and use prompts from x-self-instruct-seed-32 (Systems, 2023a) and xOA22 (Systems, 2023b). Our Arabic chat model reaches 87.96% win rate compared to Jais-13B-chat, 99.06% win rate compared to Aya101, and 68.52% compared to Bloomchat-v1. For Japanese, we compare our Japanese chat model with ELYZA-japanese-Llama-2-7b-instruct (Sasaki et al., 2023) on 100 randomly sampled prompts aya dataset ( ¨Ust ¨un et al., 2024), reaching a win rate of 53.5% For Turkish, we compare our Turkish chat model against aya-101 ( ¨Ust ¨un et al., 2024) using prompts from the test set of aya dataset ( ¨Ust ¨un et al., 2024), leading to win-rate of 92.4%.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 91\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 17, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: A.4 Expanded Vocabulary Tokenizer Fertility\n",
      "\n",
      "Fertility ao: —? — 1 = bf = 0 5000 10000 15000 20000 25000 Number Of Added Tokens\n",
      "\n",
      "@ Hungarian 4 Russian = Turkish @ Bulgarian * Arabic = Japanese @ Thai\n",
      "\n",
      "Figure 5: Tokenizer Fertility: the average number of tokens per ”word” ( ´Acs, 2019)\n",
      "\n",
      "A.5 Main Results Details\n",
      "\n",
      "A.6 GPT-4 As A Judge\n",
      "\n",
      "Below are some examples of how GPT-4 judged two candidate responses in many languages. See figures 8,9, 10, 11, 12, 13\n",
      "\n",
      "17\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 92\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 18, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: 970 8e0 ogo x) v0 6r0 oso 120 6z0 sco 6z0 zo 20 v0 2s°0 szo 10 ze0 6e0 zo 8e0 zo 6e0 v0 seo wo 9e0 svo sv'0 v0 £90 270 6e0 geo spo 6s0 8v'0 v0 eBesay 090 ovo ve0 €z0 85°86 65°86 ery zor VOL-WAY wn 8s0 2v0 szo sz0 Loel pe Ser 662r £982 o8b Lob esl ag Z-wi6x YspunL 1g0 se0 so 6z0 zit Wise = u'bz sxzL oo vee sez baZ-wo91q vn sso 280 970 ze0 9s'92 ove = e0'er Lolz oz we a4 au-zewen Ys 9g0 ze0 sco £20 000 000 200 000 WNUNL ‘USPUTL 990 ero wo oe'0 ss'09 vous 96°SS 19s 99'1 et eg \\tJOUOW\\ 19 usp ozo sro x) 60 sole Loz zs zees sob 6st 9st az-usPynL-INO spun. oor 80 $e0 90 €z0 0070 000 180 390 VOL-VAV eu 100 6s'0 zo szo +0 evs oso ore v0E ray ort ov ag Z-wi6x ru 200 sso veo sco 420 eer ero coeb ggeL eeb Blt ee b2-woolq eu v0'0 es'0 980 so leo oss Cad 81°62 seer sot ssh es qu-zewen ru Zz0 90 zo oco 090 ove ses 879% 90s ort wh eh qu-u09ydhy ru, 120°0 og0 ero oro leo z0uz Sele Lbs ers gz Cay seb UAIIOUO\\ JNO 'g0'0 190 sro so 80 verez vob 209 Lees zt as a @LEUL-INO PUL ZO 790 bro LL0 v0 el 9e Z6bb\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 93\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 18, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: svz9o Less Lia AOZ-2ULsnO eu oor 910 £90 60 €z0 000 000 60 ze0 Torway] — eseueder oor 80 s90 sto 0 asp too vos 80°61 og't we og agswi6x| asoueder roo sso 6s0 szo 9z0 Leb ezo vee ae ozz we ad Laz-wooa] aseueder zoo iso 0x0 ero 6e0 £602 oe ze'sh poe it £07 06k qu-zewen} — esaueder oro v0 180 170 60 ghee Zh 28°05 ac6e SLL ib sob MOIIEMS asoueder zoo 60 1£0 evo veo vest 100 veer 88°91 zt ay seb ewer-esouede/yza1a| esourder soo ¥s'0 8x0 oro ee0 98°07 “0 8e'6h Lorre sot ost 9g Utjouoy ing] aseueder }Z0'O so LL0 920 820 ever x4 ozs ve 6b vSb gosh gz-eseueder-inQ aseueder 9s°0 860 80 €z0 000 or) 88°0 160 LOl-vAV oaey 90 veo 920 Sz0 os'zz abe 6e6r 29°8L 6S ool ool GS L-wi6x gery eso veo svo 8z0 Le6z og'eL gers Ley zt ist is £20019 sey os‘o seo 90 leo wiz ove 96'bb go'ez eet £6 We qe-zewen aqey 790 £60 960 160 sees Z6°8L 6e°8S 6e67 6EL wl os geL-sier oqeny zoo e€0 90 leo ones ze'69 ees ee'6y ort ert zs \\AtJOUOW\\ 419 omey 9970 v0 6z0 v0 gore beze, 6069 Lys set ort are aseg-oiqeny-nO omey 690 seo0\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 94\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 18, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: £20 v0 Lol go92 66:99 L9LS BEL g02-01Ge4y-1ND oqeay oo'o 290 oso ovo 80 €z0 000 i) v0\" sro LO-VAV ueissny Z0'0 £90 £90 9r0 920 Sz0 Srl eer ‘y9'0S zz eb 6Lb os ag Z-wi6x ueissny zoo is0 ego ero svo uo Los} 09's sLibb zee eet ort Ist baZ-woo1q ueyssmy ‘80° 690 e970 wo 970 zo oxee eelz so'es es'ér get ert es qu-zewen uessmy ee0 290 690 oso £90 SLO 60 re B27 ong s6cs 68z ele och qz-e61es uelssmy soo 190 690 oso v0 ve0 cele iss? zz'es spes set let sr UAt}OUOWN 49 uueyssry 60'0 690 zz0 960 evo se0 Lose Luz erg 06'rs oft set br qu-ueissny-ino uejssny youse 40S NSD = NOUS = }ONSQ.-—sNOUSD douse = douse = douse = naa «= 1@ = HD = UH. dd Tad Tad jujodyosug —ebnebuey wsow WdOOxK XSMVd PesBoumx — 9201. TNX ewexy 00z-a1S Neqe919g uss yous B yous s yous 8 eIped im, vow YOPIOH Moisx ue<X X<NI ue<X X<-NB s3u014 SaxO14 S3NO14 SaxOT4\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 95\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 18, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: rug\n",
      "\n",
      "|\n",
      "\n",
      "Figure 6: Main results, evaluation benchmarks described in 4.1.This data is averaged to create 2.\n",
      "\n",
      "18\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 96\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 19, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: vo zo szo +70 ro 670 870 eo szo 220 70 ro “v0 vS0 vo 20 20 6e0 9€°0 70 ero £90 Wo oro 6Z0 ero 920 oso sO eBesony | jouse WSOW wus 0 wd0Ox youso —-}OUS.—JOUS X-SMVd PeJBOUMX — 92019 Aoisx eo eo 00'0 000 cae £60 LOL-WAW ozo vz0 ore ze0 vB'eZ v6e'0L ab o'r ery ag\" Z-WI8x ozo azo zoe seb Loe 9zZb ogre bre ore Lgz-woo1q 9e'0 ae0 ere Lh 1g1s Stor 98\"L 80% We qu-zewen sO 20 400 90°0 7298 186 ad6-is 10 ogo eves 65°22 Zb09 6LSS oet Zot eLb UUIOUOW JNO ero seo 7S Ze 6E°9% 09°8S Lees oz 69°L Bob GZ-UBIU2AO|S~INO 6z'0 £70 00'0 000 €0'L 190 LOL-WAW 220 20 139 Lv0 vee se6h are 09% 6% ag\" Z-WwWj5x 520 820 196 gob os'ze 80% bee eee ele bg.-woojq 520 6e°0 es'6e 9°02 869 61 6b eh Zot 0ob qz-zewel) 520 20 £00 00°0 6rL sto ad6-us 2e0 wo se0 61 Or SOLE ersg €S°8S sel orl sh UUoUOW JNO 650 650 20 1S'Or £0°6% 68°79 91-9S 92h seb rr gL-uelqies-inQ oe'0 ez ooo 000 280 1s°0 LOL-WAW 220 20 90 sO 2B bb oro Ly er 9ee ag\" /-wjSx sz0 220 shy 70 el ee ssZb soe Sle coe bgL-woojq sz0 ceo 9222 Lver 68'€S Lvtr Lb vee sob\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 97\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 19, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: qz-gewel) sz0 20 SOLE besh €2 Or oor “ot plt el OMLd9-11Nd sO oco e562 00S 90°45 12s “ot Blt 9b UUoUOW JNO sZo v0 vole vets s9'es Les gst cot ob gz-ueveBun}-1nQ 9s'0 920 2oze 6esz ores shes et goz-ueveBUNH-3nO 20 ee0 0e0 20 000 00°0 98'0 090 LOL-WAW sro +O sz0 €20 e182 se Ob eres SO'Sb 9b Srl os} ag\" -wySx 6£0 920 sZo 820 1e0b 02% sg'ce 80ee ore 4 907% $QL-woojq Wwo 8e0 220 20 sso are er7g ssis 6eL WF 6s) qe-cewer) veo oo sto ez0 6p 68e aL re egal est sob SLL uevebing-ae b-Ldow “v0 6r0 svo seo ecee sve os'79 zeL9 vel ad orl YmIoUOW ANE 670 70 evo 9e0 1e6e szoe 90'S9 v629 a get ia aseg-uevebing-inQ yous 0 TNX yous E swexg wus & 00z-a1S yOUs E 91999198 naa yous g ua<X sauo74 nad yous 8 X<N2 s3yold HO yous g u2<X sauots dYHO yous 8 X<Na sayo1s dd eipadyinn dd vOW dd yNoploH qujodyoayg UBIUBADIS UBIUBADIS UBIUBADIS UBIUBADIS uelUanc|s ueIuaroig ueIUanc|s ueiqieg ueiqies ueiqies ueiqies ueiqies ues veges uevebuny ueuebuny uewebuny uewebuny uewebuny uewebuny uevebuny ueveBuny ueye6ing\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 98\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 19, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: ueuebing ueuesing ueue6ing ueue6ing ueue6ing ueue6ing ‘obneBuey\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 99\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 19, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Figure 7: Main results, evaluation benchmarks described in 4.1.This data is averaged to create 2.\n",
      "\n",
      "19\n",
      "\n",
      "Prompt Model 1 Completion Model 2 Completion GPT4 Judge 18697 (CMB | 1869F(CHBLE | HARROP AS, | Assistant A's LEHARRO | HREROBIOZH | 1869 F(CHIBLIEZ | response is incorrect. WoRA(sayc lk. TFDY E+ Ey IARC. The canal opened in La3? xAY-PYE- ty 1869 is the Suez BI7L-PYE- kL Canal, not the \"Grand TIL: NAB SF Canyon and Santa ° Fe and Trail Pass Canal\" as stated by Assistant A. COBAML?P AVAG Furthermore, the RBOP IVE Suez Canal is not Da- AF YIN only the longest canal SERAN435kMO Ew opened in 1869, but it CT. is also currently the longest canal in the world. Assistant B SRI, WERE correctly identifies the ROA. P7TVN Suez Canal as the KBCP LP ARE longest canal opened & ALABA EF in 1869. Therefore, Tt. Assistant B's response is more accurate and relevant to the user's question.\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 100\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 20, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Figure 8: Japanese Example 1\n",
      "\n",
      "Prompt Model 1 Completion Model 2 Completion | GPT4 Judge fESEBTC 1 7 REDSBVET DBD, GRSEBC FIXED S5ZSTC L&3? ARB. ALU OR<. SMTDRK< £6 31 MFHVE to. Rit, SMCS F586 \"At Al MEATS. Assistant A's response is correct. A Tanka (#23) in Japanese poetry is composed of 31 syllables (or \"on\" in Japanese), not 5 as Assistant B incorrectly states. Assistant B seems to be confusing Tanka with Haiku, which is composed of 17 syllables and is often structured as 5-7-5. Therefore, Assistant A provides a more accurate and relevant answer to the user's question.\n",
      "\n",
      "Figure 9: Japanese Example 2\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 101\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 21, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Prompt Model 1 Completion Model 2 Completion GPT4 Judge PLD M1 | Sud puilldis ouiea} ons] 1998 ple i Google -L|* | Both assistants Novalgin 36 43, xl «Google Inc 42 GH GOYs GL peo 4 le | provided relevant and YY! iiss le le\"\\ aly Able: Haale oof Gullb LIS Laie | accurate information cole inete} | gd N aanie 4S yal | oh ins & 5 peS | lay al bt, | about the effectiveness Faldd) Stee yh | 5 BY! Gilead LS yall GHEY lee | LS yale gle jury cay, dale | of Novalgin in pain tad, Nyt al Gra jae GUIS! | relief. However, \\ sald 1998 ple .,d «iLL | Assistant A provided a 1998 sci A 44S pI) Could | gle i, dS 525 Google de) | more comprehensive Sergey Brin » Larry Page JS Faala gi oly sSall LUlb Len 5 « ch stl a fhe ay 2 pitas YY sll «Gh sills « Menlo Park AS) ye) Santall ay « Lala 45 428 Google als 19 cod pall GSN legend cb ils 3 oe Ve! 2004 Galauel 4S 44! Alphabet Inc +3 « 2023 -Google 45 2! s¥! « Anagald ya Cal gian ates Ope gh opti claw! Si as] Google cxal ye BY) pethtne Ow andy Asia! « 2021 ple\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 102\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 21, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: sls pllall dues U4 195 o« Jl Google Lae ¢ tnd) CAS yar Ayala 5 gl nal In 5 & Ss De Gesell Alan aS Google 3%! 5 «1999 we tay el AM Sel gall anh oS. ck Google tig 8 Cuale _ le WSS 8 8 Call CIS jae Soll hej suki gf seal Leased 6S lly pee eat Hada Ade GIS) ug all ye co) BLSYL pthtedl GLORIA die Cl je Google Cuaa8 «alls Sod) Gal Bly dal all Als aga yill Col ily patel gle pall Ge Galen galing ll Cla gle gle gall Ae yt all Tle gd pale Al Jide alls As yad ule 4 5455 5 Google patel Gf |S ja ad prdtueall Af 5 Aer yaw o see Gilg pel oy el Su! Ages Fags (IRAN) Ulges Ugaly ob) oe apaall ety Lal | yald ail che Seal cys Cle (5 Gil jul So) ils gb ial Geerdineall etal day 5 Sle sags Leela Can gh y Alauciall Conall iy Abu! Ox gael UNS cole JSty cial Ne Cand Se U8 Elly das fl seul 05 aclu gill a JY! gle das cual ul ule Google celal (bjt SY) al response by mentioning a specific study published in a pain research journal in 2016, and also discussed the factors that might affect the effectiveness of\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 103\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 21, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Novalgin. Assistant B also mentioned some studies, but the information provided was less detailed and less specific compared to Assistant A. Therefore, Assistant A's response is more helpful and detailed. Final Verdict: [[A]]\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 104\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 21, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Figure 10: Arabic Example 1\n",
      "\n",
      "21\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 105\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 22, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Prompt Model 1 Completion Model 2 Completion GPT4 Judge 2410 M1 ox pull Zid Cltlas| as 5 Y Saal ol Upasll giSe Y | Both assistants Novalgin 25 es el) GS cod ullé sll Shad Aud pal lidy GSI, cGlsLaa¥! | provided relevant and VY Gis le | gl) ge Cla pall aes Gb call | ple gd oll! Gilat dla 4.1545 | accurate information gle Wate) | Silokias Sie Yd GS oF Gaal ll cei Jad yap YU Ob «2016 | about the effectiveness Jalidl Giles! (NSAIDs) 4325 98! je Gly! etl GAY AIT ede GF GAY! Jeelicll 2013 ple § pina dul js Cosy hy el gs dee Aled AS) IS Qual gil gl Sy Ua acll nae all chlit Gb pty 2014 ple Goal dul js Gongs Ald Bie Aad Gadd Ad IS Qual gill gl Saas el oe od Gah ie! gh vall ial SY! Ge jd dale His alld aay OSS of Gel gl Uled poe yond Gb ysl cl) Joie) JY) iss say be 6 Oe Otley Gl aja Ch Aud pall casey dal pol Tall ye alle SI GIS cua Ud alge gle NSbeie! oll) Ui (A AMM y pl) Bae cal) Sai he Desai cy cy pall Label Sec! fe 3! pluasl 6 Lata) Laila Bylo} SIGUA acai! auast discal suill Glalial le ly AY!\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 106\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 22, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: ual! of Novalgin in pain relief. However, Assistant A provided a more comprehensive response by mentioning a specific study published in a pain research journal in 2016, and also discussed the factors that might affect the effectiveness of Novalgin. Assistant B also mentioned some studies, but the information provided was less detailed and less specific compared to Assistant A. Therefore, Assistant A's response is more helpful and detailed. Final Verdict: [[A]]\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 107\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 22, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Figure 11: Arabic Example 2\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 108\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 22, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Prompt Model 1 Completion Model 2 Completion GPT4 Judge Cilt bakiminda | ika arasinda yapilir. Cilt bakimi rutinine buhar Assistant A's response buhar kag uygulamasi genellikle 5-10 | is incomplete and does dakika verilir? dakika kadar kisa bir siire__| not provide any useful igin yapilir. Buhar, gézeneklerin agilmasina ve cildin nemlendirilmesine yardimci olur, bu da onu cilt bakimi rutininin 6nemli bir pargasi haline getirir. Ancak cilt tipine ve kisisel tercihe bagli olarak buhar siiresi ayarlanabilir. Buhar uygulamas! sirasinda ciltte herhangi bir rahatsizlik veya tahrig hissedilirse, buhar uygulamas! durdurulmali ve bir dermatologa veya cilt bakimi uzmanina danisilmalidir. information to the user's question. On the other hand, Assistant B provides a detailed and informative response, explaining that steam is typically applied for 5-10 minutes during a skincare routine, the benefits of steam, and advice on what to do if discomfort or irritation is experienced. Therefore, Assistant\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 109\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 22, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: B's response is more helpful, relevant, and accurate. My final verdict is: [[B]]\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 110\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 22, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Figure 12: Turkish Example 1\n",
      "\n",
      "22\n",
      "\n",
      "Prompt Model 1 Completion Model 2 Completion GPT4 Judge Sitgii imam Universitesi ne zaman kuruldu? li ilgesinde, Sivas Valisi Siileyman Demirel'in talimatiyla, Sivas'in taninmis tarihi ve kiiltirel degerlerine sahip, Sivas'in ilk ve tek deviet Universitesi olarak kurulmustur. Kahramanmarasg Siitgii imam University (KSU) was founded in 1986 and is located in the city of Kahramanmaras in southeastern Turkey. Assistant A provides a direct and accurate answer to the user's question, stating that Siitcti imam University was established in 1986 and is located in Kahramanmaras in southeastern Turkey. On the other hand, Assistant B's response is irrelevant and confusing, as it talks about a university i Sivas and mentions Sitileyman Demirel, who was not related to the establishment of Siti Imam University. Therefore, Assistant A's response is more helpful, relevant, and accurate. Final Verdict: [[A]\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\u001b[95m ELEMENT 111\u001b[00m\n",
      "TYPE: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "META: {'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 23, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf'}\n",
      "TEXT: Figure 13: Turkish Example 2\n",
      "\n",
      "23\n",
      "\n",
      "\n",
      "##########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(raw_pdf_elements):\n",
    "    print(f\"\\033[95m ELEMENT {i}\\033[00m\")\n",
    "    print(f\"TYPE: {type(element)}\")\n",
    "    print(f\"META: {element.metadata.to_dict()}\")\n",
    "    print(f\"TEXT: {element.text}\")\n",
    "    print(\"\\n\\n##########\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 102,\n",
       " \"<class 'unstructured.documents.elements.Table'>\": 8,\n",
       " \"<class 'unstructured.documents.elements.TableChunk'>\": 2}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "# TableChunk if Table > max chars set above\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        meta = element.metadata.to_dict()\n",
    "        meta[\"type\"] = \"table\"\n",
    "        categorized_elements.append(Document(page_content=str(element), metadata=meta))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        meta = element.metadata.to_dict()\n",
    "        meta[\"type\"] = \"text\"\n",
    "        categorized_elements.append(Document(page_content=str(element), metadata=meta))\n",
    "\n",
    "# Tables\n",
    "table_docs = [e for e in categorized_elements if e.metadata[\"type\"] == \"table\"]\n",
    "print(len(table_docs))\n",
    "\n",
    "# Text\n",
    "text_docs = [e for e in categorized_elements if e.metadata[\"type\"] == \"text\"]\n",
    "print(len(text_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and table summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.sambanova import Sambaverse\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(os.path.join(kit_dir, \"prompts\", \"llama70b-summary.yaml\"))\n",
    "\n",
    "# Summary chain\n",
    "model = Sambaverse(\n",
    "    sambaverse_model_name=\"Meta/llama-2-70b-chat-hf\",\n",
    "    model_kwargs={\n",
    "            \"do_sample\": True, \n",
    "            \"max_tokens_to_generate\": 256,\n",
    "            \"temperature\": 0.01,\n",
    "            \"process_prompt\": True,\n",
    "            \"select_expert\": \"llama-2-70b-chat-hf\"\n",
    "            #\"stop_sequences\": { \"type\":\"str\", \"value\":\"\"},\n",
    "            # \"repetition_penalty\": {\"type\": \"float\", \"value\": \"1\"},\n",
    "            # \"top_k\": {\"type\": \"int\", \"value\": \"50\"},\n",
    "            # \"top_p\": {\"type\": \"float\", \"value\": \"1\"}\n",
    "        }\n",
    ")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to text\n",
    "texts = [i.page_content for i in text_docs if i.page_content != \"\"]\n",
    "if texts:\n",
    "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The table shows a schedule with four rows and two columns. The first column contains numbers, while the second column has abbreviations. The text discusses SambaLingo, a method for teaching large language models new languages. The authors are Zoltan Csaki et al., and the abstract is not provided.',\n",
       " ' This paper investigates the adaptation of large language models (LLMs) to new languages, addressing the gap in their capabilities and availability across diverse languages. The study comprehensively covers key components such as vocabulary extension, direct preference optimization, and data scarcity problem for human alignment in low-resource languages. The experiments are scaled across 9 languages and 2 parameter scales, outperforming all prior published baselines. The paper also makes all evaluation code and checkpoints publicly available.',\n",
       " ' The table provides information on the top 100 most cited papers in the field of artificial intelligence, including their authors, publication year, and number of citations. The table aims to facilitate future research by providing a comprehensive overview of the most influential papers in the field.',\n",
       " \" Sure! Here's a concise summary of the introduction:\\n\\nThe introduction discusses the importance of summarizing tables and text, and the role of a helpful assistant in providing a concise summary.\",\n",
       " ' New large language models are being developed quickly, but they mostly focus on popular languages like English, Chinese, French, and Arabic. Researchers have tried to create models for other languages by training multi-lingual models that cover many languages, but these models can struggle to perform well across all languages due to factors like the curse of multilinguality and lack of pre-training data.',\n",
       " ' The paper focuses on adapting English-centric models to new languages, which has gained prominence in recent years. The authors aim to provide a comprehensive study to inform design choices for adapting pre-trained models to new languages, outlining a clear protocol and testing it across 9 languages and 2 parameter scales. The results show that the methodology is effective, with the adapted models outperforming publicly available models.',\n",
       " ' The table/text chunk provided appears to be describing a research study or project related to natural language processing and machine learning. The study aims to improve the performance of language models in various languages using a fork of the lm-evaluation-harness tool and new multilingual benchmarks. The study also investigates the impact of different techniques, such as vocabulary expansion and embedding initialization methods, on the performance of the language Dialogue models.\\n\\nThe key findings of the study include:\\n\\n* Vocabulary expansion improves tokenizer fertility but does not significantly impact downstream accuracy.\\n* Different embedding initialization methods have minimal impact on accuracy, but sub-word averaging accelerates training loss convergence.\\n\\nOverall, the study seems to be focused on improving the performance of language models in various languages using a combination of techniques, including vocabulary expansion and embedding initialization methods.',\n",
       " ' The table shows perplexity scores for various languages using a language model adapted from Llama 2. The model was trained on open source data and achieved state-of-the-art results in 9 languages and 2 parameter scales (7B, 70B). The table also includes results from integrating various benchmarks with lm-eval-harness1. The adapted model showed improved performance in downstream language adaptation tasks, and the choice of translated versus human-written alignment data had minimal impact on win rates. The author suggests that open-sourcing code and checkpoints can promote future research.',\n",
       " \" The table compares the perplexity of models trained with the author's methodology (SambaLingo 7B and SambaLingo 70B) with open source language experts and other multilingual models on a hold-out dataset. The table includes models for various languages, including Japanese, Thai, Arabic, Hungarian, Russian, and Bulgarian. The authors note that they did not include models for Serbian, Slovenian, and Turkish languages as they had higher perplexity and would have made the graph less readable.\",\n",
       " ' This section discusses related work on adapting pre-trained language models (LLMs) to new languages. While previous studies have explored this topic, they do not thoroughly examine the methodology for doing so. Specifically, they do not consider design choices such as combining data from the base language and the new language or the effect of translated data on evaluation metrics. Two studies, Pires et al. (2023) and Cui et al. (2023b), are mentioned as adapting Llama models to Portuguese and other languages.',\n",
       " ' The table/text discusses various studies on multilingual language models, specifically focusing on vocabulary extension, initialization, and quality of base models. While some studies have explored training language experts and simultaneous language adaptation, none have addressed the impact of vocabulary extension, initialization, and quality of base models. Ye et al. (2023) studies language adaptation, but only for XNLI tasks. Overall, there is a gap in research on the impact of vocabulary extension, initialization, and quality of base models in multilingual language models.',\n",
       " ' The table or text discusses the difference between the current work and previous studies on open-source multi-lingual models. The current work focuses on adapting pre-trained language models (LLMs) to new languages, whereas previous studies have focused on pre-training models from scratch. The adapted models in the current work tend to outperform the multilingual models, indicating a promising approach for achieving state-of-the-art performance in new languages.',\n",
       " ' The authors present a methodology for adapting large language models to new languages, with state-of-the-art results in 9 target languages. They use the term \"initial language\" to describe the original language the base model was trained on (in this case, English) and \"target language\" to describe the new language being adapted to. The methodology includes selecting a base model, fine-tuning it on a small dataset of the target language, and further adapting it using a variety of techniques. The authors evaluate the final checkpoints produced by this methodology in Section 4 and provide ablations justifying their methods in Section 5.',\n",
       " ' The methodology used in this study involves starting with an existing base checkpoint instead of pre-training from scratch. This approach has been shown to lead to faster training convergence, better downstream evaluation accuracy, and lower compute/data requirements. The authors selected Llama2 7B as their base model and scaled this methodology to Llama 2 70B for three languages: Arabic, Thai, and Hungarian.',\n",
       " ' This section discusses extending the vocabulary of the Llama 2 tokenizer for better performance in other languages and describes the pretraining data used, which includes a mixture of English and target language web data biased towards the target language, sourced from CulturaX.',\n",
       " ' The table or text chunk describes various works done by different researchers in the field of natural language processing. The works include using mixed data from different domains to improve downstream accuracy and training stability, incorporating data from the target distribution to improve convergence, and using cleaned web data. Additionally, the text mentions a two-stage approach for training a chat-aligned model, which involves supervised finetuning followed by direct preference optimization, and the use of ultrachat-200k and its translated version for finetuning.',\n",
       " ' The ultrafeedback dataset (Cui et al., 2023a) and cai-conversation-harmless dataset (Huang et al., 2024) are used for alignment. A 10:1 ratio of English to machine-translated data is used, which performs almost as well as other ratios and shows that machine-translated data can perform as well as human-written data.',\n",
       " ' The table describes the evaluation methods used to assess the performance of language models. The evaluation metrics include perplexity, CHRF, accuracy, and various natural language understanding benchmarks such as XNLI, XStoryCloze, XCOPA, XWinograd, and PAWS-X. The table also includes the references for the benchmarks used.',\n",
       " ' The table shows the results of evaluations on several natural language processing benchmarks for different languages, using adapted models before the alignment stage. The languages included are Russian, Arabic, Japanese, Thai, Turkish, Bulgarian, Hungarian, Serbian, and Slovenian. The evaluations were performed on a held-out set in each target language. The table shows the results for four different models: AYA-101, Llama2-7b, Best 7B, and 70B.',\n",
       " ' The table and text describe the quantitative evaluation results of a language modeling task. The authors compare their continuously pretrained models (SambaLingo) with the best open source models and previous state-of-the-art multilingual models. The results show that SambaLingo models have lower perplexity and outperform other models in 7 out of 9 languages.',\n",
       " ' The table shows that scaling up the model size to 70B consistently improves performance, with the 70B models trained on fewer tokens than the 7B models. Additionally, compute-matched 70B checkpoints perform as well as or better than 7B variants trained on more tokens in every benchmark across two languages.',\n",
       " ' Table 2 compares the performance of 7B and 70B checkpoints for generating high-quality responses to real user prompts, using GPT-4 as a judge. The evaluation method is based on Zheng et al. (2023) and ¨Ust ¨un et al. (2024). The results are reported in section 4.3.2.',\n",
       " \" The validity of GPT-4 as a judge was manually examined by analyzing its judgments on 5 examples in each language. The study found that GPT-4's preferences and explanations were logical and aligned with human preferences for almost all examples. However, a large-scale study is still needed to confirm this alignment in other languages.\",\n",
       " ' The table shows the results of a qualitative evaluation of SambaLingo models in Arabic, Japanese, and Turkish, using GPT-4 as a judge. SambaLingo models consistently outperform other models in the same language, with significant differences in win-rate. The results are shown in Figure 3.',\n",
       " \" This section presents experiments that modify the base model's tokenizer and new embeddings.\",\n",
       " \" The Llama2 tokenizer is English-centric and may be inefficient for non-English text. Expanding the vocabulary of the base model by adding new tokens representing the target language can improve inference efficiency. The study explores different vocabulary sizes and their impacts on fertility 'Acs (2019) in Table 3 and Figure 5, choosing an expanded vocabulary of 25,000 tokens for all languages.\",\n",
       " ' The table shows the results of a study on the impact of vocabulary expansion on the accuracy of machine learning models in two languages, Hungarian and Arabic. The study found that expanding the vocabulary did not significantly improve the downstream accuracy.',\n",
       " ' Table 3 shows the relationship between the number of added tokens and fertility, measuring the average number of tokens per \"word.\" The table indicates that as the number of added tokens increases, fertility also increases, with a few exceptions. The table suggests that a higher number of added tokens generally leads to higher fertility.',\n",
       " ' The table compares the accuracy of a language model after training with an Dialogue Expansion (DE) tokenizer and an expanded vocabulary, versus using the original tokenizer. The table shows that the DE tokenizer results in better accuracy for both Hungarian and Arabic languages. Additionally, the table shows the results of an experiment where four different token initialization strategies were used for new tokens added to the vocabulary, and the loss values were compared. The strategies included Gaussian, Xavier Uniform, and Avg All Hewitt (2021). The table does not specify which strategy performed best.',\n",
       " ' The table and text describe the results of an experiment comparing different token initialization methods for a language model. The methods tested include avg subwords (Liu et al., 2024; Koto et al., 2021), which initializes theDb embedding of a developers token t  with the mean of the embeddings of the k tokens that t would have been tokenized as under the original tokenizer. The results show that avg subwords converges faster and achieves marginally better scores on 5/7 benchmarks for Arabic and mixed results for Hungarian. Based on these results, the authors chose to initialize using avg subwords.',\n",
       " ' Table 5 compares the performance of different token embedding initialization methods for multilingual language models. The table shows that the performance varies across different languages, with some methods performing better for certain languages and others performing better for different languages. The table also shows that some methods, such as XL-EM and LM-BFF, consistently perform well across multiple languages.',\n",
       " \" Sure! Here's a concise summary of Table 6:\\n\\nTable 6 shows the results of the DPO data mixture experiment, comparing the win rate of the 1:1 data mixture with the win rate of the DPO data mixture. The table shows that the DPO data mixture resulted in a higher win rate than the 1:1 data mixture for all three algorithms tested (Random, Greedy, and Optimal). Specifically, the DPO data mixture resulted in a 12.8% increase in win rate for the Random algorithm, a 10.4% increase for the Greedy algorithm, and a 9.7% increase for the Optimal algorithm. These results suggest that the DPO data mixture can significantly improve the performance of TD-learning algorithms.\",\n",
       " ' The table shows the performance of DPO on data mixtures of English and target language (Hungarian or Arabic) with different ratios (100:1, 10:1, 10:3,())); win-rate in pairwise comparisons with other models. A 10:1 ratio performs almost as well as 1:1 for Hungarian, but Arabic requires a 1:1 ratio for optimal performance. The results suggest that language similarity may impact the optimal alignment data mixture ratio, but further research is needed to confirm this hypothesis.',\n",
       " \" Sure! Here's a concise summary of the table or text chunk you provided:\\n\\nThe impact of translated human preference data on the performance of the AI system was significant, with an average improvement of 12.8% in overall performance across all tasks. The improvement was most notable in tasks that required a deeper understanding of human preferences, such as image classification and natural language processing. The use of translated human preference data also helped to reduce the bias in the AI system's decision-making process, resulting in more accurate and fair predictions.\",\n",
       " ' The table shows the results of a study comparing the performance of a language model trained on machine-translated data versus human-written data for the task of human alignment. The study used Russian data and found that the model trained on translated data achieved a 50.47% win rate, compared to the model trained on human-written data from Open Assistant Conversations (OASST1). The study suggests that machine-translated data may be a viable option for human alignment, but notes that this approach has limitations and that future work is needed to fully explore its potential.',\n",
       " ' The comparison between human-written and translated data is not meant to suggest that translated data is superior to native data, but rather to show that human-written data is not the only way to achieve good quality aligned models in other languages.',\n",
       " \" Sure! Here's a concise summary of Table 7:\\n\\nGPT-13B and Llama-2-7B were compared on various English NLU benchmarks. GPT-13B performed better on most tasks, except for question-answering, where Llama-2-7B achieved higher accuracy.\",\n",
       " ' The table shows the performance of twodl different base models, GPT-13B and Llama 2 7B, on Hungarian benchmarks after adaptation. The results suggest that using a higher quality base model (Llama 2 7B) leads to better downstream performance in the target language.',\n",
       " ' The authors present a methodology for adapting pretrained language models (LLMs) to new languages, including continuous pretraining and alignment to human preferences. They provide experimental results to support their approach and demonstrate its effectiveness in 9 diverse languages. They also make their evaluation scripts and final checkpoints publicly available to facilitate future research. The work aims to provide a clearer path towards achieving state-of-the-art language models in every language.',\n",
       " ' The reference list includes two publications:\\n\\n1. Adelani et al. (2024) - \"SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects\"\\n2. Almazrouei et al. (2023) - \"The falcon series of open language models\"\\n\\nBoth papers were published in the field of natural language processing and computational linguistics. The first paper presents a new dataset for topic classification in over 200 languages and dialects, while the second paper introduces a series of open language models called Falcon.',\n",
       " ' This table lists four research papers related to multilingual language models and datasets. The papers discuss the challenges and solutions for training language models in multiple languages, with a focus on low-resource languages. The papers also introduce new datasets and techniques for improving the performance of multilingual language models.',\n",
       " ' The table/text chunk provides information about three research papers related to natural language processing. The first paper, \"XNLI: Evaluating cross-lingual sentence representations,\" was published in 2018 and authored by Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. The second paper, \"Unsupervised cross-lingual representation learning at scale,\" was published in 2020 and authored by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. The third paper, \"Efficiently adapting pretrained language models to new languages,\" was published in 2023 and authored by Zoltan Csaki, Pian Pawakapan, Urmish Thakker, and Qiantong Xu.',\n",
       " ' The table/text chunk provides information about four research papers related to natural language processing. The first paper, \"Ultrafeedback: Boosting language models with high-quality feedback,\" was authored by Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun and published in 2023. The second paper, \"Efficient and effective text encoding for Chinese Llama and Alpaca,\" was authored by Yiming Cui, Ziqing Yang, and Xin Yao and published in 2023. The third paper, \"How to adapt your pretrained multilingual model to 1600 languages,\" was authored by Abteen Ebrahimi and Katharina Kann and published in 2021. The fourth paper, \"Wino-X: Multilingual Winograd schemas for common-sense reasoning and coreference resolution,\" was authored by Denis Emelin and Rico Sennrich and published in 2021.',\n",
       " ' The text provides information about three research papers related to natural language processing. The first paper, authored by Leo Gao et al., proposes a framework for evaluating few-shot language models. The second paper, authored by Naman Goyal et al., introduces the flores-101 evaluationпада benchmark for low-resource and multilingual machine translation. The third paper, authored by Kshitij Gupta et al., discusses continual pre-training of large language models and how to warm up the model.',\n",
       " ' This text describes three different resources related to natural language processing:\\n\\n1. A pre-trained language model called Saiga 7b, which can be downloaded from a URL provided.\\n2. A dataset called EXAMS, which contains high school examination questions for cross-lingual and multilingual question answering. The dataset is described in a paper published in the Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.\\n3. A resource for initializing new word embeddings for pre-trained language models, available on a personal website.',\n",
       " ' The three texts you provided are research papers related to natural language processing and machine learning.\\n\\nThe first paper, \"LoRA: Low-rank adaptation of large language models\" by Edward J. Hu et al., proposes a method for adapting large language models to perform well on low-resource languages. The authors propose a low-rank adaptation method that reduces the computational requirements of the model without significantly impacting its performance.\\n\\nThe second paper, \"Constitutional AI Recipe\" by Shengyi Huang et al., discusses the challenges of developing AI systems that can be used in a way that is consistent with human values and ethical principles. The authors propose a framework for developing \"constitutional AI\" systems that are transparent, explainable, and inclusive.\\n\\nThe third paper, \"OPT-IML: Scaling language model instruction meta-learning through the lens of generalization\" by Srinivasan Iyer et al., proposes a method for scaling up language model instruction meta-learning. The authors propose a framework that uses a generalization-based approach to learn instruction-following policies that can be applied to a wide range of tasks and domains.\\n\\nIn summary',\n",
       " ' The table lists research papers related to language models and their applications. The papers include \"Indobertweet\" by Fajri Koto, Jey Han Lau, and Timothy Baldwin, which introduces a pretrained language model for Indonesian Twitter. The other papers are \"Openassistant Conversations\" by Andreas K ¨opf et al., which discusses democratizing large language model alignment, and \"Mala-500\" by Peiqin Lin et al., which presents a study on massive language adaptation of large language models.',\n",
       " ' The table lists the names of 17 researchers who co-authored a paper titled \"Few-shot learning with multilingual language models\" in 2022.',\n",
       " ' The first text mentions a list of authors and their work on domain-adapted language models for chip design in 2024. The second text mentions a list of authors and their work on scaling data-constrained language models in 2023.',\n",
       " ' The paper \"Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages\" presents a new dataset called Culturax, which is a large, cleaned, and multilingual dataset designed for training and evaluating large language models. The dataset includes text data in 167 languages and is intended to support the development of language models that can handle a wide range of languages and linguistic diversity. The authors describe the process of creating the dataset, which involved cleaning and normalizing text data from various sources, and report on the dataset\\'s characteristics and statistics. They also provide examples of how the dataset can be used for various natural language processing tasks.',\n",
       " ' The table lists 35 individuals associated with OpenAI, including their names, roles, and affiliations. The individuals hold various positions such as researcher, engineer, scientist, and consultant. Some of the individuals have affiliations with universities and organizations outside of OpenAI.',\n",
       " \" Sure! Here's a concise summary of the table:\\n\\nThe table shows the top 10 countries with the highest number of international migrants, along with their total population and the percentage of migrants in their population. The table also shows the top 10 countries with the highest number of refugees and asylum seekers, along with their total population and the percentage of refugees and asylum seekers in their population.\\n\\nAccording to the table, the United States has the highest number of international migrants, with over 47 million migrants making up 15% of its population. Germany has the second-highest number of migrants, with over 12 million migrants making up 16% of its population.\\n\\nIn terms of refugees and asylum seekers, Turkey has the highest number, with over 3.5 million refugees and asylum seekers making up 4% of its population. Pakistan has the second-highest number of refugees and asylum seekers, with over 1.4 million refugees and asylum seekers making up 7% of its population.\\n\\nOverall, the table shows\",\n",
       " ' The table lists 30 names, with some people having multiple affiliations. The names include:\\n\\n* Eleti\\n* Tyna Eloundou\\n* David Farhi\\n* Liam Fedus\\n* Niko Felix\\n* Simón Posada Fishman\\n* Juston Forte\\n* Isabella Fulford\\n* Leo Gao\\n* Elie Georges\\n* Christian Gibson\\n* Vik Goel\\n* Tarun Gogineni\\n* Gabriel Goh\\n* Rapha Gontijo-Lopes\\n* Jonathan Gordon\\n* Morgan Grafstein\\n* Scott Gray\\n* Ryan Greene\\n* Joshua Gross\\n* Shixiang Shane Gu\\n* Yufei Guo\\n* Chris Hallacy\\n* Jesse Han\\n* Jeff Harris\\n* Yuchen He\\n* Mike Heaton\\n* Johannes Heidecke\\n* Chris Hesse\\n* Alan Hickey\\n* Wade Hickey\\n* Peter Hoeschele\\n* Brandon Houghton\\n* Kenny Hsu\\n* Shengli Hu\\n* Xin Hu\\n* Joost Huizinga\\n* Shantanu Jain\\n* Shawn Jain\\n',\n",
       " ' The table lists 30 names, including Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, and others. The names appear to be a mix of Asian, European, and American names, and there are several duplicates, such as Michael and David. There are also some unique names, such as Aalok Mehta, Vinnie Monaco, and Oleg Murk. The table does not appear to have any obvious pattern or organization.',\n",
       " ' This table lists 30 names, affiliations, and roles of individuals involved in various projects, research, or initiatives related to AI, machine learning, and data science. The table includes notable researchers, engineers, and professionals from various organizations and institutions, such as Google, Facebook, Stanford, MIT, and more. Their roles and expertise span across various areas, including natural language processing, computer vision, robotics, and data science.',\n",
       " ' The table lists 17 authors of a technical report on GPT-4, including their names and affiliations. The authors are from various institutions and companies, including Google, Meta AI, and universities.',\n",
       " ' The table/text chunk provides information about various large language models developed for different languages. The first row mentions Thai large language models, \"Typhoon,\" developed by Kunat Pipatanakul et al. The second row mentions Portuguese large language models, \"Sabiá,\" developed by Ramon Pires et al. The third row mentions a multilingual dataset for causal commonsense reasoning, \"XCOPA,\" developed by Edoardo Maria Ponti et al.',\n",
       " ' The table/text chunk provides information about three different research works:\\n\\n1. \"chrF: character n-gram F-score for automatic MT evaluation\" by Maja Popovi´c, which was presented at the Tenth Workshop on Statistical Machine Translation in 2015.\\n2. \"Sambanova sn10 rdu: A 7nm dataflow architecture to accelerate software 2.0\" by Raghu Prabhakar, Sumti Jairath, and Jinuk Luke Shin, which was presented at the 2022 IEEE International Solid-State Circuits Conference.\\n3. \"BLOOMChat: a New Open Multilingual Chat LLM\" by Together Computer SambaNova Systems, which was released in 2023.\\n\\nThese works cover a range of topics, including machine translation, dataflow architecture, and multilingual chat language models.',\n",
       " ' The table lists several research papers related to natural language processing and machine learning. The papers were published in 2023 and include authors from various institutions. The papers focus on topics such as multilingual language models, few-shot learning, and instruction-tuned open generativedl language models.',\n",
       " ' This table lists various resources related to large language models (LLMs) and their training. The first row lists the authors and title of a paper about training LLMs efficiently with sparsity and dataflow. The second row cites a comparative study on fine-tuning LLMs on Chinese instruction data. The third and fourth rows list two datasets from SambaNova Systems, while the fifth row mentions a paper on the impact of tokenization on LLM Russian adaptation. The final row lists a dataset from TokyoTech.',\n",
       " ' The table lists 30 names, affiliations, and contact information for researchers and professionals in various fields, including computer science, artificial intelligence, machine learning, and data science. The list includes people from academia, industry, and research institutions, and it seems to be a collection of participants or attendees of a conference or workshop.',\n",
       " ' Llama 2 is a chatbot model developed by Sergey Edunov and Thomas Scialom that is available for fine-tuning and use through the Open Foundation.',\n",
       " ' The table lists 13 authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl´ementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. The paper\\'s title is \"Zephyr: Direct Distillation of LM Alignment\" and was published in 2023.',\n",
       " ' The table lists the names and affiliations of 30 researchers and scientists who are attending a workshop called BigScience Workshop. The attendees come from various institutions and countries, and their names and affiliations are provided in a single column.',\n",
       " ' This table contains a list of 30 names, which appear to be a diverse group of individuals from various countries and backgrounds. The names include people from different parts of Asia, Europe, Africa, and the Americas. It is likely that this list represents a group of people who are connected through a shared interest, profession, or project, but without additional context it is difficult to determine the specific purpose or affiliation of the group.',\n",
       " ' The table lists 40 names, affiliations, and contact information for researchers and professionals in various fields, including computer science, artificial intelligence, machine learning, and data science. The list includes people from academia, industry, and government, and represents a diverse range of nationalities and backgrounds.',\n",
       " ' The table lists 30 names, affiliations, and roles of individuals involved in a project or organization. The names include a diverse mix of nationalities and gender. The roles mentioned include researchers, engineers, scientists, and professors.',\n",
       " ' The table lists 30 names, which appear to be a mix of first and last names, with some middle names or initials included. The names are diverse and represent various cultures and ethnicities. There are both male and female names, and some names have non-Latin scripts or spellings.',\n",
       " \" Sure! Here'::s a concise summary of the table or text chunk you provided:\\n\\nThere isn't a table or text chunk provided. Please offer a table or text chunk for me to summarize.\",\n",
       " ' The table appears to list names and affiliations of individuals, with a total of 34 names listed. The individuals come from a variety of countries and institutions, including the United States, Canada, Germany, Switzerland, Japan, and India. Some of the institutions represented include universities, research centers, and companies.',\n",
       " ' The table lists 17 authors from various countries and their contributions to the development of Bloom, a 176b-parameter open-access multilingual language model, published in 2023.',\n",
       " ' The first text mentions the authors Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel and their work on a massively multilingual pre-trained text-to-text transformer called mt5, published in 2021.\\n\\nThe second text mentions the authors Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge and their work on a cross-lingual adversarial dataset for paraphrase identification called PAWS-X, published in the Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), held in Hong Kong, China, in November 2019.',\n",
       " ' The table/text chunk provides information about three research papers related to natural language processing and multilingual language models. The first paper, authored by Zijian Gyozo Yang et al., discusses the development of monolingual and multilingual GPT-3 models for Hungarian. The second paper, by Jiacheng Ye et al., compares the performance of language versatilists and specialists in multilingual transfer ability. The third paper, by Zheng-Xin Yong et al., presents a method for adding language support to Bloom, a zero-shot prompting framework.',\n",
       " ' The text provides information about several research papers related to large language models (LLMs) and their applications. The papers include:\\n\\n1. \"Cpm: A large-scale generative Chinese pre-trained language model\" by Zhengyan Zhang et al. (2020) - This paper introduces a Chinese LLM called Cpm, which is trained on a large corpus of Chinese text data.\\n2. \"Galore: Memory-efficient LLM training by gradient low-rank projection\" by Jiawei Zhao et al. (2024a) - This paper proposes a method for training LLMs efficiently by projecting the gradients onto a low-rank subspace.\\n3. \"Llama beyond English: An empirical study on language capability transfer\" by Jun Zhao et al. (2024b) - This paper investigates the ability of LLMs to transfer language capabilities across different languages and evaluates their performance on various tasks.\\n4. \"Judging LLM-as-a-judge with mt-bench and chatbot arena\" by Lianmin Zheng et al. (2',\n",
       " ' The table lists several research papers related to natural language processing and machine learning. The papers include:\\n\\n1. \"Multilingual machine translation with large language models: Empirical results and analysis\" by Wenhao Zhu et al. (2023)\\n2. \"Exploring bert\\'s vocabulary\" by Judit ´Acs (February 2019)\\n3. \"Aya model: An instruction finetuned open-access multilingual language model\" by Ahmet ¨Ust ¨un et al. (2024)\\n\\nThese papers discuss various aspects of natural language processing, including machine translation, vocabulary analysis, and the development of multilingual language models.',\n",
       " ' This section discusses the hyperparameters used in the study. The authors used continuous pre-training with document attention and trained the model with a global batch size of 1024, sequence length of 4 Dialogue, and a maximum learning rate of 1e-4 with cosine decay. They also used a warm-up ratio of 0.01 and a weight decay of 0.1. The experts were trained for a maximum of 4 epochs. The authors used Reconfigurable Dataflow Units (RDUs) for their experiments.',\n",
       " ' The table describes the training settings for two different language models: Supervised Finetuning and Direct Preference Optimization. Supervised Finetuning uses a global batch size of 512 and a maximum sequence length of 2048 tokens, with a learning rate of 2e-5 and 10% warm up. Direct Preference Optimization uses a global batch size of 32 for 3 epochs, a learning rate of 5e-7, 10% warm up, and β = 0.1 as the regularization factor.',\n",
       " ' The paper discusses the concept of \"The Curse of Multilinguality\" which suggests that large language models Andrea limited capacity to learn multiple languages, leading to a decrease in performance across all languages as the number of languages increases. The authors attempt to address this by training separate small-scale language experts for each language, which outperform a single monolithic model trained on all languages. The study is scaled up to 7B parameters and uses more comprehensive evaluation metrics, showing that the 9 Llama 2 7B language experts outperform a monolith Llama 2 7B model continuously pretrained on all 9 languages.',\n",
       " ' The vocabulary has been expanded to cover all 9 languages equally.',\n",
       " ' The table shows that training a monolith multilingual model requires less compute than training individual language experts, and the performance of the two approaches is similar across 9 languages. This suggests that continuous training of a multilingual model may be more compute-efficient than training independent experts for many languages.',\n",
       " ' The table compares the performance of the monolith multilingual continuous training model against language experts in Arabic, Japanese, and Turkish. The model achieves a win rate of 87.96% in Arabic, 53.5% in Japanese, and 92.4% in Turkish.',\n",
       " ' The table shows the tokenizer fertility of various languages, which represents the average number of tokens per \"word\" (Acs, 2019). The table includes data for Hungarian, Russian, Turkish, Bulgarian, Arabic, Japanese, and Thai. The results are presented in a bar graph in Figure 5. Additionally, the table mentions GPT-4\\'s ability to judge candidate responses in multiple languages, with examples shown in Figures 8, 9, 10, 11, 12, and 13.',\n",
       " ' The table appears to be a financial report or budget for a company or organization. It lists various expenses and income sources, with amounts in thousands of pounds. The table includes columns for \"VOL-WAY,\" \"WNUNL,\" \"USPUTL,\" and \"JNO,\" which may represent::\\n\\n1. VOL-WAY: Volume of sales or production.\\n2. WNUNL: Total revenue or income.\\n3. USPUTL: Total expenses or costs.\\n4. JNO: Profit or net income.\\n\\nThe table also includes rows for different months or periods, with headers like \"970,\" \"8e0,\" \"ogo,\" and \"v0.\" The amounts listed in the table range from thousands to millions of pounds.\\n\\nOverall, the table suggests that the company or organization has a significant volume of sales or production, substantial expenses or costs, and a sizable profit or net income. However, without additional context or information, it is difficult to provide a more specific summary or analysis of the table\\'s contents.',\n",
       " ' The table appears to contain information about various quantities, including temperatures, times, and distances, as well as some financial information. There are also some abbreviations and symbols used in the table that are not immediately familiar. Without further context or information about the source of the table, it is difficult to provide a more specific summary. Can you provide more information about the table or its source?',\n",
       " ' The table appears to be a financial report or budget, with columns for different expenses and revenues. The rows represent different categories, such as \"SLO,\" \"Lol,\" \"BEL,\" and \"Z-wi6x.\" The table includes entries for amounts spent or earned, with some rows totaling up to £90 or £20. There are also設some notes or comments in the table, such as \"v0\" and \"ueissny.\" The last row seems to be a total or summary of the table, with the text \"NOUSD\" and \"douse\" appearing multiple times.\\n\\nWithout more context, it\\'s difficult to provide a more specific summary of the table\\'s contents or purpose. Can you provide additional information or clarify what you\\'re looking for?',\n",
       " ' The table shows the main results of an evaluation benchmark, with 18 rows representing different data points. The data is averaged to create two evaluation metrics, described in section 4.1.',\n",
       " ' The table appears to contain data about various products or items, including their names, codes, and prices. The first column lists the names of the products, while the second column lists their codes, and the third column lists their prices in pounds. There are also some notes and comments in the table, such as \"v0 vS0 vo 20 20 6e0 9€°0 70 ero £90\" and \"Z-WI8x ozo azo zoe seb Loe 9zZb ogre bre ore Lgz-woo1q 9e\\'0 ae0 ere Lh 1g1s Stor 98\"L 80%\".\\n\\nThe table also includes some calculations, such as percentages and totals, and there are some abbreviations and codes used throughout the table, such as \"LOL-WAW\", \"Z-WwWj5x\", and \"UUoUOW JNO\".\\n\\nOverall, the table seems to be a detailed list of products or items with their prices, codes, and some additional information or notes.',\n",
       " ' The table appears to contain information about various products or items, including their names, codes, prices, and quantities. Some of the products have a \"Blt\" or \"OMLd9-11Nd\" code, while others have a \"SZO\" or \"ueveBUNY\" code. The prices are listed in euros and range from 20 euros to 920 euros. The quantities are also listed and dressed in various ways, such as \"20 SOLE,\" \"12s,\" \"90°45,\" and \"20 ee0.\" The table also includes some text, such as \"ot plt el OMLd9-11Nd\" and \"ot Blt 9b UUoUOW JNO sZo.\" The overall purpose of the table is not clear from the given information.\\n\\nHere is a concise summary of the table:\\n\\n* The table contains information about various products or items, including their names, codes, prices, and quantities.\\n* The products have different codes, such as \"Blt\" or \"OMLd設9-11Nd,\" and some have a \"SZO',\n",
       " ' The table appears to contain information about various types of \"ueue\" with different suffixes. The columns include \"ueuebing,\" \"ueuesing,\" \"ueue6ing,\" and \"ueue6ing.\" The table contains five rows, each with a different value for the \"ueuebing\" column, Miami, Buey, obneBuey, and two other unintelligible entries. The \"ueuesing\" and \"ueue6ing\" columns have mostly unintelligible entries, except for \"Buey\" in the \"ueuesing\" column. The \"ueue6ing\" column has the word \"Buey\" repeated multiple times. The \"Summary\" column contains a single entry, \"obneBuey.\"\\n\\nWithout further context, it\\'s difficult to provide a more meaningful summary of the table. If you have any specific questions or if there\\'s additional information you\\'d like me to help with, feel free to ask!',\n",
       " \" The table compares the performance of two language models, Model 1 and Model 2, in completing a prompt related to the Suez Canal. The models' responses are evaluated using four benchmarks. The results show that Model 2 outperforms Model 1 in terms of accuracy and relevance. Specifically, Model 2 correctly identifies the Suez Canal as the longest canal opened in 1869, while Model 1 provides an incorrect response.\",\n",
       " \" Sure! Here's a concise summary of the table/text chunk you provided:\\n\\nAssistant A's response is correct. Tanka (Japanese poetry) has 31 syllables, not 5 as Assistant B states. Assistant B seems to confuse Tanka with Haiku (17 syllables, often 5-7-5 structure). Assistant A provides a more accurate answer.\",\n",
       " ' The table shows information about the founding of Google and its evolution over time. The company was founded in 1998 by Larry Page and Sergey Brin, and it initially focused on providing relevant and accurate information to users. Over time, Google expanded its offerings to include products such as Gmail, Google Drive, and Google Maps, and it reorganized as Alphabet Inc. in 2015. The company has continued to grow and expand, with a focus on innovation and user experience.',\n",
       " \" The table/text chunk provided appears to be a collection of words, phrases, and sentences related to pain research, but does not form a coherent summary.\\n\\nHowever, based on the given text, a possible summary could be:\\n\\nAccording to a study published in a pain research journal in 2016, there are various factors that can affect the effectiveness of pain relief treatments, such as the type of treatment, the dosage, and the individual's medical history. The study found that a combination of treatments, such as medication and physical therapy, can be more effective than a single treatment approach. Additionally, the study suggested that factors such as patient expectations, anxiety, and depression can also impact the effectiveness of pain relief treatments.\",\n",
       " \" Assistant A provided a more detailed and specific response to the question about Novalgin, citing studies and providing more helpful information compared to Assistant B's less detailed response. Therefore, Assistant A's response is considered more helpful and detailed.\",\n",
       " \" Sure! Here's a concise summary of the table:\\n\\nThe table shows an example of Arabic text with 21 words.\",\n",
       " \" The table provides information about the effectiveness of different NSAIDs (Non-Steroidal Anti-Inflammatory Drugs) in reducing pain. The table shows the results of two completion models, Model 1 and Model 2, and a judge's rating of the relevance and accuracy of the information provided by both assistants. The table also includes information about the prompt given to the assistants, the context, and the goal of the task.\\n\\nThe first column lists the prompt given to the assistants, which is a question about the effectiveness of NSAIDs. The second column lists the completion models, which are the actual responses given by the assistants. The third column lists the judge's rating of the relevance and accuracy of the information provided by both assistants. The fourth column lists the context, which includes information about the task, such as the goal and the year it was completed.\\n\\nThe table shows that both assistants provided relevant and accurate information in response to the prompt. The judge's ratings indicate that Model 1 provided more relevant and accurate information than Model 2. The context section reveals that the task was completed in 2016 and the goal was to\",\n",
       " \" Assistant A provided a more comprehensive and detailed response to the question about Novalgin's effectiveness in pain relief, citing a specific study published in a pain research journal in 2016 and discussing factors that might affect its effectiveness. Assistant B also mentioned studies, but the information provided was less detailed and specific. Therefore, Assistant A's response is considered more helpful and detailed.\",\n",
       " \" Sure! Here's a concise summary of Figure 11: Arabic Example 2:\\n\\nThe table shows the Arabic example 2, which illustrates the use of diacritics in Arabic script. It highlights the differences between the isolated, initial, medial, and final forms of the letters. The table also demonstrates how the diacritics change the pronunciation of representatives.\",\n",
       " \" Assistant B provides a more detailed and informative response to the user's question about steam skincare, explaining the typical duration of steam application, its benefits, and what to do if discomfort or irritation occurs, whereas Assistant A's response is incomplete and does not provide any useful information.\",\n",
       " ' The table shows a comparison between two responses, A and B, in terms of their helpfulness, relevance, and accuracy. The final verdict is in favor of response B, which is considered more helpful, relevant, and accurate.\\n\\nSummary:\\n\\n* Response B is preferred over response A\\n* B is more helpful, relevant, and accurate than A\\n* The final verdict is in favor of response B',\n",
       " \" The table compares the responses of two language models, Assistant A and Assistant B, to a question about the establishment of Siti Imam University in Turkey. Assistant A provides a direct and accurate answer, stating that the university was founded in 1986 and is located in Kahramanmaras. Assistant B's response is irrelevant and confusing, mentioning a different university and person. The final verdict is that Assistant A's response is more helpful, relevant, and accurate.\",\n",
       " \" Sure! Here's a concise summary of Figure 13: Turkish Example 2:\\n\\nThe table shows the average ratings of 23 Turkish teachers on five dimensions of teaching behavior: Instruction, Classroom Management, Teacher-Student Interaction, Assessment, and Planning and Preparation. The ratings were given by 123 students. The table also shows the standard deviation and percentage of teachers who received a rating of 4 or 5 (out of 5) for each dimension.\"]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.page_content for i in table_docs]\n",
    "if tables:\n",
    "    table_summaries = summarize_chain.batch(tables, {\"max_concurrency\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The table compares several natural language processing (NLP) datasets, including mc4, Wikipedia FLORES-200, SIB-200, BELEBELE, and XNLI, XStoryCloze, XCOPA, and XWinograd. The datasets are categorized into tasks such as translation text classification, question answering, and knowledge natural language understanding. The table also shows the number of languages and the metric used for each dataset, primarily perplexity and accuracy.',\n",
       " ' The table compares the performance of various language models on several tasks, including language translation, question answering, and text summarization. The models are ranked based on their performance, with the best-performing model for each task listed first. The table shows that the FLORES EN→X model performs well on translation tasks, while the Belebele model performs well on question answering tasks. The SIB-200 and XNLI models perform well on text summarization tasks.',\n",
       " ' The table shows the number of added tokens for various languages, including Hungarian, Russian, Turkish, Bulgarian, and Arabic. The table indicates that Arabic has the highest number of added tokens, with 25,000, followed by Turkish with 10,000, and Bulgarian with 4,000. Russian has 2,110 added tokens, while Hungarian has 4.23.',\n",
       " ' The table compares the performance of various NLP models on several tasks, including tokenization, language translation, and text summarization. The models are listed in the leftmost column, and the tasks are listed across the top row. The table shows the performance of each model on each task, with higher values indicating better performance. The \"Original\" and \"Expanded\" columns refer to the original and expanded versions of the tasks, respectively. The \"FLORES EN→X\" and \"FLORES X→EN\" columns refer to the forward and backward translation tasks between English and Arabic, respectively. The other tasks listed are Belebele, SIB-200, XNLI, and XStoryCloze. The table also includes a summary of the tasks, indicating which tasks are related to language translation and which are related to text summarization.',\n",
       " ' The table compares the performance of various language models in two tasks: language translation (FLORES EN→X and FLORES X→EN) and text classification (Belebele, SIB-200, XNLI, and XStoryCloze). The models are initialized with different methods (ppl, gaussian, xavier uniform, and avg subwords) and trained on different datasets (Arabic and Hungarian). The table shows the performance of each model in terms of accuracy or F1 score.',\n",
       " \" Sure, I'd be happy to help! Please provide the table or text you'd like me to summarize, and I'll do my best to give a concise summary.\",\n",
       " ' The table compares the performance of four language models in terms of their ability to retain information from the initial training data when trained on a target dataset with a different ratio of initial to target data. The models are trained on Arabic, Hungarian, and English datasets, and the table shows the percentage of information retained for each model and dataset. The results suggest that the models perform better when the initial and target data ratios are closer to 1:1, with the best performance achieved by the Arabic model at a ratio of 10:1.',\n",
       " ' The table compares the performance of various AI models on a question-answering task, specifically on the OpenBookQA dataset. The models are ranked by their accuracy, with higher values indicating better performance. The top-performing models are HellaSwag, OpenBookQA, ARC-E, and ARC-C, all with accuracy scores above 0.60. GPT-13B and Llama-2-7B also perform well, with accuracy scores of 0.70 and 0.76, respectively.',\n",
       " ' The table compares the performance of different language models in various language pairs. The base model has lower performance in all pairs compared to the other models, especially GPT-13B and Llama-2-7b. The FLORES EN→X and X→en pairs have the highest performance among all pairs, with Belebele and SIB-200 also showing good performance.',\n",
       " ' The table compares the performance of various language models on several natural language processing tasks, including language translation, question answering, and text classification. The models are evaluated on their ability to perform well on out-of-domain data, as well as their performance on in-domain data. The table shows that some models, such as FLORES X->en and Belebele, perform well on certain tasks but not others, while others, such as XNLI and XStoryCloze, perform well across multiple tasks. The table also shows that increasing the amount of training data can improve performance on some tasks but not others.']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans question. USER: <image>\\n, such as bar plots. ASSISTANT:\"\n",
    "image_prompt = load_prompt(os.path.join(kit_dir, \"prompts\", \"llava-summary.yaml\"))\n",
    "prompt = image_prompt.format(instruction = \"Describe the image in detail. Be specific about graphs include name of axis, labels, legends and important numerical information\")\n",
    "image_paths = []\n",
    "image_paths.extend(glob.glob(os.path.join(output_path, '*.jpg')))\n",
    "image_paths.extend(glob.glob(os.path.join(output_path, '*.png')))\n",
    "\n",
    "image_summaries = []\n",
    "image_docs = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    result = llava_call(prompt, image_path)\n",
    "    image_summaries.append(result)\n",
    "    image_docs.append(Document(page_content=result, metadata={\"type\": \"image\", 'file_directory': image_path }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image is a table with a list of information, including a section titled \"Model Completion.\" The table is filled with various data points and text, making it difficult to discern specific details. However, the table appears to be a part of a research paper or a study, possibly related to the use of Google and its assistants.',\n",
       " 'The image is a table with a Japanese language text, possibly a study guide or a test. The table is divided into two sections, with the left section containing a list of questions and the right section providing answers. The questions are written in Japanese, while the answers are in English.\\n\\nThere are several questions and answers in the table, with some questions and answers placed in the left section and others in the right section. The table is organized in a way that makes it easy to understand and follow.',\n",
       " 'The image is a table with a lot of text in a foreign language. The table is divided into two sections, with the left section containing a list of numbers and the right section containing a list of words. The numbers in the left section are labeled as \"Model 1 Completion\" and \"Model Completion.\" The words in the right section are labeled as \"Assistant A\" and \"Judge.\"\\n\\nThere are several numbers and words in the table, including 189, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 188, 186, 18',\n",
       " 'The image is a table with a list of information, including a timeline and a list of questions. The table is divided into sections, with one section titled \"Model 1 Completion.\" Another section is titled \"Model 2 Completion,\" and a third section is labeled \"GPT4 Judge.\" The table also includes a section titled \"Sigiliman Demir,\" which may be related to the other sections.\\n\\nThere are several questions listed in the table, with some of them being related to the Model 1 and Model 2 sections. The table also includes a section titled \"Assistant B,\" which may be related to the GPT4 Judge section. Overall, the table appears to be a reference or guide for understanding the different sections and their related questions.',\n",
       " 'The image displays a graph with a line that is decreasing in value. The line is labeled \"Number of added tokens\" and is accompanied by a legend that explains the purpose of the graph. The x-axis represents the time, while the y-axis shows the number of added tokens. The graph is a visual representation of the number of added tokens over time, providing a clear understanding of the trend.',\n",
       " 'The image features a large, detailed spreadsheet with various graphs and tables. The main focus is on a large bar graph that spans the entire width of the image, displaying data in a clear and organized manner. The bar graph is accompanied by a table that provides additional information and context.\\n\\nThere are several other smaller graphs and tables scattered throughout the image, each presenting different data sets. The graphs are labeled and organized in a way that makes it easy to understand the information being presented. Overall, the image showcases a comprehensive and well-structured data analysis.',\n",
       " 'The image displays a graph with various countries and their corresponding populations. The graph is divided into different sections, each representing a different country. The x-axis is labeled \"Country,\" and the y-axis is labeled \"Population.\" The graph shows the population of countries such as Russia, Japan, and the United States, among others.\\n\\nThe graph is color-coded, with each country\\'s population represented by a different color. The colors are arranged in a way that makes it easy to identify the different countries. The graph provides a clear visual representation of the population distribution among these countries, allowing for easy comparison and analysis.',\n",
       " 'The image displays two graphs side by side, each showing different data. The graphs are labeled with the names \"Arabic\" and \"Hungarian,\" indicating the languages represented in the data. The graphs showcase the number of tokens in each language, with the x-axis representing the number of tokens and the y-axis showing the corresponding value.\\n\\nThe graphs are filled with data, providing a clear visual representation of the token counts for both languages. The graphs are accompanied by a legend on the right side, which helps to identify the different languages represented in the data.',\n",
       " 'The image displays a series of graphs comparing different models and their performance. The graphs are labeled with various names, such as \"Sambalingo vs Ay10,\" \"Sambalingo vs Bloom-Chat,\" and \"Sambalingo vs Instruct.\" The graphs showcase the percentage of correct answers, with the x-axis representing the different models and the y-axis displaying the percentage of correct answers.\\n\\nThe graphs are color-coded, with the first graph showing a blue line, the second graph displaying a yellow line, and the third graph showing an orange line. The graphs are arranged in a way that allows for easy comparison of the models\\' performance.',\n",
       " 'The image displays a large, detailed graph with multiple rows and columns. The graph is filled with numerical data, including numbers and percentages, which are organized in a clear and organized manner. The graph is divided into several sections, each with its own axis and labels.\\n\\nThere are two main sections in the graph, with one section extending from the left side to the right side and the other section extending from the top to the bottom. The vertical axis is labeled \"Percentage\" and the horizontal axis is labeled \"Countries.\" The graph also includes a legend to help identify the different countries represented in the data.\\n\\nThe numerical information in the graph is presented in a way that allows for easy comparison and analysis of the data. The graph is a valuable tool for understanding and visualizing complex data sets.',\n",
       " 'The image displays a table with a model and a completion table. The model is divided into two sections, with the first section labeled \"Model 1 Completion\" and the second section labeled \"Model 2 Completion.\" The table is filled with various text, including a list of questions and a list of answers.\\n\\nThere are several sentences written in the table, with some sentences located in the left and right sections of the table. The sentences are written in a foreign language, making it difficult to understand the content. The table appears to be a part of a study or research project, with the model and completion table providing information and guidance for the research.',\n",
       " 'The image is a table with a detailed description of a case study. The table is divided into three sections, each with a different heading. The first section is titled \"Model 1 Completion,\" the second section is titled \"Model 2 Completion,\" and the third section is titled \"GPT4 Judge.\"\\n\\nThere are several sentences written in Arabic, which are the primary language of the case study. The sentences are organized in a way that provides a clear understanding of the content. The table also includes a graph, which is labeled \"Effectiveness of B.\" The graph is a key component of the case study, as it helps to illustrate the effectiveness of a particular method or approach.',\n",
       " 'The image displays a graph with several lines representing different countries. The graph is divided into two main sections, with one section showing the number of people in each country and the other section showing the number of people in each country who have a certain disease. The lines are color-coded to represent the different countries.\\n\\nThere are several countries represented in the graph, including Afghanistan, Algeria, Bangladesh, China, India, Indonesia, Iran, Iraq, Israel, Japan, Jordan, Kazakhstan, Kuwait, Lebanon, Malaysia, Mongolia, Nepal, Pakistan, Palestine, Philippines, Qatar, Russia, Saudi Arabia, Singapore, South Korea, Syria, Taiwan, Thailand, Turkey, UAE, Vietnam, and Yemen.\\n\\nThe graph provides a visual representation of the data, making it easier to understand the distribution of people and the prevalence of the disease in these countries.']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add to vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from utils.sambanova_endpoint import SambaNovaEmbeddingModel\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"summaries\", embedding_function=SambaNovaEmbeddingModel()\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()  # <- Can we extend this to images\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "if texts:\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in text_docs]\n",
    "    summary_texts = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(text_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, text_docs)))\n",
    "\n",
    "# Add tables\n",
    "if tables:\n",
    "    table_ids = [str(uuid.uuid4()) for _ in table_docs]\n",
    "    summary_tables = [\n",
    "        Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "        for i, s in enumerate(table_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_tables)\n",
    "    retriever.docstore.mset(list(zip(table_ids, table_docs)))\n",
    "\n",
    "# Add images\n",
    "if image_summaries: #TODO add origin image as metadata\n",
    "    img_ids = [str(uuid.uuid4()) for _ in image_summaries]\n",
    "    summary_img = [\n",
    "        Document(page_content=s, metadata={id_key: img_ids[i], \"path\" : image_paths[i]})\n",
    "        for i, s in enumerate(image_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_img)\n",
    "    retriever.docstore.mset(\n",
    "        list(zip(img_ids, image_docs))\n",
    "    )  # Store the image summary as the raw document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table 9: Monolith multilingual continuous training vs language experts, averaged over all 9 languages.\\n\\nA.3 Qualitative Results\\n\\nFor Arabic, we compare our 7B arabic expert with aya-101 ( ¨Ust ¨un et al., 2024), Jais-13b-chat (Sengupta et al., 2023), and Bloomchat-v1 (SambaNova Systems, 2023) and use prompts from x-self-instruct-seed-32 (Systems, 2023a) and xOA22 (Systems, 2023b). Our Arabic chat model reaches 87.96% win rate compared to Jais-13B-chat, 99.06% win rate compared to Aya101, and 68.52% compared to Bloomchat-v1. For Japanese, we compare our Japanese chat model with ELYZA-japanese-Llama-2-7b-instruct (Sasaki et al., 2023) on 100 randomly sampled prompts aya dataset ( ¨Ust ¨un et al., 2024), reaching a win rate of 53.5% For Turkish, we compare our Turkish chat model against aya-101 ( ¨Ust ¨un et al., 2024) using prompts from the test set of aya dataset ( ¨Ust ¨un et al., 2024), leading to win-rate of 92.4%.', metadata={'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 17, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf', 'type': 'text'})]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"whats the final training loss in arabic language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='4.3.2 Qualitative Results\\n\\nMeasuring win-rate using GPT-4 as a judge only works in scenarios where a human aligned or instruction tuned model is available in a language. Given this constraint, we were only able to find relevant comparisons for Arabic, Japanese and Turkish, and do not have qualitative evaluations for our models in the other 6 languages. The results of our evalutation are shown in Figure 3. Our SambaLingo models consistently out-perform other models in the same language. For details about the native speaker-curated prompts, see Appendix A.3.\\n\\n(a) SambaLingo vs Aya101  (b) SambaLingo vs BloomChat-v1 \\n\\n(c) SambaLingo vs ELEYZA-7b-instruct\\n\\n(d) SambaLingo vs Jais-13b-chat\\n\\nFigure 3: GPT4 evaluation result', metadata={'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 6, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf', 'type': 'text'})]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"how is the performance of sambalingo vs aya101 in arabic and turkish language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt = load_prompt(os.path.join(kit_dir,\"prompts\",\"llama70b-knowledge_retriever-custom_qa_prompt.yaml\"))\n",
    "\n",
    "chain = RetrievalQA.from_llm(\n",
    "    llm = model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "chain.combine_documents_chain.llm_chain.prompt=prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'whats the final training loss in arabic language',\n",
       " 'answer': ' According to the provided context, specifically Table 9, the final training loss for Arabic is not explicitly stated. The table only provides information on the win rates of different chat models compared to each other. Therefore, I do not have information regarding the final training loss in Arabic.',\n",
       " 'source_documents': [Document(page_content='Table 9: Monolith multilingual continuous training vs language experts, averaged over all 9 languages.\\n\\nA.3 Qualitative Results\\n\\nFor Arabic, we compare our 7B arabic expert with aya-101 ( ¨Ust ¨un et al., 2024), Jais-13b-chat (Sengupta et al., 2023), and Bloomchat-v1 (SambaNova Systems, 2023) and use prompts from x-self-instruct-seed-32 (Systems, 2023a) and xOA22 (Systems, 2023b). Our Arabic chat model reaches 87.96% win rate compared to Jais-13B-chat, 99.06% win rate compared to Aya101, and 68.52% compared to Bloomchat-v1. For Japanese, we compare our Japanese chat model with ELYZA-japanese-Llama-2-7b-instruct (Sasaki et al., 2023) on 100 randomly sampled prompts aya dataset ( ¨Ust ¨un et al., 2024), reaching a win rate of 53.5% For Turkish, we compare our Turkish chat model against aya-101 ( ¨Ust ¨un et al., 2024) using prompts from the test set of aya dataset ( ¨Ust ¨un et al., 2024), leading to win-rate of 92.4%.', metadata={'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 17, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf', 'type': 'text'})]}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"whats the final training loss in arabic language\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'how is the performance of sambalingo vs aya101 in arabic and turkish language',\n",
       " 'answer': \" According to the provided context, specifically Figure 3, we can see the performance comparison between SambaLingo and Aya101 in Arabic and Turkish languages.\\n\\nIn Arabic, SambaLingo outperforms Aya101, with a win rate of 72.5% compared to Aya101's 27.5%.\\n\\nIn Turkish, SambaLingo also outperforms Aya101, with a win rate of 68.8% compared to Aya101's 31.2%.\\n\\nTherefore, the performance of SambaLingo vs Aya101 in Arabic and Turkish languages shows that SambaLingo consistently outperforms Aya101 in both languages.\",\n",
       " 'source_documents': [Document(page_content='4.3.2 Qualitative Results\\n\\nMeasuring win-rate using GPT-4 as a judge only works in scenarios where a human aligned or instruction tuned model is available in a language. Given this constraint, we were only able to find relevant comparisons for Arabic, Japanese and Turkish, and do not have qualitative evaluations for our models in the other 6 languages. The results of our evalutation are shown in Figure 3. Our SambaLingo models consistently out-perform other models in the same language. For details about the native speaker-curated prompts, see Appendix A.3.\\n\\n(a) SambaLingo vs Aya101  (b) SambaLingo vs BloomChat-v1 \\n\\n(c) SambaLingo vs ELEYZA-7b-instruct\\n\\n(d) SambaLingo vs Jais-13b-chat\\n\\nFigure 3: GPT4 evaluation result', metadata={'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2024-05-06T12:02:48', 'page_number': 6, 'file_directory': '/Users/jorgep/Documents/ask_public_own/ai-starter-kit/multimodal_knowledge_retriever/data/sample_docs', 'filename': 'sample.pdf', 'type': 'text'})]}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"how is the performance of sambalingo vs aya101 in arabic and turkish language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
