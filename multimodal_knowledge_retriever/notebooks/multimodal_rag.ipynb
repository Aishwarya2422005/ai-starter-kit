{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.sambanova_endpoint import SambaNovaEndpoint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(repo_dir,'.env'))\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_binary = image_file.read()\n",
    "        base64_image = base64.b64encode(image_binary).decode()\n",
    "        return base64_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you are allowed to swim in the lake. The image shows a pier extending over the water, and there are no visible signs or barriers that prohibit swimming. The serene environment and the presence of a pier suggest that it is a suitable location for swimming and enjoying the water.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sambastudio call\n",
    "# Define the endpoint URL and key\n",
    "def llava_call(prompt, image_path):\n",
    "    image=image_to_base64(image_path)\n",
    "    endpoint_url = f\"{os.environ.get('BASE_URL')}/api/predict/generic/{os.environ.get('PROJECT_ID')}/{os.environ.get('ENDPOINT_ID')}\"\n",
    "    endpoint_key = os.environ.get('API_KEY')\n",
    "    # Define the data payload\n",
    "    data = {\n",
    "        \"instances\": [{\n",
    "            \"prompt\": prompt,\n",
    "            \"image_content\": f\"{image}\"\n",
    "        }],\n",
    "        \"params\": {\n",
    "            \"do_sample\": {\"type\": \"bool\", \"value\": \"false\"},\n",
    "            \"max_tokens_to_generate\": {\"type\": \"int\", \"value\": \"100\"},\n",
    "            \"temperature\": {\"type\": \"float\", \"value\": \"1\"},\n",
    "            \"top_k\": {\"type\": \"int\", \"value\": \"50\"},\n",
    "            \"top_logprobs\": {\"type\": \"int\", \"value\": \"0\"},\n",
    "            \"top_p\": {\"type\": \"float\", \"value\": \"1\"}\n",
    "        }\n",
    "    }\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"key\": endpoint_key\n",
    "    }\n",
    "    response = requests.post(endpoint_url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()[\"predictions\"][0]['completion']\n",
    "\n",
    "prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans question. USER: <image>\\nAre you allowed to swim here?. ASSISTANT:\"\n",
    "image_path = os.path.join(kit_dir,\"data/view.jpg\")\n",
    "llava_call(prompt, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you are allowed to swim in the lake near the pier. The image shows a pier extending out into the water, which suggests that it is a popular spot for swimming and other water-related activities."
     ]
    }
   ],
   "source": [
    "#replicate  usage\n",
    "import replicate\n",
    "\n",
    "output = replicate.run(\n",
    "    \"yorickvp/llava-13b:b5f6212d032508382d61ff00469ddda3e32fd8a0e75dc39d8a4191bb742157fb\",\n",
    "    input={\n",
    "        \"image\": f\"data:application/octet-stream;base64,{image_to_base64(os.path.join(kit_dir,'data/view.jpg'))}\",\n",
    "        \"top_p\": 1,\n",
    "        \"prompt\": \"Are you allowed to swim here?\",\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    ")\n",
    "\n",
    "# The yorickvp/llava-13b model can stream output as it's running.\n",
    "# The predict method returns an iterator, and you can iterate over that output.\n",
    "for item in output:\n",
    "    # https://replicate.com/yorickvp/llava-13b/api#output-schema\n",
    "    print(item, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Path to save images\n",
    "path =  os.path.join(kit_dir, \"data/\")\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path + \"SambaNova_Suite_Solution_Brief_06-21-23.pdf\",\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=1000,\n",
    "    new_after_n_chars=800,\n",
    "    combine_text_under_n_chars=500,\n",
    "    image_output_dir_path=path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x2da034550>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2da034520>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2da034160>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2a46b7310>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2da0341f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2d9f886d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2d9f89420>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2d9f89210>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2d9f8b9a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2d9f880a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2d9f891e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2d9f88dc0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x2b339bee0>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 13}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "# TableChunk if Table > max chars set above\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
