{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web crawling RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from tqdm.autonotebook import trange\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from models.sambanova_endpoint import SambaNovaEndpoint\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_htmls(urls):\n",
    "    docs=[]\n",
    "    for url in urls:\n",
    "        #print(url)\n",
    "        loader = AsyncHtmlLoader(url, verify_ssl=False)\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "def link_filter(all_links, excluded_links):\n",
    "    clean_excluded_links=set()\n",
    "    for excluded_link in excluded_links:\n",
    "        parsed_link=urlparse(excluded_link)\n",
    "        clean_excluded_links.add(parsed_link.netloc + parsed_link.path)\n",
    "    filtered_links = set()\n",
    "    for link in all_links:\n",
    "        # Check if the link contains any of the excluded links\n",
    "        if not any(excluded_link in link for excluded_link in clean_excluded_links):\n",
    "            filtered_links.add(link)\n",
    "    return filtered_links\n",
    "\n",
    "def find_links(docs, excluded_links=None):\n",
    "    if excluded_links is None:\n",
    "        excluded_links = []\n",
    "    all_links = set()  \n",
    "    excluded_link_suffixes = {\".ico\", \".svg\", \".jpg\", \".png\", \".jpeg\", \".\"}\n",
    "    \n",
    "    for doc in docs:\n",
    "        page_content = doc.page_content\n",
    "        base_url = doc.metadata[\"source\"]\n",
    "        excluded_links.append(base_url)\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        # Identify the main content section (customize based on HTML structure)\n",
    "        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')\n",
    "        \n",
    "        if main_content:\n",
    "            links = main_content.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                # Check if the link is not an anchor link and not in the excluded links or suffixes\n",
    "                if (\n",
    "                    not href.startswith(('#', 'data:', 'javascript:')) and\n",
    "                    not any(href.endswith(suffix) for suffix in excluded_link_suffixes)\n",
    "                ):\n",
    "                    full_url, _ = urldefrag(urljoin(base_url, href))\n",
    "                    all_links.add(full_url)\n",
    "                    \n",
    "    all_links=link_filter(all_links, set(excluded_links))\n",
    "    return all_links\n",
    "\n",
    "def clean_docs(docs):\n",
    "    html2text_transformer = Html2TextTransformer()\n",
    "    docs=html2text_transformer.transform_documents(documents=docs)\n",
    "    return docs\n",
    "\n",
    "def web_crawl(urls, excluded_links=None, depth = 1):\n",
    "    if excluded_links == None:\n",
    "        excluded_links = []\n",
    "    if depth > 3:\n",
    "        depth = 3\n",
    "    scrapped_urls=[]\n",
    "    raw_docs=[]\n",
    "    for i in range(depth):\n",
    "        scraped_docs = load_htmls(urls)\n",
    "        scrapped_urls.extend(urls)\n",
    "        urls=find_links(scraped_docs, excluded_links)\n",
    "        excluded_links.extend(scrapped_urls)\n",
    "        raw_docs.extend(scraped_docs)\n",
    "    docs=clean_docs(scraped_docs)\n",
    "    return docs, scrapped_urls\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=200, length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(text)\n",
    "    return chunks\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"BAAI/bge-large-en\",\n",
    "        embed_instruction=\"\",  # no instruction is needed for candidate passages\n",
    "        query_instruction=\"Represent this paragraph for searching relevant passages: \",\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    vectorstore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def get_custom_prompt():\n",
    "    custom_prompt_template = \"\"\"<s>[INST] <<SYS>>\\n\"Use the following pieces of context to answer the question at the end. \n",
    "        If the answer is not in context for answering, say that you don't know, don't try to make up an answer or provide an answer not extracted from provided context. \n",
    "        Cross check if the answer is contained in provided context. If not than say \"I do not have information regarding this.\" \n",
    "\n",
    "        context\n",
    "        {context}\n",
    "        end of context\n",
    "        <</SYS>>\n",
    "\n",
    "        Question: {question}\n",
    "        Helpful Answer: [/INST]\"\"\"\n",
    "\n",
    "    CUSTOMPROMPT = PromptTemplate(template=custom_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return CUSTOMPROMPT\n",
    "\n",
    "def get_retriever_qa(vectorstore):\n",
    "    llm = SambaNovaEndpoint(\n",
    "        model_kwargs={\"do_sample\": False, \"temperature\": 0.0},\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.5, \"k\": 4},\n",
    "    )\n",
    "    retrieval_chain = RetrievalQA.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        input_key=\"question\",\n",
    "        output_key=\"answer\",\n",
    "    )\n",
    "    ## Inject custom prompt\n",
    "    retrieval_chain.combine_documents_chain.llm_chain.prompt = get_custom_prompt()\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  1.77it/s]\n",
      "/Users/jorgep/Documents/ask_public_own/test_env/lib/python3.10/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.espn.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  2.94it/s]\n",
      "/Users/jorgep/Documents/ask_public_own/test_env/lib/python3.10/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lilianweng.github.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  1.49it/s]\n",
      "/Users/jorgep/Documents/ask_public_own/test_env/lib/python3.10/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'sambanova.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "filtered_sites = [\"facebook.com\", \"twitter.com\", \"instagram.com\", \"linkedin.com\", \"telagram.me\", \"reddit.com\", \"whatsapp.com\", \"wa.me\"]\n",
    "urls=[\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\", \"https://sambanova.ai/\"]\n",
    "docs, urls=web_crawl(urls, excluded_links=filtered_sites, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.espn.com',\n",
       " 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'https://sambanova.ai/']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "text_chunks = get_text_chunks(docs)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "vectorstore = get_vectorstore(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the language model, and the retrievalQA chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../../web_crawled_data_retriever/export.env\")\n",
    "retrieval_chain=get_retriever_qa(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgep/Documents/ask_public_own/test_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#user_question = \"which are the mars expeditions?\"\n",
    "#user_question = \"what it means planning in an llm agent\"\n",
    "#user_question = \"wich are the games for today?\"\n",
    "#user_question = \"what is the SN40?\"\n",
    "user_question = \"which kind of memory can an agent have?\"\n",
    "response = retrieval_chain({\"question\": user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response = According to the provided context, an agent can have the following types of memory:\n",
      "\n",
      "* Short-term memory: This type of memory is used for in-context learning and is restricted by the finite context window length of the Transformer.\n",
      "* Long-term memory: This type of memory is provided by an external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Therefore, the answer to the question is: An agent can have\n"
     ]
    }
   ],
   "source": [
    "print(f'Response ={response[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
