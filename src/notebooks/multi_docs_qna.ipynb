{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from models.sambanova_endpoint import SambaNovaEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO BE FILLED TO ACCESS THE LLM ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDE: Directory containing pdf\n",
    "# input_data_loc = \n",
    "\n",
    "# # PROVIDE: API Info\n",
    "# base_url=f'https://sjc1-demo1.sambanova.net'\n",
    "# project_id=\n",
    "# endpoint_id=\n",
    "# api_key="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract text and metadata from pdf\n",
    "def get_pdf_text_and_metadata(pdf_doc):\n",
    "    text = []\n",
    "    metadata = []\n",
    "    pdf_reader = PdfReader(pdf_doc)\n",
    "    for page in pdf_reader.pages:\n",
    "        text.append(page.extract_text())\n",
    "        metadata.append({\"filename\": pdf_doc, \"page\": pdf_reader.get_page_number(page)})\n",
    "    return text, metadata\n",
    "\n",
    "\n",
    "# Read the pdf files and extract text + metadata\n",
    "def get_data_for_splitting(pdf_docs):\n",
    "    files_data = []\n",
    "    files_metadatas = []\n",
    "    for file in pdf_docs:\n",
    "        text, meta = get_pdf_text_and_metadata(file)\n",
    "        files_data.extend(text)\n",
    "        files_metadatas.extend(meta)\n",
    "    return files_data, files_metadatas\n",
    "\n",
    "\n",
    "# Chunk the extracted data\n",
    "def get_text_chunks(text, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=200, length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.create_documents(text, metadata)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"BAAI/bge-large-en\",\n",
    "        embed_instruction=\"\",  # no instruction is needed for candidate passages\n",
    "        query_instruction=\"Represent this sentence for searching relevant passages: \",\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    vectorstore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def get_qa_retrieval_chain(vectorstore):\n",
    "    llm = SambaNovaEndpoint(model_kwargs={\"do_sample\": False, \"temperature\": 0.0})\n",
    "\n",
    "    conversation_chain = RetrievalQA.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        return_source_documents=True,\n",
    "        input_key=\"question\",\n",
    "    )\n",
    "    return conversation_chain\n",
    "\n",
    "\n",
    "def get_conversation_chain(vectorstore):\n",
    "    llm = SambaNovaEndpoint(model_kwargs={\"do_sample\": False, \"temperature\": 0.0})\n",
    "\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, retriever=vectorstore.as_retriever(), memory=memory\n",
    "    )\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the pdf files and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs = [f for f in glob.glob(f\"{input_data_loc}/*.pdf\")]\n",
    "\n",
    "# get pdf text\n",
    "raw_text, meta_data = get_data_for_splitting(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text chunks\n",
    "text_chunks = get_text_chunks(raw_text, meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store (for example: FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# create vector store\n",
    "vectorstore = get_vectorstore(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inititalize the Large language model\n",
    "\n",
    "- **Note**: api info will have to be updated to point to customers endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SambaNovaEndpoint(\n",
    "    base_url=base_url,\n",
    "    project_id=project_id,\n",
    "    endpoint_id=endpoint_id,\n",
    "    api_key=api_key,\n",
    "    model_kwargs={\"do_sample\": False, \"temperature\": 0.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a ConversationalRetrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "#     llm=llm,\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "#     memory=memory,\n",
    "#     return_source_documents=True,\n",
    "#     )\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.5, \"k\": 4},\n",
    ")\n",
    "retrieval_chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If the answer is not in context for answering, say that you don't know, don't try to make up an answer or provide an answer not extracted from provided context. \n",
    "Cross check if the answer is contained in provided context. If not than say \"I do not have information regarding this.\"\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "CUSTOMPROMPT = PromptTemplate(\n",
    "    template=custom_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "## Inject custom prompt\n",
    "retrieval_chain.combine_documents_chain.llm_chain.prompt = CUSTOMPROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is the input voltage range for LT8625S?\"\n",
    "response = retrieval_chain({\"question\": user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response = The LT8625S has an input voltage range of 2.7V to 18V.\n"
     ]
    }
   ],
   "source": [
    "print(f'Response ={response[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
