llm: 
  "api": "fastapi"  # set either sambastudio, sambaverse or fastapi
  "do_sample": False
  "temperature": 0.01
  "max_tokens_to_generate": 2048
  "sambaverse_model_name": "Meta/Meta-Llama-3-70B-Instruct"
  "coe": True #set as true if using Sambastudio CoE endpoint
  "select_expert": "llama3-8b" # "Meta-Llama-3-70B-Instruct" #set if using Sambaverse or SambaStudio CoE llm expert
  #fastapi expert name -> "llama3-70b"

prompts: 
    "generate_qa_prompt": "prompts/generate_Q&A.yaml"

generation:
  output_path: "./output/synthetic_data.jsonl"
  amount_per_document: 5
  include_thoughts: true
  include_references: true
  model_family: "llama3"
  system_prompt: "You are a helpful assistant for question-answering tasks.\n"