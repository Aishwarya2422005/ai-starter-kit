{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edgar: Comparative Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/79s0z9ss0dd6__6jc245j5t40000gp/T/ipykernel_33704/3150193592.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from typing import List, Any\n",
    "from tqdm.autonotebook import trange\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# Langchain imports\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    load_prompt\n",
    ")\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Llama index imports\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext, VectorStoreIndex\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.llms.base import llm_completion_callback\n",
    "\n",
    "\n",
    "#from utils.sambanova_endpoint import SambaNovaEndpoint\n",
    "from langchain_community.llms.sambanova import SambaStudio, Sambaverse\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(repo_dir,'.env'))\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber vs Lift 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SambaNovaLLMWrapper(CustomLLM):\n",
    "    context_window: int = 3900\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"sambanova_llama7b\"\n",
    "    \n",
    "    def _get_sambanova_llm(self):\n",
    "\n",
    "        #llm = Sambaverse(\n",
    "        #    sambaverse_model_name='Meta/llama-2-7b-chat-hf',\n",
    "        #    model_kwargs={\n",
    "        #        'do_sample': False,\n",
    "        #        'max_tokens_to_generate': 512,\n",
    "        #        'select_expert': 'llama-2-7b-chat-hf',\n",
    "        #        'process_prompt': False,\n",
    "        #        'temperature': 0.01,\n",
    "        #    },\n",
    "        #)\n",
    "\n",
    "        llm = SambaStudio(\n",
    "            streaming=True,\n",
    "            model_kwargs={\n",
    "                'max_tokens_to_generate': 512,\n",
    "                'select_expert': 'Meta-Llama-3-70B-Instruct',\n",
    "                'process_prompt': False,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return llm\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        llm = self._get_sambanova_llm()\n",
    "        response = llm(prompt)\n",
    "        return CompletionResponse(text=response)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "        self, prompt: str, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        llm = self._get_sambanova_llm()\n",
    "        llm_response = llm(prompt)\n",
    "        for token in llm_response:\n",
    "            response += token\n",
    "            yield CompletionResponse(text=response, delta=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LLM \n",
    "llm = SambaNovaLLMWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# Instatiate embedding model\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    query_instruction=\"Represent the query for retrieval: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare service context\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Parsing nodes: 100%|██████████| 5/5 [00:00<00:00, 140.13it/s]\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:29<00:00,  1.99s/it]\n",
      "Parsing nodes: 100%|██████████| 5/5 [00:00<00:00, 227.24it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:17<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "## Load data \n",
    "lyft_docs = SimpleDirectoryReader(\n",
    "    input_files=[os.path.join(kit_dir,\"data/sec-edgar-filings/pdfs/lyft_10k_2021_5pages.pdf\")]\n",
    ").load_data()\n",
    "uber_docs = SimpleDirectoryReader(\n",
    "    input_files=[os.path.join(kit_dir,\"data/sec-edgar-filings/pdfs/uber_10k_2021_5pages.pdf\")]\n",
    ").load_data()\n",
    "\n",
    "## Build indices\n",
    "lyft_index = VectorStoreIndex.from_documents(lyft_docs, show_progress=True, service_context=service_context)\n",
    "\n",
    "uber_index = VectorStoreIndex.from_documents(uber_docs, show_progress=True, service_context=service_context)\n",
    "\n",
    "## Build query engines\n",
    "lyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "uber_engine = uber_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate query engine tools\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=lyft_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lyft_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Lyft financials for year 2021\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=uber_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"uber_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Uber financials for year 2021\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Instantiate Sub query engine\n",
    "s_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[uber_10k] Q: What are the customer segments that grew the fastest for Uber\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[uber_10k] Q: What are the geographies that grew the fastest for Uber\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[lyft_10k] Q: What are the customer segments that grew the fastest for Lyft\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[lyft_10k] Q: What are the geographies that grew the fastest for Lyft\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[lyft_10k] A:  Unable to determine from the provided context. The provided context appears to be a scanned copy of a 10-K filing for Lyft, but it does not contain readable text. The text is garbled and appears to be a result of an OCR (Optical Character Recognition) error. Therefore, it is not possible to determine the customer segments that grew the fastest for Lyft from the provided context.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[uber_10k] A:  The query is asking about customer segments that grew the fastest for Uber. However, the provided context information does not contain any specific data or information about customer segments or their growth rates. The context appears to be related to Uber's mission, values, and annual report, but it does not provide the necessary information to answer this query. Therefore, it is not possible to provide a specific answer to this question based on the given context.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[uber_10k] A:  The query is not answerable based on the provided context information. The provided text does not mention specific geographies or growth rates for Uber. It appears to be a section of Uber's mission statement and values.\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[lyft_10k] A:  Not found in the given context.\n",
      "\u001b[0m It is not possible to compare and contrast the customer segments and geographies that grew the fastest for Uber and Lyft based on the provided context information. The context does not provide the necessary data or information to answer this query.\n",
      "Generated 4 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[uber_10k] Q: What is the revenue of Uber in 2020\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[uber_10k] Q: What is the revenue of Uber in 2021\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[lyft_10k] Q: What is the revenue of Lyft in 2020\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[lyft_10k] Q: What is the revenue of Lyft in 2021\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[uber_10k] A:  The revenue of Uber in 2020 is not mentioned in the provided text.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[uber_10k] A:  The revenue of Uber in 2021 is not explicitly mentioned in the provided text. The text appears to be from Uber's 2021 Annual Report, but it does not contain specific financial information such as revenue.\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[lyft_10k] A:  Not found in the given context.\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[lyft_10k] A:  Not found in the given context.\n",
      "\u001b[0m It is not possible to compare the revenue growth of Uber and Lyft from 2020 to 2021 based on the provided context, as the revenue figures for both companies in 2020 and 2021 are not available.\n"
     ]
    }
   ],
   "source": [
    "## Run queries\n",
    "response = s_engine.query(\n",
    "    \"Compare and contrast the customer segments and geographies that grew the fastest\"\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "response = s_engine.query(\n",
    "    \"Compare revenue growth of Uber and Lyft from 2020 to 2021\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber vs Lift 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# Load uber data\n",
    "loader = PyPDFLoader(os.path.join(kit_dir,\"data/sec-edgar-filings/pdfs/uber_10k_2021_5pages.pdf\"))\n",
    "data = loader.load()\n",
    "for document in data:\n",
    "    document.metadata['company'] = 'Uber'\n",
    "    document.metadata['year'] = 2021\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "uber_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lyft data\n",
    "loader = PyPDFLoader(os.path.join(kit_dir,\"data/sec-edgar-filings/pdfs/lyft_10k_2021_5pages.pdf\"))\n",
    "data = loader.load()\n",
    "for document in data:\n",
    "    document.metadata['company'] = 'Lyft'\n",
    "    document.metadata['year'] = 2021\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "lyft_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 uber split docs\n",
      "17 lyft split docs\n",
      "32 all docs\n"
     ]
    }
   ],
   "source": [
    "splits = [*uber_splits,*lyft_splits]\n",
    "\n",
    "print(f\"{len(uber_splits)} uber split docs\")\n",
    "print(f\"{len(lyft_splits)} lyft split docs\")\n",
    "print(f\"{len(splits)} all docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings and create vector store\n",
    "embedding = HuggingFaceInstructEmbeddings(\n",
    "    query_instruction=\"Represent the query for retrieval: \"\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llm object from Sambanova endpoint class\n",
    "#llm = Sambaverse(\n",
    "#            sambaverse_model_name='Meta/llama-2-7b-chat-hf',\n",
    "#            model_kwargs={\n",
    "#                'do_sample': False,\n",
    "#                'max_tokens_to_generate': 512,\n",
    "#                'select_expert': 'llama-2-7b-chat-hf',\n",
    "#                'process_prompt': False,\n",
    "#                'temperature': 0.01,\n",
    "#            },\n",
    "#        )\n",
    "\n",
    "llm = SambaStudio(\n",
    "            streaming=True,\n",
    "            model_kwargs={\n",
    "                'max_tokens_to_generate': 512,\n",
    "                'select_expert': 'Meta-Llama-3-70B-Instruct',\n",
    "                'process_prompt': False,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "class QuestionListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        questions = [question for question in lines if '?' in question]\n",
    "        return LineList(lines=questions)\n",
    "\n",
    "output_parser = QuestionListOutputParser()\n",
    "\n",
    "query_decomposition_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    \n",
    "    template=\"\"\"[INST] <<SYS>>Decompose a complex query into a list of questions directly and concisely.<<SYS>>\n",
    "    Query: {question}\n",
    "    Output: [/INST]\"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_decomposition_prompt.save(os.path.join(kit_dir,'prompts/llama70b-edgar_comparative_qna-query_decomposition_prompt.yaml'))\n",
    "query_decomposition_prompt = load_prompt(os.path.join(kit_dir,'prompts/llama70b-edgar_comparative_qna-query_decomposition_prompt.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse LineList from completion 1. Got: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.7/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mBaseModel):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mBaseModel):\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/pydantic/main.py:532\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    531\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.7/v/model_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the revenue breakdowns for the two documents?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# multiquery results\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m multiquery_retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mmultiquery_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/retrievers.py:360\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[1;32m    359\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/retrievers.py:221\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    224\u001b[0m         result,\n\u001b[1;32m    225\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/retrievers.py:214\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 214\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain/retrievers/multi_query.py:166\u001b[0m, in \u001b[0;36mMultiQueryRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    156\u001b[0m     run_manager: CallbackManagerForRetrieverRun,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get relevant documents given a user query.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m        Unique union of relevant documents from all generated queries\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_original:\n\u001b[1;32m    168\u001b[0m         queries\u001b[38;5;241m.\u001b[39mappend(query)\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain/retrievers/multi_query.py:183\u001b[0m, in \u001b[0;36mMultiQueryRetriever.generate_queries\u001b[0;34m(self, question, run_manager)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_queries\u001b[39m(\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate queries based upon user input.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m        List of LLM generated queries that are similar to the user input\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain, LLMChain):\n\u001b[1;32m    187\u001b[0m         lines \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain/chains/llm.py:129\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    125\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    126\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    128\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain/chains/llm.py:283\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[0;34m(self, llm_result)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m         {\n\u001b[1;32m    286\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse_result(generation),\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[1;32m    288\u001b[0m         }\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[1;32m    290\u001b[0m     ]\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[1;32m    292\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain/chains/llm.py:286\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m         {\n\u001b[0;32m--> 286\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[1;32m    288\u001b[0m         }\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[1;32m    290\u001b[0m     ]\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[1;32m    292\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:66\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/ASK/fastAPI/ai-starter-kit/edgar_qna/edgar_fastapi/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:35\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model version for PydanticOutputParser: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m                    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m             )\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic\u001b[38;5;241m.\u001b[39mValidationError, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parser_exception(e, obj)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pydantic v1\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse LineList from completion 1. Got: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.7/v/model_type"
     ]
    }
   ],
   "source": [
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=query_decomposition_prompt, output_parser=output_parser)\n",
    "\n",
    "# \"lines\" is the attribute name of the parsed output\n",
    "multiquery_retriever = MultiQueryRetriever(\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\n",
    "        'k': 3,\n",
    "        'filter': {'$or': [{'company': {'$eq': 'Uber'}}, {'company': {'$eq': 'Lyft'}}]},\n",
    "    }), \n",
    "    llm_chain=llm_chain, \n",
    "    parser_key=\"lines\", \n",
    "    verbose = True\n",
    ")  \n",
    "\n",
    "question = \"What are the revenue breakdowns for the two documents?\"\n",
    "\n",
    "# multiquery results\n",
    "multiquery_retrieved_docs = multiquery_retriever.get_relevant_documents(\n",
    "    query=question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt for answering and summarization\n",
    "summarization_prompt_template = \"\"\"[INST] <<SYS>>You're a respectful, helpful assistant. Follow these rules:\n",
    "1. Use only the information provided in the context section.\n",
    "2. Provide relevant information to answer the question.<<SYS>>\n",
    "Write an answer to the following question based on the following context:\n",
    "Question:\n",
    "{original_question}\n",
    "Context:\n",
    "{context}\n",
    "Answer: [/INST]\"\"\"\n",
    "summarization_prompt = PromptTemplate.from_template(summarization_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt.save(os.path.join(kit_dir,'prompts/llama70b-edgar_comparative_qna-answering_and_summarization_prompt.yaml'))\n",
    "summarization_prompt = load_prompt(os.path.join(kit_dir,'prompts/llama70b-edgar_comparative_qna-answering_and_summarization_prompt.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multiquery_retrieved_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39msummarization_prompt)\n\u001b[1;32m      3\u001b[0m stuff_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(llm_chain\u001b[38;5;241m=\u001b[39mllm_chain, document_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m stuff_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmultiquery_retrieved_docs\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_question\u001b[39m\u001b[38;5;124m'\u001b[39m: question})\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multiquery_retrieved_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# Define StuffDocumentsChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")\n",
    "\n",
    "response = stuff_chain.invoke({\"input_documents\": multiquery_retrieved_docs, 'original_question': question})\n",
    "print(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are the revenue breakdowns for Microsoft and Apple in their respective 10-K reports, and how do they compare in terms of total revenue and revenue from different segments?\",\n",
    "    \"What are the key risks mentioned in the risk factors section of both Microsoft and Apple's 10-K reports, and how do they differ in terms of potential impact and mitigation strategies?\",\n",
    "    \"How do the corporate governance structures of Microsoft and Apple, as outlined in their 10-K filings, compare in terms of board composition, executive compensation, and shareholder rights?\",\n",
    "    \"What are the major investments and acquisitions disclosed in the investment section of Microsoft and Apple's 10-K reports, and how do they reflect each company's strategic priorities and growth strategies?\",\n",
    "    \"How do the research and development expenditures disclosed in Microsoft and Apple's 10-K reports compare in terms of absolute spending and percentage of revenue, and what insights can be drawn regarding their innovation efforts?\",\n",
    "    \"What are the legal proceedings and regulatory issues disclosed in the legal proceedings section of both Microsoft and Apple's 10-K filings, and how do they differ in terms of nature, severity, and potential impact on the companies?\",\n",
    "    \"How do the financial performance metrics such as net income, operating margins, and cash flow ratios disclosed in Microsoft and Apple's 10-K reports compare, and what factors contribute to any observed differences?\",\n",
    "    \"What are the geographical revenue breakdowns provided in the geographic segments section of both Microsoft and Apple's 10-K reports, and how do they reflect each company's international presence and market diversification?\",\n",
    "    \"How do the sustainability initiatives and environmental disclosures in Microsoft and Apple's 10-K filings compare, including information on energy consumption, carbon footprint, and supply chain sustainability efforts?\",\n",
    "    \"What are the forward-looking statements and risk factors outlined in the Management's Discussion and Analysis (MD&A) sections of Microsoft and Apple's 10-K reports, and how do they reflect each company's outlook, challenges, and opportunities in the market?\",\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
