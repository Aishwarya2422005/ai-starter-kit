{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 Synthetic Dataset Generation for Embedding Finetuning Tasks\n",
    "\n",
    "In this Jupyter notebook, we demonstrate how to leverage a Python script designed for generating a synthetic dataset of (query, relevant document) pairs from a corpus of documents that can be used to finetune embeddings models to improve performance in custom RAG and retrival AI use cases. We use natural language processing (NLP) techniques and a language model to automate the creation of a dataset suitable for tasks such as question answering, search, and information retrieval.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary components from our script. This involves loading the corpus, generating queries, and saving our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Corpus\n",
    "\n",
    "We begin by importing the relevant helper functions from the script, initializing our `CorpusLoader` with a directory containing our PDF documents. This class will load and split our corpus into training and validation sets. \n",
    "\n",
    "We create the corpus of text chunks by leveraging LlamaIndex to load some sample PDFs, and parsing/chunking into plain text chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generate_fine_tune_embed_dataset import (\n",
    "    CorpusLoader,\n",
    "    QueryGenerator,\n",
    "    SambaNovaEndpoint,\n",
    "    LangChainLLM,\n",
    "    OpenAI,\n",
    "    save_dict_safely,\n",
    ")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"./sample_data\"\n",
    "val_ratio = 0.2\n",
    "\n",
    "corpus_loader = CorpusLoader(directory=data_directory, val_ratio=val_ratio)\n",
    "train_corpus = corpus_loader.load_corpus(corpus_loader.train_files)\n",
    "val_corpus = corpus_loader.load_corpus(corpus_loader.val_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Loaded Corpora\n",
    "\n",
    "After loading the training and validation corpora, we save them to files for later use. This ensures we can easily reload the corpora without reprocessing the original documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_output_path = \"./data/train_corpus.json\"\n",
    "val_corpus_output_path = \"./data/val_corpus.json\"\n",
    "\n",
    "corpus_loader.save_corpus(train_corpus, train_corpus_output_path)\n",
    "corpus_loader.save_corpus(val_corpus, val_corpus_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Language Model (LLM)\n",
    "\n",
    "For generating queries, we define the language model (LLM) to use. You can choose between a SambaNova model or an OpenAI / other LLM provider model based on your requirements and access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example LLM instantiation:\n",
    "# For a Sambanova LLM:\n",
    "# base_url=\"YOUR_BASE_URL\"\n",
    "# project_id=\"YOUR_PROJECT_ID\"\n",
    "# endpoint_id=\"YOUR_ENDPOINT_ID\"\n",
    "# api_key=\"YOUR_API_KEY\"\n",
    "\n",
    "base_url = \"https://sjc3-demo2.sambanova.net\"\n",
    "project_id = \"60774d44-3cc3-47eb-aa91-87fae2e8655e\"\n",
    "endpoint_id = \"b0e414eb-4863-4a8c-9839-3c2dfa718ae5\"\n",
    "api_key = \"e2a3bac7-c31c-4712-a408-bb4b64d92c41\"\n",
    "\n",
    "llm = SambaNovaEndpoint(\n",
    "    base_url=base_url,\n",
    "    project_id=project_id,\n",
    "    endpoint_id=endpoint_id,\n",
    "    api_key=api_key,\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_tokens_to_generate\": 512,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Convert SN Endpoint to LangChain LLM As The Wrapper Is In Langchain\n",
    "llm = LangChainLLM(llm=llm)\n",
    "\n",
    "\n",
    "# For OpenAI:\n",
    "# llm = OpenAI(model='gpt-3.5-turbo')  # This line remains commented in the script for instructional purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the QueryGenerator with your language model\n",
    "# Note: Ensure you have access to the LLM and its credentials\n",
    "# Note: Depending on the size of your corpus & model inference time, this can take a long time!\n",
    "\n",
    "query_generator = QueryGenerator(llm=llm)\n",
    "\n",
    "train_queries, train_relevant_docs = query_generator.generate_queries(\n",
    "    train_corpus, verbose=True\n",
    ")\n",
    "val_queries, val_relevant_docs = query_generator.generate_queries(\n",
    "    val_corpus, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Generated Queries\n",
    "\n",
    "It's essential to inspect the generated queries and their corresponding relevant documents to ensure the quality of our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display generated queries and documents\n",
    "def display_generated_data(queries, relevant_docs, corpus, num_samples=5):\n",
    "    sample_queries = random.sample(list(queries.items()), num_samples)\n",
    "\n",
    "    for query_id, query in sample_queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        doc_ids = relevant_docs[query_id]\n",
    "        for doc_id in doc_ids:\n",
    "            print(\n",
    "                f\"Relevant Document: {corpus[doc_id][:200]}...\"\n",
    "            )  # Display the first 200 characters\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "display_generated_data(train_queries, train_relevant_docs, train_corpus)\n",
    "display_generated_data(val_queries, val_relevant_docs, val_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Dataset\n",
    "\n",
    "Finally, we save our generated dataset safely to ensure it can be used for training NLP models without running into memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 14:50:40,945 - INFO - Saving data to ./data/train_dataset.json...\n",
      "Saving data: 100%|██████████| 3/3 [00:00<00:00, 263.20it/s]\n",
      "2024-02-06 14:50:40,961 - INFO - Saving data to ./data/val_dataset.json...\n",
      "Saving data: 100%|██████████| 3/3 [00:00<00:00, 973.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_output_path = \"./data/train_dataset.json\"\n",
    "val_output_path = \"./data/val_dataset.json\"\n",
    "\n",
    "save_dict_safely(\n",
    "    {\n",
    "        \"queries\": train_queries,\n",
    "        \"corpus\": train_corpus,\n",
    "        \"relevant_docs\": train_relevant_docs,\n",
    "    },\n",
    "    train_output_path,\n",
    ")\n",
    "save_dict_safely(\n",
    "    {\"queries\": val_queries, \"corpus\": val_corpus, \"relevant_docs\": val_relevant_docs},\n",
    "    val_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I Conclusion\n",
    "\n",
    "This notebook provides a comprehensive guide on generating a synthetic dataset for NLP tasks using Python. By automating the generation of queries and relevant documents, we streamline the process of creating rich datasets for training models on tasks such as question answering and information retrieval. In Part II of the Series We'll FineTune The Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Fine-Tuning Embedding Models for Enhanced NLP Performance\n",
    "\n",
    "In Part 2 of this series, we will leverage the synthetic dataset generated in Part 1 to fine-tune an open-source embedding model using Sentence Transformers. The goal is to improve the model's performance on custom Retrieval AI and question answering (QA) use cases by adapting the embeddings to our specific dataset.\n",
    "\n",
    "## Setup\n",
    "\n",
    "To begin, we will import necessary functions from our fine-tuning script. This includes components for loading the dataset, configuring the fine-tuning process, and executing the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetune_embedding_model import DatasetLoader, FineTuneModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Synthetic Dataset\n",
    "\n",
    "Our first step is to load the synthetic dataset created in Part 1. This dataset includes pairs of queries and relevant documents that we will use to fine-tune our embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"./data/train_dataset.json\"\n",
    "val_dataset_path = \"./data/val_dataset.json\"\n",
    "\n",
    "# Initialize the dataset loader\n",
    "dataset_loader = DatasetLoader(train_dataset_path)\n",
    "val_dataset_loader = DatasetLoader(val_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Fine-Tuning Process\n",
    "\n",
    "With our dataset ready, we can now initialize the model for fine-tuning. We'll specify the model identifier, paths to our training and validation datasets, and other training parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"  # Example model ID\n",
    "batch_size = 8\n",
    "epochs = 4\n",
    "output_path = \"./finetuned_model\"\n",
    "\n",
    "# Initialize the fine-tuning model\n",
    "finetune_model = FineTuneModel(\n",
    "    model_id=model_id,\n",
    "    train_dataset_path=train_dataset_path,\n",
    "    val_dataset_path=val_dataset_path,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "Now, we're ready to fine-tune our model. This process will adjust the embeddings to better suit our synthetic dataset, potentially improving performance on our target NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lma-kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
