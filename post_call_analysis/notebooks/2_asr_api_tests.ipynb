{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import tarfile\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../.env')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header: key generated by Sambanova Studio \n",
    "\n",
    "headers = {\n",
    "    'content-type': 'application/json',\n",
    "    'key': os.getenv('SAMBASTUDIO_KEY'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint urls\n",
    "\n",
    "base_url = 'https://sjc1-demo1.sambanova.net'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful methods\n",
    "\n",
    "def get_call(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        logging.info('GET request successful!')\n",
    "        logging.debug(f'Response: {response.text}')\n",
    "    else:\n",
    "        logging.error(f'GET request failed with status code: {response.status_code}')\n",
    "        logging.debug(f'Error message: {response.text}')\n",
    "    return response\n",
    "\n",
    "def post_call(url, params, headers):\n",
    "    response = requests.post(url, json=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        logging.info('POST request successful!')\n",
    "        logging.debug(f'Response: {response.text}')\n",
    "    else:\n",
    "        logging.error(f'POST request failed with status code: {response.status_code}')\n",
    "        logging.debug(f'Error message: {response.text}')\n",
    "    return response\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    minutes, seconds = map(int, time_str.split(':'))\n",
    "    return  minutes * 60 + seconds\n",
    "\n",
    "def get_df_output(response_content: str) -> DataFrame:\n",
    "    compressed_bytes = io.BytesIO(response_content)\n",
    "    \n",
    "    with tarfile.open(fileobj=compressed_bytes, mode=\"r:gz\") as tar:\n",
    "        output_tar_member = tar.getmember('results/output.csv')\n",
    "        output_file = tar.extractfile(output_tar_member)\n",
    "        output_df = pd.read_csv(io.BytesIO(output_file.read()), names = ['audio_path', 'results_path', 'speaker', 'start_time', 'end_time', 'unformatted_transcript', 'formatted_transcript'])\n",
    "        output_df['start_time'] = output_df.apply(lambda x: time_to_seconds(x['start_time']), axis = 1)\n",
    "        output_df['end_time'] = output_df.apply(lambda x: x['start_time'] + int(x['end_time'])/16000, axis = 1)\n",
    "        output_df = output_df[['start_time', 'end_time', 'speaker', 'formatted_transcript']].rename(columns={'formatted_transcript': 'text'})\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:GET request failed with status code: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET: status of requested dataset\n",
    "\n",
    "datasets_url = '/api/datasets'\n",
    "dataset_name = '/PCA_dataset'\n",
    "url = base_url + datasets_url + dataset_name\n",
    "\n",
    "get_call(url, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): sjc1-demo1.sambanova.net:443\n",
      "DEBUG:urllib3.connectionpool:https://sjc1-demo1.sambanova.net:443 \"POST /api/datasets HTTP/1.1\" 400 27\n",
      "ERROR:root:POST request failed with status code: 400\n",
      "DEBUG:root:Error message: {\"detail\":\"field required\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POST: creation of a new dataset\n",
    "\n",
    "url = base_url + datasets_url\n",
    "\n",
    "params = {\n",
    "  'dataset_name': 'pca_test_v2',\n",
    "  'application_field': 'speech',\n",
    "  'job_type': ['batch_predict'],\n",
    "  'language': 'english',\n",
    "  'task': 'ASR With Diarization',\n",
    "  'dataset_storage': 'LOCAL',\n",
    "  'dataset_path': 'default/cap-engagements/datasets/local-dataset-42cf8587-6ce1-41bb-b7a4-2a069ae146bc', \n",
    "  'description': 'test',\n",
    "  'file_type': 'test',\n",
    "  'url': '',\n",
    "}\n",
    "\n",
    "post_call(url, params, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After talking Varun Malyala, he says that \"Uploading a Dataset is not supported via snsdk directly\", so we'd need to add a new feature (Feb 26, 24). \"Right now, Uploading a local Dataset is only possible via snapi or UI\", so we're going to try with snapi later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST: creation of a new project\n",
    "\n",
    "projects_url = '/api/projects'\n",
    "url = base_url + projects_url\n",
    "\n",
    "params = {\n",
    "  'project_name': 'pca_project_test',\n",
    "  'description': 'test project for pca'\n",
    "}\n",
    "\n",
    "post_call(url, params, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5399bf56-201e-49d1-b663-dbf4678cebc4'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET: status of requested project\n",
    "\n",
    "project_name = '/pca_project_test'\n",
    "url = base_url + projects_url + project_name\n",
    "\n",
    "response = get_call(url, headers)\n",
    "parsed_reponse = json.loads(response.text)\n",
    "project_id = parsed_reponse['data']['project_id']\n",
    "project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST: creation of a new batch inference\n",
    "\n",
    "jobs_url = '/' + project_id + '/jobs'\n",
    "url = base_url + projects_url + jobs_url\n",
    "\n",
    "params = {\n",
    "  'task': 'ASR With Diarization',\n",
    "  'job_type': 'batch_predict',\n",
    "  'job_name': 'pca_test_pipeline',\n",
    "  'project': project_id,\n",
    "  'model_checkpoint': 'Diarization_ASR_Pipeline_V2',\n",
    "  'description': 'test diarization pipeline',\n",
    "  'dataset': 'PCA_test',\n",
    "}\n",
    "\n",
    "response = post_call(url, params, headers)\n",
    "parsed_reponse = json.loads(response.text)\n",
    "job_id = parsed_reponse['data']['job_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): sjc1-demo1.sambanova.net:443\n",
      "DEBUG:urllib3.connectionpool:https://sjc1-demo1.sambanova.net:443 \"GET /api/projects/5399bf56-201e-49d1-b663-dbf4678cebc4/jobs/458c3b1b-7deb-4d9f-b0ad-12fd04b47f2a HTTP/1.1\" 200 1251\n",
      "INFO:root:GET request successful!\n",
      "DEBUG:root:Response: {\"data\":{\"job_name\":\"pca_test_pipeline\",\"job_id\":\"458c3b1b-7deb-4d9f-b0ad-12fd04b47f2a\",\"job_type\":\"batch_predict\",\"project_id\":\"5399bf56-201e-49d1-b663-dbf4678cebc4\",\"status\":\"EXIT_WITH_0\",\"dataset_id\":\"42cf8587-6ce1-41bb-b7a4-2a069ae146bc\",\"input_data_path\":\"default/cap-engagements/datasets/local-dataset-42cf8587-6ce1-41bb-b7a4-2a069ae146bc\",\"model_checkpoint\":\"Diarization_ASR_Pipeline_V2\",\"checkpoint_id\":null,\"hyperparams\":[{\"param_name\":\"sockets\",\"value\":3,\"description\":\"Number of sockets each instance of the model uses\"}],\"user_id\":\"rodrigo.maldonado\",\"time_created\":\"2024-02-23T20:45:37.866825+00:00\",\"time_updated\":\"2024-02-23T20:47:44.084972+00:00\",\"description\":\"test diarization pipeline\",\"result_path\":\"default/cap-engagements/5399bf56-201e-49d1-b663-dbf4678cebc4/jobs/458c3b1b-7deb-4d9f-b0ad-12fd04b47f2a/results\",\"parallel_instances\":1,\"load_state\":false,\"tenant_id\":\"a016b375-1e4a-46a5-8f66-03c3dbcb5f11\",\"app_id\":\"b6aefdf7-02a4-4384-9c3c-8a81d735a54e\",\"app_name\":\"ASR With Diarization\",\"environment_variables\":{},\"rdu_arch\":null,\"dataset\":\"PCA_test\",\"dataset_path\":\"default/cap-engagements/datasets/local-dataset-42cf8587-6ce1-41bb-b7a4-2a069ae146bc\",\"model_id\":\"d7dffdd2-2d74-4ad6-9a03-3d3f5de94fa3\",\"completion_percentage\":100}}\n"
     ]
    }
   ],
   "source": [
    "# GET: status of batch inference\n",
    "\n",
    "url = base_url + projects_url + jobs_url + '/' + job_id\n",
    "\n",
    "response = get_call(url, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Our Primeti 33. What is yet as emergency?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>11.9</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yes, sir, I need to, uh, uh. I need an ambulan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>12.3</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>A car.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>14.7</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Carol Wood Drive. Yes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>14.6</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  end_time     speaker  \\\n",
       "0           0       2.5  SPEAKER_01   \n",
       "1           3      11.9  SPEAKER_00   \n",
       "2          11      12.3  SPEAKER_01   \n",
       "3          13      14.7  SPEAKER_00   \n",
       "4          14      14.6  SPEAKER_01   \n",
       "\n",
       "                                                text  \n",
       "0          Our Primeti 33. What is yet as emergency?  \n",
       "1  Yes, sir, I need to, uh, uh. I need an ambulan...  \n",
       "2                                             A car.  \n",
       "3                             Carol Wood Drive. Yes.  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET: download results from pipeline\n",
    "\n",
    "download_results_url = '/results/download'\n",
    "url = base_url + projects_url + jobs_url + '/' + job_id + download_results_url\n",
    "\n",
    "response = get_call(url, headers)\n",
    "df = get_df_output(response.content)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR With Diarization\n",
      "=====================================================\n",
      "Name                : ASR With Diarization\n",
      "ID                  : b6aefdf7-02a4-4384-9c3c-8a81d735a54e\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "ASR Without Diarization\n",
      "=====================================================\n",
      "Name                : ASR Without Diarization\n",
      "ID                  : a36cc322-dd36-40e3-9641-d87ac48fe2c4\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "CLIP\n",
      "=====================================================\n",
      "Name                : CLIP\n",
      "ID                  : 6c14325a-1be7-4e48-b38f-19b33745fc3b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Databox\n",
      "=====================================================\n",
      "Name                : Databox\n",
      "ID                  : 199e9684-785c-4df0-8dc3-49e808d8eba5\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Deepseek 6.7B single socket\n",
      "=====================================================\n",
      "Name                : Deepseek 6.7B single socket\n",
      "ID                  : 2eeb4b7f-bc56-48c4-8814-ef9d1e8806b8\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "DePlot\n",
      "=====================================================\n",
      "Name                : DePlot\n",
      "ID                  : 40f16b58-72a9-404f-a7c3-afc0d27a2343\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "Dialog Act Classification\n",
      "=====================================================\n",
      "Name                : Dialog Act Classification\n",
      "ID                  : 0498c73a-5c03-456a-a645-3820728cfcae\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Document Classification\n",
      "=====================================================\n",
      "Name                : Document Classification\n",
      "ID                  : a28085cc-da42-44f7-9d06-95f60d06e4cb\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Fake Box for Testing\n",
      "=====================================================\n",
      "Name                : Fake Box for Testing\n",
      "ID                  : 3bcf5b6b-6d17-45ce-bb1e-543bed912f7b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "FFN MNIST\n",
      "=====================================================\n",
      "Name                : FFN MNIST\n",
      "ID                  : baf32143-cfee-4911-a5dd-545453e85b36\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Generative Tuning 1.5B\n",
      "=====================================================\n",
      "Name                : Generative Tuning 1.5B\n",
      "ID                  : e681c226-86be-40b2-9380-d2de11b19842\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Generative Tuning 13B\n",
      "=====================================================\n",
      "Name                : Generative Tuning 13B\n",
      "ID                  : 57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Image Classification\n",
      "=====================================================\n",
      "Name                : Image Classification\n",
      "ID                  : 1c528476-f933-4168-b9fa-fb26a7180708\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 13B\n",
      "=====================================================\n",
      "Name                : Llama 2 13B\n",
      "ID                  : 1bf617cb-8afb-4bbd-b92f-c15ebfdca10b\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 70B\n",
      "=====================================================\n",
      "Name                : Llama 2 70B\n",
      "ID                  : 82254d3b-7239-458b-9da8-da1aca9b7fba\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 7B\n",
      "=====================================================\n",
      "Name                : Llama 2 7B\n",
      "ID                  : ec012370-6ffa-4a3a-b230-2c62613f1d89\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 7B 8-socket\n",
      "=====================================================\n",
      "Name                : Llama 2 7B 8-socket\n",
      "ID                  : 21d706a3-d9fb-4998-aa9b-ad9ff2c3a920\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Low Data Resource Text Classification\n",
      "=====================================================\n",
      "Name                : Low Data Resource Text Classification\n",
      "ID                  : 35f812a5-bb33-44e0-9e63-10191e44bb0c\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Named Entity Recognition\n",
      "=====================================================\n",
      "Name                : Named Entity Recognition\n",
      "ID                  : c27a105f-d0be-4bef-b2a4-4d6bf747ebdc\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Reranking\n",
      "=====================================================\n",
      "Name                : Reranking\n",
      "ID                  : f25c8247-f73a-4dd9-9871-d5c10675239c\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Sentence Detection\n",
      "=====================================================\n",
      "Name                : Sentence Detection\n",
      "ID                  : f67c5390-da52-4105-ae17-12434fa7d03b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Sentiment Analysis\n",
      "=====================================================\n",
      "Name                : Sentiment Analysis\n",
      "ID                  : 6db6d59f-ae40-4513-b218-4218fc97e40f\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Simple Text Classifier\n",
      "=====================================================\n",
      "Name                : Simple Text Classifier\n",
      "ID                  : 3aaf5b6b-6d17-45ce-bb1e-543bed912f7b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Simple Text Classifier Pipeline\n",
      "=====================================================\n",
      "Name                : Simple Text Classifier Pipeline\n",
      "ID                  : cbc92c2f-2a01-4ef5-b85a-78ee7f392005\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Speaker Diarization\n",
      "=====================================================\n",
      "Name                : Speaker Diarization\n",
      "ID                  : cbba6d31-104a-4295-ac21-7e91da09ab9b\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "Speech Recognition\n",
      "=====================================================\n",
      "Name                : Speech Recognition\n",
      "ID                  : ecf84906-0924-4ce1-a1a2-c008f5334820\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "Text Embedding\n",
      "=====================================================\n",
      "Name                : Text Embedding\n",
      "ID                  : 89fbfbe6-ee77-4f5c-9ff6-56e2ab69f6ee\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!snapi app list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app id for ASR With Diarization\n",
    "\n",
    "app_id = 'b6aefdf7-02a4-4384-9c3c-8a81d735a54e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1msnapi dataset add [OPTIONS]\u001b[0m\u001b[1m                                            \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      " Add a new dataset                                                              \n",
      "                                                                                \n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-file\u001b[0m                          \u001b[1;33mTEXT\u001b[0m                                      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-dataset\u001b[0m\u001b[1;36m-name\u001b[0m       \u001b[1;32m-n\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Dataset name \u001b[2m[default: None]\u001b[0m        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]  \u001b[0m                        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-apps\u001b[0m               \u001b[1;32m-apps\u001b[0m      \u001b[1;33mTEXT\u001b[0m  App IDs or names to which this      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          Dataset will be associated          \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-metadata\u001b[0m\u001b[1;36m-file\u001b[0m      \u001b[1;32m-mf\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Metadata File containing dataset    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          metadata file paths. Only           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          .yaml/.json files are supported.    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          Example -                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          Dataset Metadata file schema        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"labels_file\":                      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/labels_file\",              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"train_filepath\":                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/train_filepath\",           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"validation_filepath\":              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/validation_filepath\",      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"test_filepath\":                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/test_filepath\",            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          }                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-description\u001b[0m        \u001b[1;32m-d\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Free-form text description of       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          dataset                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-file_type\u001b[0m          \u001b[1;32m-ft\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Free-form text file types in        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          dataset                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-url\u001b[0m                \u001b[1;32m-u\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Free-form text url source of        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          dataset                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-application_field\u001b[0m  \u001b[1;32m-af\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Field of application of dataset     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-language\u001b[0m           \u001b[1;32m-l\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Language of NLP dataset             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-job_type\u001b[0m           \u001b[1;32m-j\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Job types for dataset (either       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          'train' & 'evaluation' or           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          'batch_predict')                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-source_type\u001b[0m        \u001b[1;32m-st\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Dataset Source type (either 'local' \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          & 'aws' or 'localMachine')          \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-source_file\u001b[0m        \u001b[1;32m-sf\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Source File. Only .yaml/.json files \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          are supported. source file should   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          contain one of source configs for   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          local, aws or localMachine          \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          1. Local Config                     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {\"source_path\": \"string\"  // The    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          local source path on NFS.},         \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          2. AWS Config                       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"bucket\": \"string\",            //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          The AWS S3 bucket name.             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"folder\": \"string\",            //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          The folder or prefix within the     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          bucket.                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"accessKeyId\": \"string\",       //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          AWS Access Key ID for               \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          authentication.                     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"secretAccessKey\": \"string\",   //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          AWS Secret Access Key for           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          authentication.                     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"region\": \"string\"             //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          AWS region where the S3 bucket is   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          located.                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          },                                  \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          3. Local Machine Config             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {\"source_path\": \"string\"  // The    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          source path on the local machine.}  \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-test\u001b[0m               \u001b[1;32m-t\u001b[0m         \u001b[1;33m    \u001b[0m  Flag for test                       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                          \u001b[1;33m    \u001b[0m  Show this message and exit.         \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we have to reference a json file that has the path of the data set that we'll upload\n",
    "\n",
    "!snapi dataset add --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder Information:\n",
      "  - Number of Files: 1\n",
      "  - Total Size: 19.62 MB\n",
      "\n",
      "Are you sure you want to proceed? (\u001b[33myes\u001b[0m/no)\n",
      ": Uploading files\n",
      "Dataset folder upload complete: ../test_data/\n",
      "Dataset added successfully.\n",
      "Time taken to upload the dataset: 5.327938079833984 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = f'echo yes | snapi dataset add \\\n",
    "    --dataset-name local_file_test_3 \\\n",
    "    --job_type batch_predict \\\n",
    "    --apps b6aefdf7-02a4-4384-9c3c-8a81d735a54e \\\n",
    "    --source_type localMachine \\\n",
    "    --source_file ../data/datasets/source.json \\\n",
    "    --application_field language \\\n",
    "    --language english \\\n",
    "    --description test_description_3'\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress so far about the ASR pipeline. Next, a list with the tasks that still need to be done:\n",
    "- There's an error with dataset API search or GET (more details [here](https://sjc1-demo1.sambanova.net/api/docs#/)), which are returning 500 everytime they're getting info from a previously deleted dataset. I was checking this issue with Sharad Venkateswaran on a help-sambastudio channel \n",
    "- Integration tests with all methods. So far, every method has been tested, but we need to do integration tests as next step. Process will be: dataset creation, project creation, job creation, job monitoring, and retrieve results. Also, this pipeline contemplates as input a path with the audio file, and outputs a csv, so take that in mind. Contact Jorge Piedrahita or Rodrigo Maldonado for more details\n",
    "- Documentation on class and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config('../config.yaml')\n",
    "\n",
    "PENDING_RDU_JOB_STATUS = 'PENDING_RDU'\n",
    "SUCCESS_JOB_STATUS = 'EXIT_WITH_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchASRProcessor():\n",
    "    \n",
    "    def __init__(self, config) -> None:\n",
    "        self.headers = {\n",
    "            'content-type': 'application/json',\n",
    "            'key': os.getenv('SAMBASTUDIO_KEY'),\n",
    "        }\n",
    "        self.datasets_path = config['asr']['datasets']['datasets_path']\n",
    "        self.dataset_id = None\n",
    "        self.dataset_name = config['asr']['datasets']['dataset_name']\n",
    "        self.dataset_description = config['asr']['datasets']['dataset_description']\n",
    "        self.dataset_source_type = config['asr']['datasets']['dataset_source_type']\n",
    "        self.dataset_source_file = config['asr']['datasets']['dataset_source_file']\n",
    "        self.dataset_language = config['asr']['datasets']['dataset_language']\n",
    "        \n",
    "        self.asr_with_diarization_app_id = config['asr']['apps']['asr_with_diarization_app_id']\n",
    "        self.application_field = config['asr']['apps']['application_field']\n",
    "        \n",
    "        self.base_url = config['asr']['urls']['base_url']\n",
    "        self.datasets_url = config['asr']['urls']['datasets_url'] \n",
    "        self.projects_url = config['asr']['urls']['projects_url'] \n",
    "        self.jobs_url = config['asr']['urls']['jobs_url'] \n",
    "        self.download_results_url = config['asr']['urls']['download_results_url'] \n",
    "    \n",
    "        self.project_name = config['asr']['projects']['project_name']\n",
    "        self.project_description = config['asr']['projects']['project_description']\n",
    "        \n",
    "        self.job_name = config['asr']['jobs']['job_name']\n",
    "        self.job_task = config['asr']['jobs']['job_task']\n",
    "        self.job_type = config['asr']['jobs']['job_type']\n",
    "        self.job_description = config['asr']['jobs']['job_description']\n",
    "        self.model_checkpoint = config['asr']['jobs']['model_checkpoint']\n",
    "        \n",
    "        self.output_path = config['asr']['output']['output_path']\n",
    "        \n",
    "        \n",
    "    def _get_call(self, url, params = None, success_message = None):\n",
    "        response = requests.get(url, params=params, headers=self.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logging.info('GET request successful!')\n",
    "            logging.info(success_message)\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'GET request failed with status code: {response.status_code}')\n",
    "            raise Exception(f'Error message: {response.text}')\n",
    "        return response\n",
    "\n",
    "    def _post_call(self, url, params, success_message = None):\n",
    "        response = requests.post(url, json=params, headers=self.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logging.info('POST request successful!')\n",
    "            logging.info(success_message)\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'POST request failed with status code: {response.status_code}')\n",
    "            raise Exception(f'Error message: {response.text}')\n",
    "        return response\n",
    "    \n",
    "    def _delete_call(self, url):\n",
    "        response = requests.delete(url, headers=self.headers)    \n",
    "        if response.status_code == 200:\n",
    "            logging.info(f'Dataset {self.dataset_name} deleted successfully.')\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'Failed to delete the resource. Status code: {response.status_code}')\n",
    "            raise Exception(f'Error message: {response.text}')    \n",
    "        return response\n",
    "\n",
    "    def _time_to_seconds(self, time_str):\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        return  minutes * 60 + seconds\n",
    "\n",
    "    def _get_df_output(self, response_content: str) -> DataFrame:\n",
    "        compressed_bytes = io.BytesIO(response_content)\n",
    "        \n",
    "        with tarfile.open(fileobj=compressed_bytes, mode=\"r:gz\") as tar:\n",
    "            output_tar_member = tar.getmember(self.output_path)\n",
    "            output_file = tar.extractfile(output_tar_member)\n",
    "            output_df = pd.read_csv(io.BytesIO(output_file.read()), names = ['audio_path', 'results_path', 'speaker', 'start_time', 'sample_duration', 'unformatted_transcript', 'formatted_transcript'])\n",
    "            output_df['start_time'] = output_df.apply(lambda x: self._time_to_seconds(x['start_time']), axis = 1)\n",
    "            output_df['end_time'] = output_df.apply(lambda x: x['start_time'] + int(x['sample_duration'])/16000, axis = 1)\n",
    "            output_df = output_df[['start_time', 'end_time', 'speaker', 'formatted_transcript']].rename(columns={'formatted_transcript': 'text'})\n",
    "        \n",
    "        return output_df\n",
    "\n",
    "    def search_dataset(self):\n",
    "        url = self.base_url + self.datasets_url + '/search'\n",
    "        params = {\n",
    "            'dataset_name': self.dataset_name\n",
    "        }\n",
    "        response = self._get_call(url, params, f'Dataset {self.dataset_name} found in SambaStudio')\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        return parsed_reponse['data']['dataset_id']\n",
    "\n",
    "    def delete_dataset(self):\n",
    "        dataset_id = self.search_dataset()\n",
    "        url = self.base_url + self.datasets_url + '/' + dataset_id\n",
    "        response = self._delete_call(url)\n",
    "        logging.info(response.text)\n",
    "        \n",
    "    def create_dataset(self, path):\n",
    "                \n",
    "        # create pca directory and source.json file\n",
    "        pca_directory = self.datasets_path + '/' + self.dataset_name\n",
    "        \n",
    "        if not os.path.isdir(self.datasets_path):\n",
    "            os.mkdir(self.datasets_path) \n",
    "            \n",
    "        if not os.path.isdir(pca_directory):\n",
    "            logging.info(f'Datasets path: {pca_directory} wan\\'t found')\n",
    "            \n",
    "            source_file_data = {\n",
    "                \"source_path\": pca_directory\n",
    "            }\n",
    "            with open(self.dataset_source_file, 'w') as json_file:\n",
    "                json.dump(source_file_data, json_file)\n",
    "            os.mkdir(pca_directory)\n",
    "            \n",
    "            logging.info(f'PCA Directory: {pca_directory} created')\n",
    "    \n",
    "        # validate audio file\n",
    "        audio_format = path.split('.')[-1]\n",
    "        \n",
    "        if audio_format == 'mp3':\n",
    "            shutil.copyfile(path, pca_directory + '/pca_file.mp3')\n",
    "        elif audio_format == 'wav':\n",
    "            shutil.copyfile(path, pca_directory + '/pca_file.wav')\n",
    "        else:\n",
    "            raise Exception('Only mp3 and wav audio files supported')\n",
    "        \n",
    "        # create dataset\n",
    "        command = f'echo yes | snapi dataset add \\\n",
    "            --dataset-name {self.dataset_name} \\\n",
    "            --job_type {self.job_type} \\\n",
    "            --apps {self.asr_with_diarization_app_id} \\\n",
    "            --source_type {self.dataset_source_type} \\\n",
    "            --source_file {self.dataset_source_file} \\\n",
    "            --application_field {self.application_field} \\\n",
    "            --language {self.dataset_language} \\\n",
    "            --description \"{self.dataset_description}\"'\n",
    "        \n",
    "        os.system(command)\n",
    "        logging.info(f'Creating dataset: {self.dataset_name}')\n",
    "                    \n",
    "    def create_load_project(self):\n",
    "\n",
    "        url = self.base_url + self.projects_url + '/' + self.project_name\n",
    "\n",
    "        response = self._get_call(url, success_message=f'Project {self.project_name} found in SambaStudio')\n",
    "        not_found_error_message = f\"{self.project_name} not found\"\n",
    "\n",
    "        if not_found_error_message in response.text:\n",
    "            \n",
    "            logging.info(f'Project {self.project_name} wasn\\'t found in SambaStudio')\n",
    "            \n",
    "            url = base_url + projects_url\n",
    "\n",
    "            params = {\n",
    "                'project_name': self.project_name,\n",
    "                'description': self.project_description\n",
    "            }\n",
    "\n",
    "            response = self._post_call(url, params, success_message=f'Project {self.project_name} created!')\n",
    "\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        project_id = parsed_reponse['data']['project_id']\n",
    "        return project_id\n",
    "    \n",
    "    def run_job(self, project_id):\n",
    "        \n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=project_id)\n",
    "        \n",
    "        params = {\n",
    "            'task': self.job_task,\n",
    "            'job_type': self.job_type,\n",
    "            'job_name': self.job_name,\n",
    "            'project': project_id,\n",
    "            'model_checkpoint': self.model_checkpoint,\n",
    "            'description': self.job_description,\n",
    "            'dataset': self.dataset_name,\n",
    "        }\n",
    "\n",
    "        response = self._post_call(url, params, success_message='Job running')\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        job_id = parsed_reponse['data']['job_id']\n",
    "        \n",
    "        return job_id\n",
    "    \n",
    "    def check_job_progress(self, project_id, job_id):\n",
    "\n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=project_id) + '/' + job_id\n",
    "\n",
    "        status = PENDING_RDU_JOB_STATUS\n",
    "        while status != SUCCESS_JOB_STATUS:\n",
    "            response = self._get_call(url, success_message='Still waiting for job to finish')\n",
    "            parsed_reponse = json.loads(response.text)   \n",
    "            status = parsed_reponse['data']['status']\n",
    "            logging.info(f'Job status: {status}')\n",
    "            if status == SUCCESS_JOB_STATUS:\n",
    "                logging.info(f'Job finished!')\n",
    "                break\n",
    "            time.sleep(10)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def check_dataset_creation_progress(self):\n",
    "        # TO-DO: check if dataset creation is async. Then, this method will be necessary to know when it finishes and continue with the pipeline.\n",
    "        # Take 'check_job_progress' as reference\n",
    "        url = self.base_url + self.datasets_url + '/' + self.dataset_name\n",
    "        print(url)\n",
    "        response = self._get_call(url)\n",
    "        print(response.text)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def retrieve_results(self, project_id, job_id):\n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=project_id) + '/' + job_id + self.download_results_url\n",
    "\n",
    "        response = self._get_call(url, success_message='Results downloaded!')\n",
    "        df = self._get_df_output(response.content)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr = BatchASRProcessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:GET request failed with status code: 500\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error message: {\"detail\":\"Something went wrong\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[155], line 92\u001b[0m, in \u001b[0;36mBatchASRProcessor.search_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/search\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     89\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\n\u001b[1;32m     91\u001b[0m }\n\u001b[0;32m---> 92\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m found in SambaStudio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m parsed_reponse \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed_reponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[155], line 45\u001b[0m, in \u001b[0;36mBatchASRProcessor._get_call\u001b[0;34m(self, url, params, success_message)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET request failed with status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mException\u001b[0m: Error message: {\"detail\":\"Something went wrong\"}"
     ]
    }
   ],
   "source": [
    "asr.search_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Dataset PCA_dataset found in SambaStudio\n",
      "INFO:root:Dataset PCA_dataset deleted successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detail\":\"The Dataset: f335c6f9-dfdb-4f41-a5b8-00b9e0784abe was successfully marked for deletion from the Dataset Hub.\"}\n"
     ]
    }
   ],
   "source": [
    "asr.delete_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder Information:\n",
      "  - Number of Files: 1\n",
      "  - Total Size: 19.62 MB\n",
      "\n",
      "Are you sure you want to proceed? (\u001b[33myes\u001b[0m/no)\n",
      ": Uploading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating dataset: PCA_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset folder upload complete: ../data/datasets/PCA_dataset\n",
      "Dataset added successfully.\n",
      "Time taken to upload the dataset: 3.572451114654541 seconds\n",
      "https://sjc1-demo1.sambanova.net/api/datasets/PCA_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:GET request failed with status code: 500\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error message: {\"detail\":\"Failed to get dataset\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m asr\u001b[38;5;241m.\u001b[39mcreate_dataset(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../test_data/911_test.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m \u001b[43masr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_dataset_creation_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m end2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(start)\n",
      "Cell \u001b[0;32mIn[155], line 213\u001b[0m, in \u001b[0;36mBatchASRProcessor.check_dataset_creation_progress\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[0;32m--> 213\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# status = PENDING_RDU_JOB_STATUS\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# while status != SUCCESS_JOB_STATUS:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m#     response = self._get_call(url, success_message='Still waiting for job to finish')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m#         break\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m#     time.sleep(10)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[155], line 45\u001b[0m, in \u001b[0;36mBatchASRProcessor._get_call\u001b[0;34m(self, url, params, success_message)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET request failed with status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mException\u001b[0m: Error message: {\"detail\":\"Failed to get dataset\"}"
     ]
    }
   ],
   "source": [
    "# test time and to know if dataset creation is async or not\n",
    "start = time.time()\n",
    "asr.create_dataset(path='../test_data/911_test.wav')\n",
    "end = time.time()\n",
    "asr.check_dataset_creation_progress()\n",
    "end2 = time.time()\n",
    "print(start)\n",
    "print(end)\n",
    "print(end2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Project PostCallAnalysis_Project found in SambaStudio\n"
     ]
    }
   ],
   "source": [
    "project_id = asr.create_load_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:POST request successful!\n",
      "INFO:root:Job running\n"
     ]
    }
   ],
   "source": [
    "job_id = asr.run_job(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PENDING_RDU\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PENDING_RDU\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PENDING_RDU\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: EXIT_WITH_0\n",
      "INFO:root:Job finished!\n"
     ]
    }
   ],
   "source": [
    "result = asr.check_job_progress(project_id, job_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Results downloaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Our Primeti 33. What is yet as emergency?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>11.9</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yes, sir, I need to, uh, uh. I need an ambulan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>12.3</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>A car.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>14.7</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Carol Wood Drive. Yes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>14.6</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>16.6</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yeah, I.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>21.4</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, sir. What's the phone number you calling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>31.6</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Uh, sir? Oh, I have a. We have a gentleman her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>37.2</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, how does he. He's a 50 years old. Ser 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38</td>\n",
       "      <td>39.5</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yes, he's not breathing, sir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40</td>\n",
       "      <td>53.0</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, and he's not conscious either. Don't con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56</td>\n",
       "      <td>58.0</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>We need them. You get when you just party on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>57</td>\n",
       "      <td>62.9</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Just you in a way there. We're on Wemo. But I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>63</td>\n",
       "      <td>71.9</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yes, we have a personal doctor if you're with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>72</td>\n",
       "      <td>82.4</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Oh, okay, well, we're on our way there. If you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>84</td>\n",
       "      <td>87.3</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Uh, no, just the doctor. So the doctor's been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>87</td>\n",
       "      <td>88.7</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, so did doctor see what happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>89</td>\n",
       "      <td>96.7</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Uh, um, doctor, did you see what happens to an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>96</td>\n",
       "      <td>100.4</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, we're in a way. I'm just I'm just passin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>101</td>\n",
       "      <td>105.5</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>A you cure's pumping. He's pumping his chest, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>105</td>\n",
       "      <td>114.6</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, okay, we're on it. We're less than a mil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_time  end_time     speaker  \\\n",
       "0            0       2.5  SPEAKER_01   \n",
       "1            3      11.9  SPEAKER_00   \n",
       "2           11      12.3  SPEAKER_01   \n",
       "3           13      14.7  SPEAKER_00   \n",
       "4           14      14.6  SPEAKER_01   \n",
       "5           15      16.6  SPEAKER_00   \n",
       "6           17      21.4  SPEAKER_01   \n",
       "7           22      31.6  SPEAKER_00   \n",
       "8           32      37.2  SPEAKER_01   \n",
       "9           38      39.5  SPEAKER_00   \n",
       "10          40      53.0  SPEAKER_01   \n",
       "11          56      58.0  SPEAKER_00   \n",
       "12          57      62.9  SPEAKER_01   \n",
       "13          63      71.9  SPEAKER_00   \n",
       "14          72      82.4  SPEAKER_01   \n",
       "15          84      87.3  SPEAKER_00   \n",
       "16          87      88.7  SPEAKER_01   \n",
       "17          89      96.7  SPEAKER_00   \n",
       "18          96     100.4  SPEAKER_01   \n",
       "19         101     105.5  SPEAKER_00   \n",
       "20         105     114.6  SPEAKER_01   \n",
       "\n",
       "                                                 text  \n",
       "0           Our Primeti 33. What is yet as emergency?  \n",
       "1   Yes, sir, I need to, uh, uh. I need an ambulan...  \n",
       "2                                              A car.  \n",
       "3                              Carol Wood Drive. Yes.  \n",
       "4                                                 NaN  \n",
       "5                                            Yeah, I.  \n",
       "6   Okay, sir. What's the phone number you calling...  \n",
       "7   Uh, sir? Oh, I have a. We have a gentleman her...  \n",
       "8   Okay, how does he. He's a 50 years old. Ser 50...  \n",
       "9                       Yes, he's not breathing, sir.  \n",
       "10  Okay, and he's not conscious either. Don't con...  \n",
       "11      We need them. You get when you just party on.  \n",
       "12  Just you in a way there. We're on Wemo. But I ...  \n",
       "13  Yes, we have a personal doctor if you're with ...  \n",
       "14  Oh, okay, well, we're on our way there. If you...  \n",
       "15  Uh, no, just the doctor. So the doctor's been ...  \n",
       "16               Okay, so did doctor see what happen.  \n",
       "17  Uh, um, doctor, did you see what happens to an...  \n",
       "18  Okay, we're in a way. I'm just I'm just passin...  \n",
       "19  A you cure's pumping. He's pumping his chest, ...  \n",
       "20  Okay, okay, we're on it. We're less than a mil...  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = asr.retrieve_results(project_id, job_id)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
