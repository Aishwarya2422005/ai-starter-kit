{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ðŸ¤– Static Agentic RAG with CodeGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can at times expect an AI application to understand their queries, even if they are vague and contain embedded semantic logic, such as a short form query that is actually compound in nature.  There is no explicit breakdown of all subqueries, because domain experts generally understand what is meant by the short form query.  In production, this is a challenging task for an AI model to handle, as foundation models are generally trained on internet scale information, which is not gauranteed to contain domain specific logic.  In fact, this kind of logic could even be harmful for generalization, as the langugage would not make much sense with respect to other contexts.  \n",
    "\n",
    "Queries, even if clear, may be compound in nature and may require subquery decomposition for targeted QA using paradigms/techniques such as RAG.  This in turn may also necessitate the need for: reranking, vectorstore filtering for initial entity-aware document retrieval to remove/prevent noise, aggregation, etc.\n",
    "\n",
    "Furthermore, compound queries may be comparitive in nature and may require mathematical operations, which may require the use of a code generation model to generate the code for the mathematical operations.  Alternatively, agents with tools/libraries that can handle such operations can be used, which will add more determinism, and hopefully reliability.  \n",
    "\n",
    "Note that the longer and more complex the agentic pipeline, the more the possibilities for cumulative errors to come up, and so development and maintenance may become difficult to scale across edge cases.  If this is the case, it may be preferable to move to a dynamic, human in the loop type of system. That route has its own unique challenges, such as robust routing and team setups.  A simple example can be found here: [Corrective RAG team](./../../agent_workflows/notebooks/corrective_rag_team.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.vectordb.vector_db import VectorDb\n",
    "from utils.model_wrappers.api_gateway import APIGateway \n",
    "from utils.agents.static_RAG_with_coding import CodeRAG\n",
    "\n",
    "examples = []\n",
    "\n",
    "CONFIG_PATH = os.path.join(kit_dir, \"config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embedding model for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_info(CONFIG_PATH: str):\n",
    "        \"\"\"\n",
    "        Loads json config file\n",
    "        \"\"\"\n",
    "        # Read config file\n",
    "        with open(CONFIG_PATH, 'r') as yaml_file:\n",
    "            config = yaml.safe_load(yaml_file)\n",
    "        api_info = config[\"api\"]\n",
    "        llm_info =  config[\"llm\"]\n",
    "        embedding_model_info = config[\"embedding_model\"]\n",
    "        retrieval_info = config[\"retrieval\"]\n",
    "        prompts = config[\"prompts\"]\n",
    "        \n",
    "        return api_info, llm_info, embedding_model_info, retrieval_info, prompts\n",
    "\n",
    "def load_embedding_model(embedding_model_info: dict) -> None:\n",
    "        embeddings = APIGateway.load_embedding_model(\n",
    "            type=embedding_model_info[\"type\"],\n",
    "            batch_size=embedding_model_info[\"batch_size\"],\n",
    "            coe=embedding_model_info[\"coe\"],\n",
    "            select_expert=embedding_model_info[\"select_expert\"]\n",
    "            ) \n",
    "        return embeddings  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the vectorstore for use as a base database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, embedding_model_info, _, _ = get_config_info(CONFIG_PATH=CONFIG_PATH)\n",
    "embeddings = load_embedding_model(embedding_model_info=embedding_model_info)\n",
    "vdb = VectorDb()\n",
    "vectorstore = vdb.load_vdb(kit_dir + \"/data/uber_lyft.chromadb\", \n",
    "                           embeddings, \n",
    "                           collection_name='agent_workflows_default_collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CodeRAG static agent pipeline\n",
    "\n",
    "We are going to answer very complex, compound questions.  Numerous components will be used for both RAG [RagComponents](./../../utils/rag/rag_components.py), which inherits methods from [BaseComponents](./../../utils/rag/base_components.py).  [CodegenComponents](./../../utils/code_gen/codegen_components.py) will also be used.  For conciseness, components will not be listed in code here, but the links should route to the respective files/modules.  \n",
    "\n",
    "The long, CodeRAG pipeline will be explained here.  \n",
    "\n",
    "First, we need to define the state that will be required for this large-scale question-answering task:\n",
    "\n",
    "    class CodeRAGGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    A typed dictionary representing the state of a CodeRAG graph.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question.\n",
    "        subquestions: A list of subquestions, typically generated by a LLM agent.\n",
    "        entities: A list of entities determined by a LLM agent.\n",
    "        generation: The most recent generation from a LLM, which may include code generation.\n",
    "        documents: A list of documents that were retrieved from the vectorstore and passed retrieval grading filtering.\n",
    "        answers: A list of answers that have been accumulated from the pipeline.\n",
    "        original_question: The original question, which may be needed if subquestions were generated, etc.\n",
    "        code: The generated code from a LLM agent.\n",
    "        runnable: A binary flag of \"executed\" or \"exception\", depending on if code exeution succeeded or not.\n",
    "        error: The error message from an exception if any occurred.\n",
    "        rag_counter: The RAG counter.\n",
    "        code_counter: The code counter, which is used for maximum amount of retries after catching exceptions.\n",
    "        examples: A list of examples for query reformulation.\n",
    "    \"\"\"\n",
    "\n",
    "    question : str\n",
    "    subquestions: List[str]\n",
    "    entities: List[str]\n",
    "    generation : str\n",
    "    documents : List[str]\n",
    "    answers: List[str] \n",
    "    original_question: str \n",
    "    code: str\n",
    "    runnable: str\n",
    "    error: str\n",
    "    rag_counter: int\n",
    "    code_counter: int\n",
    "    examples: Optional[list]\n",
    "\n",
    "We will need state for the question, subquestions (that will be proposed by an agent), entities (that will be determined from an agent), generation (the response that is generated at various points of the pipeline), documents (the documents that are retrieved from the vectorstore), answers (the accumulated answers that are provided if subqueries were generated for answering), original_question (logging the original question in case of subquery generation and answering), code (the code that is generated by the codegen agent), runnable (the runnable code flag - executed or exception, that is generated by the agents), error (an exception message after running the Python REPL tool), rag_counter (the counter for the RAG agents), code_counter (the counter for the code generation agents), and examples (provided for the prompt reformulation, if needed).\n",
    "\n",
    "Next, we will add the nodes for all the components we will need for th graph (excluding components to be used for conditional edges):\n",
    "\n",
    "def create_rag_nodes(self) -> StateGraph:\n",
    "        \"\"\"\n",
    "        Creates the nodes for the CodeRAG graph state.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            The StateGraph object containing the nodes for the CodeRAG graph state.\n",
    "        \"\"\"\n",
    "\n",
    "        workflow: StateGraph = StateGraph(CodeRAGGraphState)\n",
    "\n",
    "        # Define the nodes\n",
    "        workflow.add_node(\"initialize_code_rag\", self.initialize_code_rag)\n",
    "        workflow.add_node(\"reformulate_query\", self.reformulate_query)\n",
    "        workflow.add_node(\"get_new_query\", self.pass_state)\n",
    "        workflow.add_node(\"generate_subquestions\", self.generate_subquestions)\n",
    "        workflow.add_node(\"detect_entities\", self.detect_entities)\n",
    "        workflow.add_node(\"retrieve\", self.retrieve_w_filtering)\n",
    "        workflow.add_node(\"grade_documents\", self.grade_documents)\n",
    "        workflow.add_node(\"generate\", self.rag_generate)\n",
    "        workflow.add_node(\"pass_from_qa\", self.pass_state)\n",
    "        workflow.add_node(\"pass_to_codegen\", self.pass_to_codegen)\n",
    "        workflow.add_node(\"code_generation\", self.code_generation)\n",
    "        workflow.add_node(\"determine_runnable_code\", self.determine_runnable_code)\n",
    "        workflow.add_node(\"refactor_code\", self.refactor_code)\n",
    "        workflow.add_node(\"code_error_msg\", self.code_error_msg)\n",
    "        workflow.add_node(\"failure_msg\", self.failure_msg)\n",
    "        workflow.add_node(\"aggregate_answers\", self.aggregate_answers)\n",
    "        workflow.add_node(\"return_final_answer\", self.final_answer)\n",
    "\n",
    "        return workflow\n",
    "\n",
    "\n",
    "Now we will need to setup the edges and conditional edges of the graph for our apps conrtol flow:\n",
    "\n",
    "\n",
    "    def build_rag_graph(self, workflow: StateGraph) -> object:\n",
    "        \"\"\"\n",
    "        Builds a graph for the RAG workflow.\n",
    "\n",
    "        This method constructs a workflow graph that represents the sequence of tasks\n",
    "        performed by the RAG system. The graph is used to execute the workflow and\n",
    "        generate code.\n",
    "\n",
    "        Args:\n",
    "            workflow: The workflow object (StateGraph containing nodes) to be modified.\n",
    "\n",
    "        Returns:\n",
    "            The compiled application object for static CodeRAG\n",
    "        \"\"\"\n",
    "\n",
    "        # Build graph\n",
    "\n",
    "        checkpointer: MemorySaver = MemorySaver()\n",
    "\n",
    "        workflow.set_entry_point(\"initialize_code_rag\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"initialize_code_rag\",\n",
    "            self.use_examples,\n",
    "            {\n",
    "                \"answer_generation\": \"get_new_query\",\n",
    "                \"example_selection\": \"reformulate_query\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"reformulate_query\", \"get_new_query\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"get_new_query\",\n",
    "            self.route_question,\n",
    "            {\n",
    "                \"answer_generation\": \"detect_entities\",\n",
    "                \"subquery_generation\": \"generate_subquestions\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"generate_subquestions\", \"detect_entities\")\n",
    "        workflow.add_edge(\"detect_entities\", \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "        workflow.add_edge(\"grade_documents\", \"generate\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"generate\",\n",
    "            self.check_hallucinations,\n",
    "            {\n",
    "                \"not supported\": \"failure_msg\",\n",
    "                \"useful\": \"pass_from_qa\",\n",
    "                \"not useful\": \"failure_msg\",\n",
    "            }\n",
    "        )\n",
    "        workflow.add_edge(\"failure_msg\", \"pass_from_qa\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"pass_from_qa\", \n",
    "            self.determine_cont, \n",
    "            {\n",
    "                \"continue\": \"pass_to_codegen\",\n",
    "                \"iterate\": \"detect_entities\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"pass_to_codegen\",\n",
    "            self.route_question_to_code,\n",
    "            {\n",
    "                \"llm\": \"aggregate_answers\",\n",
    "                \"codegen\": \"code_generation\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"code_generation\", \"determine_runnable_code\")\n",
    "        workflow.add_conditional_edges(\n",
    "        \"determine_runnable_code\",\n",
    "        self.decide_to_refactor,\n",
    "        {\n",
    "            \"executed\": \"return_final_answer\", \n",
    "            \"exception\": \"refactor_code\",\n",
    "            \"unsuccessful\": \"code_error_msg\"\n",
    "        },\n",
    "        )\n",
    "        workflow.add_edge(\"refactor_code\", \"determine_runnable_code\")\n",
    "        workflow.add_edge(\"code_error_msg\", \"return_final_answer\")\n",
    "        workflow.add_edge(\"aggregate_answers\", \"return_final_answer\")\n",
    "        workflow.add_edge(\"return_final_answer\", END)\n",
    "\n",
    "        app: CompiledGraph = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "        return app\n",
    "\n",
    "*Note that conditional edges come from components methods, but are not instantiated as nodes.  They are internal routers and are only used as connectors between other nodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a config\n",
    "config = {\"configurable\": {\"thread_id\": \"1234\"}}\n",
    "\n",
    "# instantiate rag\n",
    "rag = CodeRAG(\n",
    "    configs=CONFIG_PATH,\n",
    "    embeddings=embeddings,\n",
    "    vectorstore=vectorstore,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Initialize chains\n",
    "rag.initialize()\n",
    "\n",
    "# Build nodes\n",
    "workflow = rag.create_rag_nodes()\n",
    "print(workflow)\n",
    "\n",
    "# Build graph\n",
    "app = rag.build_rag_graph(workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built this complex application.  Now let's visualize the graph and then have a discussion about different scenarios and how they may play out, if routed correctly via the agent system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.display_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of pipeline\n",
    "\n",
    "This complicated pipeline should demonstrate how seemingly straight forward user queries can require more and more complexity, which in turn means more optimization of agent system prompts, implementations for edge cases, etc.  It is recommended to start developing RAG systems for simple cases along with users and scale up complexity when really needed for user experience.  At some point, there will need to be a tradeoff between user experience/ease of use, latency (long chains will incur more and more calls to LLMs), and development challenges.\n",
    "\n",
    "### Flow:\n",
    "\n",
    "After initializing some counters to keep track of coding attempts, etc. The user query encounters the first conditional edge, which is an agent judge that determines, based on the [example router](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-example_judge.yaml) prompt, if the query needs reformulation or not.  If reformulation is needed, the judge determines the response to be \"example selection\".  Example selection routes to the reformulate query node. This node calls the reformulate_query method from the RAGComponents class.  This method calls the get_example_selector method to create a retriever for the examples.  After the retriever is instantiated, the get_examples method is called to obtain examples, based on the similarity of the example queries and the user query.  Each example contains a key value pair, which is the expected user query to be reformulated and the reformulated query.  A reformulation chain with the system prompt: [query reformulation](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-query_reformulation.yaml).  If no reformulation is needed, then the query simply passes to the next node, get_new_query, which is essentially just a pass through node that exists so we can connect a conditional edge to it (conditional edges need to be connected to nodes, not other edges).  \n",
    "\n",
    "The next conditional edge is another agent judge.  This agent determines if the query is compound in nature and requires query decomposition, via its system prompt [subquery router](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-subquery_router.yaml).  If subquery generation is required, the query is then routed to the generate subquestions node.  This node calls the generate_subquestion method from RAGComponents, which obtains the question from the current state and passes it to the subquery_chain, which uses the prompt [subquery generation](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-subquery_generation.yaml).  Subquestions are converted to a list by splitting on new lines and then returned to subquestions in the state as an update.  If no subqueries are required, the query is simply passed along again.  \n",
    "\n",
    "Either the question or list of subquestions are then passed to a loop, which would be flattened with the use of batch online inference, if latency requirements demand it.  This loop consists of: entity detection, retrieval with metadata filtering, document grading for relevancy, RAG generation, hallucination detection, and answer relevancy determination.  All of these steps ensure that high quality data is received and help to filter out noise when retrieving docuemtns/contexts and that answers are checked to be faithful to the source materials and helpful.  The entity detection is performed via a chain using the [entity determination](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-entity_determination.yaml) prompt and some logic that handles queries versus subqueries.  Entities are then used at the next node, retrieve, which calls the retrieve_w_filtering of RAGComponents to first filter the vectorstore by the entity name using metadata (simplified as filename, as the front ends typically involve creating vectorstores from files).  If reranking is chosen in the configs (and is recommended if not using a fine tuned embedding model), then reranking will be performed on the documents and they will be resorted.  The initial top k_retrieved_documents is the initial amount of documents to retrieve and rerank, and the final_k_retrieved_documents is the amount of documents to keep and hand to the rest of the chain.  Sequence length of the model is the main factor controlling the final k, as document grading should filter out irrelevant documents.  After documents have been obtained and passed into the graph state, they are then graded for relevancy, which is done by the grade_documents method of RAGComponents.  The grading is done by comparing the query with the document and using the retrieval_grader chain with the [retrieval grading](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-retrieval_grading.yaml) prompt.  A binary score \"yes\" or \"no\" is returned.  If the score is \"yes\", the document is considered relevant, if \"no\", it is not.  The grading is then used to filter out irrelevant documents.  Only relevant documents are then passed to the next node, RAG generation.  With the final set of documents, RAG generation is performed at the generate node, which calls the rag_generate method from the RAGComponents class.  This method formats the list of documents into a string separated by new lines and passes the formated documents to the qa_chain.  This chain takes the question (or subquestion) and the formatted documents and is instructed by the [qa](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-qa.yaml) prompt.  The current generation is updated in the state and appended to the answer list of the state.  A conditional edge is then called to check the generation for both hallucinations (the response should be grounded in the documents) and answer relevancy (the generation should actually answer the question/subquestion).  Hallucinations are checked first.  Documents are formatted, as they were in the generation step, and then passed to the hallucination chain.  The hallucination chain uses the [hallucination detection](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-hallucination_detection.yaml) prompt to provide the binary score \"yes\" or \"no\".  If scored \"no\", then the generation will be routed to the failure message node.  If \"yes\", the generation is then checked by the grading chain, which takes the question/subquestion and generation and assess relevancy based on the [retrieval grading](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-retrieval_grading.yaml) prompt.  The grade is also a binary \"yes\" or \"no\".  If \"no\", then the generation is also passed to the failure message.  If \"yes\", then the answer/generation is kept as is and the loop continues with the answer(s) appended in the state.  If the answer is deemed as not useful or if it contains hallucinations, the question is passed to the failure_chain that uses the [failure message](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-code_exec_failure.yaml) prompt to massage the message for the user, which includes providing some suggestions for finding information besides consulting the vectorstore.  \n",
    "\n",
    "Once the question and/or all subquestions have been answered, there needs to be a determination if simple answer aggregation and summarization is required or if mathematical reasoning is needed to properly answer the question. The pass_to_codegen node collects the original question and sets it again as the question.  This is just done for simple booking keeping purposes.  A conditional edge using the route_question_to_code method from the CodeGenComponents class, is connected to this node.  At this step, the question (the original question now) is passed to the code_router chain with the [code router](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-code_router.yaml) prompt.  This chain will output either \"llm\" or \"codegen\", depending on the nature of the question.  If \"llm\" is determined, the aggregate answers node will call the aggregate_answers method in RAGComponents.  This method will convert the list of answers to a string seperated by two new lines so they are still somewhat individual and the question (original question) and string formatted list of answers will be passed to the aggregation_chain, which is instructed by the [answer aggregation](../../prompt_engineering/prompts/llama3_8b-prompt_engineering-answer_aggregation.yaml) prompt to answer the question as best and as fully as possible, while also allowing for partial answering over nothing.  If \"codegen\" is determined, the question and list of answers (formatted as a string) will be passed to the code_generation node.  The node will call the code_generation method from the CodeGenComponents class.  This method takes the questions and formatted list of answers and determines variables from those answers and tries to write code with those varables and the question.  After the code has been written, the parsed Python code snippet is passed to the determine_runnable_code node, which calls both Python REPL and the codegen_qc chain.  *It should be noted that Python REPL can execute arbitrary code on the host machine (e.g., delete files, make network requests). Use with caution.  For more information general security guidelines please see https://python.langchain.com/v0.2/docs/security/.* - From Langchain documentation @ [Python REPL tool](https://python.langchain.com/v0.2/docs/integrations/tools/python/).  If the code can run without an exception it is set as the result.  If an exception occurs, it is caught and set as the result.  Once result is populated, there is some cleanup to prevent chain related exceptions (unfriendly string characters) and it is fed into the codegen_qc chain with instructions from: [codegen_qc](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-codegen_qc.yaml).  The output of this chain is either \"runnable\" or \"exception\" and is fed to the is_runnable state.  At this point there is a conditional edge that calls decide_to_refactor from the CodeGenComponents class.  This method assesses the is_runnable output from the state and returns \"executed\", \"exception\", or \"unsuccessful\".  Both \"executed\" and \"exception\" are directly read from the state's last step.  However, \"unsuccessful\" is determined if the code_counter state variable (which is updated later) reaches the user's set limit via the configuration.  If there is an exception, the code and the exception message will be passed to the refactor_code node and method of the same name in CodeGen Components.  The code_counter will be updated by 1 and the refactor chain using the [code refactor](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-code_refactor.yaml) prompt will be invoked.  This will attempt to refactor the code, based on the model's codegen capabilities.  What follows is generally similar to the code_generation node.  Python REPL is used to try to run the refactored code.  If the code can run without an exception, the refactor_code method returns the state with updated: code, the resultant generation, and the updated code_counter.  If an exception occurs, the state is returned with updated: code, the error, and an updated code_counter.  The state is then fed back into the determine_runnable_code node to iteratively retry in a loop until either the code has been executed or the code_counter reaches the maximum number of attempts specified in the application configuration by the user.  If the code_counter exceeds the maximum number of attempts, there is routing to the code_error_msg node and method.  The final code and error/exception is then simply passed into a deterministic overall error message.  This message is appended to the answers list.  \n",
    "\n",
    "To summarize everything that has been done and to answer the original question, the original question and final generation are passed to the final_chain with the [final chain](./../../prompt_engineering/prompts/llama3_8b-prompt_engineering-final_chain.yaml) prompt.  This massages the information into one, final answer.  If the question could not be answered, it provides some suggestionso n how a user may be able to find more information.  \n",
    "\n",
    "### Considerations\n",
    "\n",
    "This example should provide insight, through working examples, of how seemingly simple user expectations can balloon into very complicated implementations. Latency, cumulative error, and prompt optimization should all be considered.  It is recommended to build from simple queries to queries with a higher level of complexity and functionality demand as a project or application progresses to keep a healthy check on scaling needs for the application and to know when enough has been implemented.  In many cases, it may be that a hierarchical approach with human in the loop guidance is a better end experience than a very long and involved pipeline, even if latency is reduced via custom hardware, such as SambaNova Systems RDUs due to the other challenges mentioned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(question: str) -> None:\n",
    "    \n",
    "    try:\n",
    "        response = rag.call_rag(app, question, config)\n",
    "    except Exception as e:\n",
    "        response = {}\n",
    "        response[\"answer\"] = str(e)\n",
    "\n",
    "    display(Markdown(\"---Response---\"))\n",
    "    display(Markdown(response[\"answer\"]))\n",
    "\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a number of questions of varying levels of complexity and functionality demands and assess where out of the box Llama 3 70B instruct lands for all LLM calls, without fine tuning of either the LLM or embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Could the trading price of lyft stock be volatile?\",\n",
    "    \"Could the trading price of uber stock be volatile?\",\n",
    "    \"What was the difference in revenue for Uber between 2020 and 2021?\",\n",
    "    \"What was the difference in revenue for Lyft between 2020 and 2021?\",\n",
    "    \"What was the change in revenue for Lyft for 2020 and 2021 as a percentage?\",\n",
    "    \"What is 5 + 5?\",\n",
    "    \"Provide the business overviews for Uber and Lyft.\",\n",
    "    \"What are the growth strategies for Lyft and Uber?\",\n",
    "    \"How do the growth strategies differ between Lyft and Uber?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for question in questions:\n",
    "    answers.append(run_pipeline(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the question and answer pairs to get a sense of how the pipeline worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ques, ans in zip(questions, answers):\n",
    "    display(Markdown(ques))\n",
    "    display(Markdown(\"---ANSWER---\"))\n",
    "    display(Markdown(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Many questions have been answered, but some seem to be unanswered and/or partially answered.  It generally looks like the cause is challenges with retrieval, but these questions are also naive and made without deep understanding of these documents, so the information may not always be present.  The agentic pipeline, though, seems to have all the capabilities it needs to answer these kinds of questions if information can be retrieved, from subquery generation to codegen for arithmetic reasoning.\n",
    "\n",
    "### Questions\n",
    "\n",
    "#### Could the trading price of lyft stock be volatile?\n",
    "Information was retrieved that directly suggests that the stock may be volatile.  \n",
    "\n",
    "#### Could the trading price of uber stock be volatile?\n",
    "No explicit mention to stock volatility was found.  However, retrieved contexts contain fairly concrete information that suggests that the stock could be volatile, so the LLM was able to determine that the stock could be volatile.\n",
    "\n",
    "#### What was the difference in revenue for Uber between 2020 and 2021?\n",
    "Information for the revenue for both years was retrieved.  The original question suggested that math was required, so information was routed to codegen.  The information from the answers list was able to be parsed for inputs for the basic code that the codegen module put together.  Since this was simple code, it was executed without any errors and so refactoring loops were required.  This output was then passed to the final message node to be reformatted/stylized.\n",
    "\n",
    "#### What was the difference in revenue for Lyft between 2020 and 2021?\n",
    "Information for the revenue for both years was retrieved (it was necessary to set a high initial and final k value), incurring latency so embedding model fine tuning should be considered.  The original question suggested that math was required, so information was routed to codegen.  The information from the answers list was able to be parsed for inputs for the basic code that the codegen module, so reliabble code was generated.  The simple code was executed successfully and the output was then routed to the the final message node. \n",
    "\n",
    "#### What was the change in revenue for Lyft for 2020 and 2021 as a percentage?\n",
    "\n",
    "Information for the revenue for both years was retrieved, see the note above about latency and embedding model fine tuning.  Like the question above, the question required routing to codegen.  Codegen was successful with slightly more complex logic than above.  The code executed correctly and was the output was later massaged by the final message node.\n",
    "\n",
    "#### What is 5 + 5?\n",
    "\n",
    "No sources could be found for this simple question.  RAG is still performed, because there is no routing, but by the time the system is routed to the code routing node, it understands that the original question requires math.  The original query is routed to the codegen node(s).  Since this is a very basic coding question, the LLM was able to create code that was executable.  The output was then passed to the final message node to be reformatted/stylized.\n",
    "\n",
    "#### Provide the business overviews for Uber and Lyft.\n",
    "\n",
    "All of the information for this query was contained in natural language and it could be parsed successfully.  The system was able to retrieve the information from the sources and format it in a way that was easy to understand.  The information was then passed to the final message node to be reformatted/stylized.\n",
    "\n",
    "#### Provide the business overviews for Uber and Lyft.\n",
    "\n",
    "Similarly to the question above, all of the information for this query was contained in natural language and it could be parsed successfully.  The system was able to retrieve the information from the sources and format it in a way that was easy to understand.  The information was then passed to the final message node to be reformatted/stylized.\n",
    "\n",
    "#### What are the growth strategies for Lyft and Uber?\n",
    "\n",
    "All information was successfully retrieved for this question and could be found in the plain text.  The information was then passed to the final message node to be reformatted/stylized.\n",
    "\n",
    "#### How do the growth strategies differ between Lyft and Uber?\n",
    "\n",
    "All information was successfully retrieved for this question and could be found in the plain text.  The information was then passed to the final message node to be reformatted/stylized.\n",
    "\n",
    "### Improvement strategies\n",
    "\n",
    "The pipeline seems to be able to handle all forms of queries/question provided to it, at least in terms of capability.  However, the retrieval, particularly for tabular information, remains a challenge with the OOB embedding model.  There are multiple ways to improve accuracy here for latency reduction; the embedding model could be fine tuned to improve retrieval results.  If latency is acceptable for users, the reranking model can be used with a high initial and final k for documents to avoid the embedding model fine tuning effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
