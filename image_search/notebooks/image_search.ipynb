{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from dotenv import load_dotenv\n",
    "import tarfile\n",
    "load_dotenv(os.path.join(repo_dir,\".env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch image ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!snapi app list | grep CLIP -A 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config(os.path.join(kit_dir,'config.yaml'))\n",
    "\n",
    "PENDING_RDU_JOB_STATUS = 'PENDING_RDU'\n",
    "SUCCESS_JOB_STATUS = 'EXIT_WITH_0'\n",
    "FAILED_JOB_STATUS = 'FAILED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchClipProcessor():\n",
    "    \n",
    "    def __init__(self, config) -> None:\n",
    "        self.headers = {\n",
    "            'content-type': 'application/json',\n",
    "            'key': os.getenv('SAMBASTUDIO_KEY'),\n",
    "        }\n",
    "        self.datasets_path = f\".{config['clip']['datasets']['datasets_path']}\"\n",
    "        self.dataset_id = None\n",
    "        self.dataset_name = config['clip']['datasets']['dataset_name']\n",
    "        self.dataset_description = config['clip']['datasets']['dataset_description']\n",
    "        self.dataset_source_type = config['clip']['datasets']['dataset_source_type']\n",
    "        self.dataset_source_file = f\".{config['clip']['datasets']['dataset_source_file']}\"\n",
    "        \n",
    "        self.clip_app_id = config['clip']['apps']['clip_app_id']\n",
    "        \n",
    "        self.base_url = config['clip']['urls']['base_url']\n",
    "        self.datasets_url = config['clip']['urls']['datasets_url'] \n",
    "        self.projects_url = config['clip']['urls']['projects_url'] \n",
    "        self.jobs_url = config['clip']['urls']['jobs_url'] \n",
    "        self.download_results_url = config['clip']['urls']['download_results_url'] \n",
    "    \n",
    "        self.project_name = config['clip']['projects']['project_name']\n",
    "        self.project_description = config['clip']['projects']['project_description']\n",
    "        self.project_id=None\n",
    "        \n",
    "        self.job_name = config['clip']['jobs']['job_name']\n",
    "        self.job_task = config['clip']['jobs']['job_task']\n",
    "        self.job_type = config['clip']['jobs']['job_type']\n",
    "        self.job_description = config['clip']['jobs']['job_description']\n",
    "        self.model_checkpoint = config['clip']['jobs']['model_checkpoint']\n",
    "        \n",
    "        self.output_path = config['clip']['output']['output_path']\n",
    "        \n",
    "        \n",
    "    def _get_call(self, url, params = None, success_message = None):\n",
    "        response = requests.get(url, params=params, headers=self.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logging.info('GET request successful!')\n",
    "            logging.info(success_message)\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'GET request failed with status code: {response.status_code}')\n",
    "            logging.error(f'Error message: {response.text}')\n",
    "        return response\n",
    "\n",
    "    def _post_call(self, url, params, success_message = None):\n",
    "        response = requests.post(url, json=params, headers=self.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logging.info('POST request successful!')\n",
    "            logging.info(success_message)\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'POST request failed with status code: {response.status_code}')\n",
    "            raise Exception(f'Error message: {response.text}')\n",
    "        return response\n",
    "    \n",
    "    def _delete_call(self, url):\n",
    "        response = requests.delete(url, headers=self.headers)    \n",
    "        if response.status_code == 200:\n",
    "            logging.info(f'Dataset {self.dataset_name} deleted successfully.')\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'Failed to delete the resource. Status code: {response.status_code}')\n",
    "            raise Exception(f'Error message: {response.text}')    \n",
    "        return response\n",
    "    def _generate_csv(self, dataset_dir):\n",
    "        image_paths = []\n",
    "        for root, dirs, files in os.walk(dataset_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "                    image_path = os.path.relpath(os.path.join(root, file), dataset_dir)\n",
    "                    image_paths.append(image_path)\n",
    "\n",
    "        df = pd.DataFrame({'image_path': image_paths, 'description': '', 'subset': '', 'metadata': ''})\n",
    "        df.to_csv(os.path.join(dataset_dir,'labels.csv'), index=False)\n",
    "\n",
    "    def _get_df_output(self, response_content: str) -> DataFrame:\n",
    "        compressed_bytes = io.BytesIO(response_content)\n",
    "        \n",
    "        with tarfile.open(fileobj=compressed_bytes, mode=\"r:gz\") as tar:\n",
    "            output_tar_member = tar.getmember(self.output_path)\n",
    "            output_file = tar.extractfile(output_tar_member)\n",
    "            output_df = pd.read_json(io.BytesIO(output_file.read()), lines=True)       \n",
    "        return output_df\n",
    "\n",
    "    def search_dataset(self, dataset_name):\n",
    "        url = self.base_url + self.datasets_url + '/search'\n",
    "        params = {\n",
    "            'dataset_name': dataset_name\n",
    "        }\n",
    "        response = self._get_call(url, params, f'Dataset {dataset_name} found in SambaStudio')\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        return parsed_reponse['data']['dataset_id']\n",
    "\n",
    "    def delete_dataset(self, dataset_name):\n",
    "        dataset_id = self.search_dataset(dataset_name)\n",
    "        url = self.base_url + self.datasets_url + '/' + dataset_id\n",
    "        response = self._delete_call(url)\n",
    "        logging.info(response.text)\n",
    "        \n",
    "        \n",
    "    def create_dataset(self, path):\n",
    "        # create clip directory and source.json file\n",
    "        \n",
    "        dataset_name = f'{self.dataset_name}_{int(time.time())}'\n",
    "            \n",
    "        clip_directory = os.path.join(self.datasets_path, dataset_name)\n",
    "        \n",
    "        if not os.path.isdir(self.datasets_path):\n",
    "            os.mkdir(self.datasets_path) \n",
    "            \n",
    "        if not os.path.isdir(clip_directory):\n",
    "            logging.info(f'Datasets path: {clip_directory} dont \\'t found')\n",
    "            \n",
    "            source_file_data = {\n",
    "                \"source_path\": clip_directory\n",
    "            }\n",
    "            \n",
    "            with open(self.dataset_source_file, 'w') as json_file:\n",
    "                json.dump(source_file_data, json_file)\n",
    "\n",
    "        shutil.copytree(path, clip_directory)\n",
    "        \n",
    "        self._generate_csv(clip_directory)\n",
    "        \n",
    "        # create dataset\n",
    "        command = f'echo yes | snapi dataset add \\\n",
    "            --dataset-name {dataset_name} \\\n",
    "            --job_type {self.job_type} \\\n",
    "            --apps {self.clip_app_id} \\\n",
    "            --source_type {self.dataset_source_type} \\\n",
    "            --source_file {self.dataset_source_file} \\\n",
    "            --description \"{self.dataset_description}\"'\n",
    "        \n",
    "        os.system(command)\n",
    "        logging.info(f'Creating dataset: {dataset_name}')\n",
    "        \n",
    "        return dataset_name\n",
    "         \n",
    "    def check_dataset_creation_progress(self, dataset_name):\n",
    "        url = self.base_url + self.datasets_url + '/' + dataset_name\n",
    "        response = self._get_call(url)\n",
    "        if response.json()[\"data\"][\"status\"]==\"Available\": \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def create_load_project(self):\n",
    "\n",
    "        url = self.base_url + self.projects_url + '/' + self.project_name\n",
    "\n",
    "        response = self._get_call(url, success_message=f'Project {self.project_name} found in SambaStudio')\n",
    "        not_found_error_message = f\"{self.project_name} not found\"\n",
    "\n",
    "        if not_found_error_message in response.text:\n",
    "            \n",
    "            logging.info(f'Project {self.project_name} wasn\\'t found in SambaStudio')\n",
    "            \n",
    "            url = self.base_url + self.projects_url\n",
    "\n",
    "            params = {\n",
    "                'project_name': self.project_name,\n",
    "                'description': self.project_description\n",
    "            }\n",
    "\n",
    "            response = self._post_call(url, params, success_message=f'Project {self.project_name} created!')\n",
    "\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        self.project_id = parsed_reponse['data']['project_id']\n",
    "        return self.project_id\n",
    "    \n",
    "    def run_job(self, dataset_name):\n",
    "        \n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=self.project_id)\n",
    "        \n",
    "        params = {\n",
    "            'task': self.job_task,\n",
    "            'job_type': self.job_type,\n",
    "            'job_name': f'{self.job_name}_{int(time.time())}',\n",
    "            'project': self.project_id,\n",
    "            'model_checkpoint': self.model_checkpoint,\n",
    "            'description': self.job_description,\n",
    "            'dataset': dataset_name,\n",
    "        }\n",
    "\n",
    "        response = self._post_call(url, params, success_message='Job running')\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        job_id = parsed_reponse['data']['job_id']\n",
    "        \n",
    "        return job_id\n",
    "    \n",
    "    def check_job_progress(self, job_id):\n",
    "        \"\"\"Check job progress of a given job.\n",
    "\n",
    "        Args:\n",
    "            job_id (str): The id of the job to check.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True when the job is finished.\n",
    "        \"\"\"\n",
    "\n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=self.project_id) + '/' + job_id\n",
    "\n",
    "        status = PENDING_RDU_JOB_STATUS\n",
    "        while status != SUCCESS_JOB_STATUS:\n",
    "            response = self._get_call(url, success_message='Still waiting for job to finish')\n",
    "            parsed_reponse = json.loads(response.text)   \n",
    "            status = parsed_reponse['data']['status']\n",
    "            logging.info(f'Job status: {status}')\n",
    "            if status == SUCCESS_JOB_STATUS:\n",
    "                logging.info('Job finished!')\n",
    "                break\n",
    "            elif status == FAILED_JOB_STATUS:\n",
    "                logging.info('Job failed!')\n",
    "                return False\n",
    "            time.sleep(10)\n",
    "        \n",
    "        return True  \n",
    "    \n",
    "    def delete_job(self, job_id):\n",
    "        url = self.base_url +  self.projects_url + self.jobs_url.format(project_id=self.project_id) + '/' + job_id\n",
    "        response = self._delete_call(url)\n",
    "        logging.info(response.text)\n",
    "        \n",
    "    def retrieve_results(self, job_id):\n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=self.project_id) + '/' + job_id + self.download_results_url\n",
    "        response = self._get_call(url, success_message='Results downloaded!')\n",
    "        df = self._get_df_output(response.content)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = BatchClipProcessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.create_load_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = clip.create_dataset(path=os.path.join(kit_dir,'data/images'))\n",
    "while not clip.check_dataset_creation_progress(dataset_name):\n",
    "    print(\"waiting for dataset available\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.search_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = clip.run_job(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clip.check_job_progress(job_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=clip.retrieve_results(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.delete_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.delete_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use inference endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(img_path= None, img=None):\n",
    "    base_url = os.environ.get(\"BASE_URL\")\n",
    "    api_key = os.environ.get(\"API_KEY\")\n",
    "    project_id = os.environ.get(\"PROJECT_ID\")\n",
    "    endpoint_id = os.environ.get(\"ENDPOINT_ID\") \n",
    "    url = f\"{base_url}/api/predict/file/{project_id}/{endpoint_id}\"\n",
    "    if img_path:\n",
    "        files = {'predict_file': open(img_path, 'rb')}\n",
    "    elif img: \n",
    "        files = {'predict_file': img}\n",
    "    else:\n",
    "        raise Exception(\"please provide a image path or a bytes image file\")\n",
    "    headers = {'key': api_key}\n",
    "    response = requests.post(url, files=files, headers=headers)\n",
    "    return response.json()[\"data\"][0]\n",
    "    \n",
    "jpg_file_path = os.path.join(kit_dir,\"data/images/animals/animals_0.png\")\n",
    "print(embed_image(jpg_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    base_url = os.environ.get(\"BASE_URL\")\n",
    "    api_key = os.environ.get(\"API_KEY\")\n",
    "    project_id = os.environ.get(\"PROJECT_ID\")\n",
    "    endpoint_id = os.environ.get(\"ENDPOINT_ID\") \n",
    "    url = f\"{base_url}/api/predict/nlp/{project_id}/{endpoint_id}\"\n",
    "    input_data = {\"inputs\": [text]}\n",
    "    headers = {'key': api_key, 'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, json=input_data, headers=headers)\n",
    "    return response.json()[\"data\"][0]\n",
    "    \n",
    "text = \"lion in the jungle\"\n",
    "print(embed_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from chromadb.api.types import is_image, is_document, Images,  Documents, EmbeddingFunction, Embeddings, Protocol\n",
    "from typing import cast, Union, TypeVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chromadb multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddable = Union[Documents, Images]\n",
    "D = TypeVar(\"D\", bound=Embeddable, contravariant=True)\n",
    "\n",
    "class ClipEmbbeding(EmbeddingFunction[D]):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def __call__(self, input: D) -> Embeddings:\n",
    "        embeddings: Embeddings = []\n",
    "        for item in input:     \n",
    "            if is_document(item):\n",
    "                output = embed_text(item)\n",
    "            elif is_image(item):\n",
    "                image = Image.fromarray(item)\n",
    "                buffer = io.BytesIO()\n",
    "                image.save(buffer, format='PNG')\n",
    "                output = embed_image(img=buffer.getvalue())\n",
    "            embeddings.append(output)\n",
    "        return cast(Embeddings, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=os.path.join(kit_dir,\"data\"))\n",
    "clip_embedding=ClipEmbbeding()\n",
    "try:\n",
    "    client.delete_collection(name=\"image_collection\")\n",
    "except:\n",
    "    pass\n",
    "collection=client.get_or_create_collection(name=\"image_collection\", embedding_function=clip_embedding, metadata={\"hnsw:space\": \"l2\"})\n",
    "collection.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add individual images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(folder_path):\n",
    "    images=[]\n",
    "    paths=[]\n",
    "    for root, _dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "                path=os.path.join(root, file)\n",
    "                paths.append(path)\n",
    "                image= np.array(Image.open(os.path.join(root, file)))\n",
    "                images.append(image)\n",
    "    return paths,images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, images=get_images(os.path.join(kit_dir,\"data/images\"))\n",
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    images=images,\n",
    "    metadatas=[{\"source\": path} for path in paths],\n",
    "    ids=paths,\n",
    "    uris=paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get()#include=[\"uris\",\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add batch preprocessed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = list(df[\"predictions\"]) \n",
    "paths = list(df[\"input\"].apply(lambda x: os.path.join(kit_dir,'data/images',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    metadatas=[{\"source\": path} for path in paths],\n",
    "    ids=paths,\n",
    "    uris=paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get()#include=[\"uris\",\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image_by_text(query,n=3):\n",
    "    result=collection.query(query_texts=[query],include=[\"uris\", \"distances\"],n_results=n)\n",
    "    return result['uris'][0], result[\"distances\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image_by_image(path,n=5):\n",
    "    image= np.array(Image.open(path))\n",
    "    result=collection.query(query_images=[image],include=[\"uris\", \"distances\"],n_results=n)\n",
    "    return result['uris'][0], result[\"distances\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_images(image_paths, distances):\n",
    "    num_images = len(image_paths)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(10*num_images, 10))\n",
    "    \n",
    "    for i, path in enumerate(image_paths):\n",
    "        img = mpimg.imread(path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Image {i+1}, d={distances[i]}')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris, distances = search_image_by_text(\"iron\")\n",
    "show_images(uris, distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris, distances = search_image_by_image(\"../download.jpeg\")\n",
    "uris.insert(0, \"../download.jpeg\")\n",
    "distances.insert(0, 0)\n",
    "show_images(uris, distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
