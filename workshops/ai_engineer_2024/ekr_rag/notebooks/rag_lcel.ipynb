{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag\n",
      "/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/miniconda3/envs/complex_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petro\n",
      "/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle # remove\n",
    "import shutil\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\")) # absolute path for ekr_rag directory\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"../../../\")) # absolute path for starter-kit directory\n",
    "print(kit_dir)\n",
    "print(repo_dir)\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from src.document_retrieval import DocumentRetrieval\n",
    "\n",
    "CONFIG_PATH = os.path.join(kit_dir,'config.yaml')\n",
    "PERSIST_DIRECTORY = os.path.join(kit_dir,f\"data/my-vector-db\")\n",
    "#save_location = kit_dir + \"/data/my-vector-db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pypdf2\n",
      "page_content='Source: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405, Text: \\nSambaNova SN40L: Scaling the AI Memory Wall\\nwith Dataﬂow and Composition of Experts\\nRaghu Prabhakar, Ram Sivaramakrishnan, Darshan Gandhi, Yun Du, Mingran Wang, Xiangyu Song, Kejie Zhang,\\nTianren Gao, Angela Wang, Karen Li, Yongning Sheng, Joshua Brot, Denis Sokolov, Apurv Vivek, Calvin Leung,\\nArjun Sabnis, Jiayu Bai, Tuowen Zhao, Mark Gottscho, David Jackson, Mark Luttrell, Manish K. Shah, Edison Chen,\\nKaizhao Liang, Swayambhoo Jain, Urmish Thakker, Dawei Huang, Sumti Jairath, Kevin J. Brown, Kunle Olukotun\\nSambaNova Systems, Inc.\\nﬁrst.last@sambanova.ai\\nAbstract —Monolithic large language models (LLMs) like\\nGPT-4 have paved the way for modern generative AI\\napplications. Training, serving, and maintaining monolithic\\nLLMs at scale, however, remains prohibitively expensive\\nand challenging. The disproportionate increase in compute-\\nto-memory ratio of modern AI accelerators have created a\\nmemory wall, necessitating new methods to deploy AI. Recent\\nresearch has shown that a composition of many smaller expert\\nmodels, each with several orders of magnitude fewer parameters,\\ncan match or exceed the capabilities of monolithic LLMs.\\n' metadata={'filename': '/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405.07518v1_1page.pdf'}\n",
      "nb of chunks: 6\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# Specify PDF file\n",
    "pdf_file = kit_dir + '/data/tmp/2405.07518v1_1page.pdf'\n",
    "\n",
    "# Initialize DocumentRetrieval class\n",
    "documentRetrieval =  DocumentRetrieval()\n",
    "print(documentRetrieval.loaders['pdf'])\n",
    "\n",
    "# Get pdf text\n",
    "raw_text = []\n",
    "meta_data = []\n",
    "if documentRetrieval.loaders['pdf'] == \"unstructured\":\n",
    "    loader = UnstructuredPDFLoader(pdf_file)\n",
    "    docs_unstructured = loader.load()\n",
    "    for doc in docs_unstructured:\n",
    "        raw_text.append(doc.page_content)\n",
    "        meta_data.append({\"filename\": pdf_file})\n",
    "elif documentRetrieval.loaders['pdf'] == \"pypdf2\":\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    for page in pdf_reader.pages:\n",
    "        raw_text.append(page.extract_text())\n",
    "        meta_data.append({\"filename\": pdf_file})#, \"page\": page_number})\n",
    "else:\n",
    "    raise ValueError(f\"{self.documentRetrieval.loaders['pdf']} is not a valid pdf loader\")\n",
    "\n",
    "\n",
    "#raw_text, meta_data = documentRetrieval.get_data_for_splitting(docs)\n",
    "#with open(kit_dir+'/streamlit/'+'raw_text.pkl', 'rb') as file:\n",
    "#    raw_text = pickle.load(file)\n",
    "#with open(kit_dir+'/streamlit/'+'meta_data.pkl', 'rb') as file:\n",
    "#    meta_data = pickle.load(file)\n",
    "\n",
    "# Get the text chunks\n",
    "text_chunks = documentRetrieval.get_text_chunks_with_metadata(docs=raw_text, meta_data=meta_data)\n",
    "print(text_chunks[0])\n",
    "print('nb of chunks: %d'%len(text_chunks))\n",
    "#print(type(text_chunks[0])) # list of langchain_core.documents.base.Document\n",
    "#print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/miniconda3/envs/complex_rag/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "2024-06-20 23:45:54,290 [INFO] - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 23:45:55,926 [INFO] - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "The directory Chroma has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 23:45:56,424 [INFO] - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-06-20 23:46:02,715 [INFO] - Vector store saved to /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/my-vector-db\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "embeddings = documentRetrieval.load_embedding_model()\n",
    "if os.path.exists(PERSIST_DIRECTORY):\n",
    "    shutil.rmtree(PERSIST_DIRECTORY)\n",
    "    print(f\"The directory Chroma has been deleted.\")\n",
    "#vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=None)\n",
    "vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=PERSIST_DIRECTORY)\n",
    "\n",
    "# Create conversation chain\n",
    "documentRetrieval.init_retriever(vectorstore)\n",
    "conversation = documentRetrieval.get_qa_retrieval_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 23:46:06,945 [WARNING] - Number of requested results 15 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is composition of experts (CoE)?\n",
      "Composition of Experts (CoE) is a modular approach that involves combining many smaller expert models, each with fewer parameters, to match or exceed the capabilities of monolithic large language models (LLMs).\n",
      "3\n",
      "\n",
      "Source #1:\n",
      "Source: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405, Text: \n",
      "research has shown that a composition of many smaller expert\n",
      "models, each with several orders of magnitude fewer parameters,\n",
      "can match or exceed the capabilities of monolithic LLMs.\n",
      "Composition of Experts (CoE) is a modular approach that lowers\n",
      "the cost and complexity of training and serving. However, this\n",
      "approach presents two key challenges when using conventional\n",
      "hardware: (1) without fused operations, smaller models have\n",
      "lower operational intensity, which makes high utilization more\n",
      "challenging to achieve; and (2) hosting a large number of models\n",
      "can be either prohibitively expensive or slow when dynamically\n",
      "switching between them.\n",
      "In this paper, we describe how combining CoE, streaming\n",
      "dataﬂow, and a three-tier memory system scales the AI memory\n",
      "wall. We describe Samba-CoE, a CoE system with 150 experts\n",
      "and a trillion total parameters. We deploy Samba-CoE on the\n",
      "SambaNova SN40L Reconﬁgurable Dataﬂow Unit (RDU) – a\n",
      "commercial dataﬂow accelerator architecture that has been co-\n",
      "designed for enterprise inference and training applications. The\n",
      "chip introduces a new three-tier memory system with on-chip\n",
      "distributed SRAM, on-package HBM, and off-package DDR\n",
      "\n",
      "{'filename': '/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405.07518v1_1page.pdf'}\n",
      "\n",
      "Source #2:\n",
      "Source: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405, Text: \n",
      "SambaNova SN40L: Scaling the AI Memory Wall\n",
      "with Dataﬂow and Composition of Experts\n",
      "Raghu Prabhakar, Ram Sivaramakrishnan, Darshan Gandhi, Yun Du, Mingran Wang, Xiangyu Song, Kejie Zhang,\n",
      "Tianren Gao, Angela Wang, Karen Li, Yongning Sheng, Joshua Brot, Denis Sokolov, Apurv Vivek, Calvin Leung,\n",
      "Arjun Sabnis, Jiayu Bai, Tuowen Zhao, Mark Gottscho, David Jackson, Mark Luttrell, Manish K. Shah, Edison Chen,\n",
      "Kaizhao Liang, Swayambhoo Jain, Urmish Thakker, Dawei Huang, Sumti Jairath, Kevin J. Brown, Kunle Olukotun\n",
      "SambaNova Systems, Inc.\n",
      "ﬁrst.last@sambanova.ai\n",
      "Abstract —Monolithic large language models (LLMs) like\n",
      "GPT-4 have paved the way for modern generative AI\n",
      "applications. Training, serving, and maintaining monolithic\n",
      "LLMs at scale, however, remains prohibitively expensive\n",
      "and challenging. The disproportionate increase in compute-\n",
      "to-memory ratio of modern AI accelerators have created a\n",
      "memory wall, necessitating new methods to deploy AI. Recent\n",
      "research has shown that a composition of many smaller expert\n",
      "models, each with several orders of magnitude fewer parameters,\n",
      "can match or exceed the capabilities of monolithic LLMs.\n",
      "\n",
      "{'filename': '/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405.07518v1_1page.pdf'}\n",
      "\n",
      "Source #3:\n",
      "Source: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405, Text: \n",
      "from the web. However, training and serving a state-of-\n",
      "the-art monolithic LLM is both an extraordinarily expensive\n",
      "affair and a complex systems engineering challenge. Training\n",
      "requires building and operating a supercomputer composed ofthousands of hosts, purpose-built networks, power and cooling\n",
      "infrastructure, and thousands of accelerators – typically\n",
      "GPUs [ 29], [30] or TPUs [ 46]–[49]. The prohibitive cost\n",
      "and expertise required to train and serve 100s of billions of\n",
      "parameters put state-of-the-art AI capabilities out of reach\n",
      "for many academic researchers and smaller organizations,\n",
      "especially when on-premise deployments are needed. For\n",
      "instance, compute costs to train OpenAI’s GPT-4 is estimated\n",
      "to be $78 million USD, and Google’s Gemini Ultra to be $191\n",
      "million USD [ 57]. Building and deploying large monolithic\n",
      "models may not be sustainable for hyperscalers [ 14] or any\n",
      "organization needing capable AI models continuously trained\n",
      "and updated on their data [ 1], [10], [16]. Finally, systems\n",
      "that cater to monolothic models have scaled compute TFLOPs\n",
      "much faster than memory bandwidth and capacity, creating the\n",
      "memory wall [ 35] where the memory system can no longer\n",
      "\n",
      "{'filename': '/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405.07518v1_1page.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Ask questions about your data\n",
    "user_question = \"What is composition of experts (CoE)?\"\n",
    "\n",
    "response = conversation.invoke({\"question\":user_question})\n",
    "#print(response)\n",
    "print(response['question'])\n",
    "print(response['answer'])\n",
    "print(len(response['source_documents']))\n",
    "print('\\nSource #1:')\n",
    "print(response['source_documents'][0].page_content)\n",
    "print(response['source_documents'][0].metadata)\n",
    "\n",
    "print('\\nSource #2:')\n",
    "print(response['source_documents'][1].page_content)\n",
    "print(response['source_documents'][1].metadata)\n",
    "\n",
    "print('\\nSource #3:')\n",
    "print(response['source_documents'][2].page_content)\n",
    "print(response['source_documents'][2].metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complex_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
