{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1234567",
   "metadata": {},
   "source": [
    "# Calling Center of Excellence (CoE) Models\n",
    "\n",
    "This notebook demonstrates how to use the `use_coe_model.py` script to call the Center of Excellence (CoE) models using different approaches. We'll explore three examples:\n",
    "\n",
    "1. Using SambaVerse to call CoE Model\n",
    "2. Using SambaStudio to call CoE with Named Expert\n",
    "3. Using SambaStudio to call CoE with Routing\n",
    "\n",
    "Before we begin, make sure you have the `use_coe_model.py` script in the same directory as this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234568",
   "metadata": {},
   "source": [
    "## Example 1: Using SambaVerse to call CoE Model\n",
    "\n",
    "In this example, we'll use SambaVerse to call the CoE model. SambaVerse provides the expert name and their API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_coe_model import SambaNovaEmbeddingModel, SambaverseEndpoint, create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambaverse\n",
    "# llm:\n",
    "#   sambaverse_model_name: \"Mistral/Mistral-7B-Instruct-v0.2\"\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaverseEndpoint(\n",
    "    sambaverse_model_name=\"Mistral/Mistral-7B-Instruct-v0.2\",\n",
    "    sambaverse_api_key=os.getenv(\"SAMBAVERSE_API_KEY\"),\n",
    "    model_kwargs={\n",
    "        \"do_sample\": False,\n",
    "        \"max_tokens_to_generate\": 1024,\n",
    "        \"temperature\": 0.1,\n",
    "        \"process_prompt\": True,\n",
    "        \"select_expert\": \"Mistral-7B-Instruct-v0.2\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Give me the code for creating a vector db in langchain\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234570",
   "metadata": {},
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with a named expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_coe_model import SambaNovaEmbeddingModel, SambaNovaEndpoint, create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaNovaEndpoint(\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens_to_generate\": 1024,\n",
    "        \"select_expert\": \"Mistral-7B-Instruct-v0.2\",\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Give me the code for creating a vector db in langchain\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234572",
   "metadata": {},
   "source": [
    "## Example 3: Using SambaStudio to call CoE with Routing\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with routing. The script will automatically determine the appropriate expert based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_coe_model import SambaNovaEmbeddingModel, SambaNovaEndpoint, create_stuff_documents_chain, create_retrieval_chain, get_expert, get_expert_val\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   coe_routing: true\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "user_query = \"Give me the code for creating a vector db in langchain\"\n",
    "\n",
    "# Get the expert by calling SambaStudio with a custom prompt workflow\n",
    "expert_response = get_expert(user_query)\n",
    "print(f\"Expert response: {expert_response}\")\n",
    "\n",
    "# Extract the expert name from the response\n",
    "expert = get_expert_val(expert_response)\n",
    "print(f\"Expert: {expert}\")\n",
    "\n",
    "# Look up the model name based on the expert\n",
    "named_expert = {\n",
    "    \"Finance expert\": \"finance-chat\",\n",
    "    \"Math expert\": \"deepseek-llm-67b-chat\",\n",
    "    \"Code expert\": \"deepseek-llm-67b-chat\",\n",
    "    \"Medical expert\": \"medicine-chat\",\n",
    "    \"Legal expert\": \"law-chat\",\n",
    "    \"Generalist\": \"Mistral-7B-Instruct-v0.2\",\n",
    "}[expert]\n",
    "print(f\"Named expert: {named_expert}\")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaNovaEndpoint(\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens_to_generate\": 1024,\n",
    "        \"select_expert\": named_expert,\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(f\"Response: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234574",
   "metadata": {},
   "source": [
    "In each example, we walk through the following steps:\n",
    "\n",
    "1. Update the `config.yaml` file with the appropriate API information and LLM parameters.\n",
    "2. Create a `SambaNovaEmbeddingModel` object for embeddings.\n",
    "3. Load documents from a URL and split them into chunks.\n",
    "4. Create a vector database using Chroma.\n",
    "5. Define the prompt template.\n",
    "6. Set up the language model based on the example configuration.\n",
    "7. Create the document chain and retrieval chain.\n",
    "8. Invoke the retrieval chain with the user query.\n",
    "9. Print the response.\n",
    "\n",
    "For Example 3, we additionally:\n",
    "- Call `get_expert()` to determine the appropriate expert based on the user query.\n",
    "- Extract the expert name using `get_expert_val()`.\n",
    "- Look up the model name based on the expert.\n",
    "\n",
    "Feel free to explore and experiment with different configurations and queries to see how the CoE models respond!\n",
    "\n",
    "If you have any questions or need further assistance, please don't hesitate to ask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
