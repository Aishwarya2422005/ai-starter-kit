{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1234567",
   "metadata": {},
   "source": [
    "# Calling Composition Of Experts (CoE) Models\n",
    "\n",
    "This notebook demonstrates how to use the `use_coe_model.py` script to call the Composition Of Experts (CoE) models using different approaches. We'll explore three examples:\n",
    "\n",
    "1. Using SambaVerse to call CoE Model\n",
    "2. Using SambaStudio to call CoE with Named Expert\n",
    "3. Using SambaStudio to call CoE with Routing\n",
    "\n",
    "Before we begin, make sure you have the `use_coe_model.py` script in the same directory as this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234568",
   "metadata": {},
   "source": [
    "## Example 1: Using SambaVerse to call CoE Model\n",
    "\n",
    "In this example, we'll use SambaVerse to call the CoE model. SambaVerse provides the expert name and their API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "497a4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1234569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:langchain_community.document_loaders.web_base:fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is primarily designed for building production-grade LLM applications. However, it does provide some capabilities that can be useful for testing purposes.\n",
      "\n",
      "One way to use LangSmith for testing is by using its tracing capabilities. Tracing allows you to record and analyze the execution of your LLM application. This can be useful for identifying and debugging issues in your application.\n",
      "\n",
      "Another way to use LangSmith for testing is by using its evaluation capabilities. Evaluation allows you to automatically grade the output of your LLM application against a set of predefined criteria. This can be useful for quickly identifying the performance of your application and for providing objective feedback to your development team.\n",
      "\n",
      "In summary, while LangSmith is primarily designed for building production-grade LLM applications, it does provide some capabilities that can be useful for testing purposes. These capabilities include tracing and evaluation. By using these capabilities, you can record and analyze the execution of your LLM application, identify and debug issues in your application, and provide objective feedback to your development team.\n"
     ]
    }
   ],
   "source": [
    "from use_CoE_model import SambaNovaEmbeddingModel, SambaverseEndpoint, create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambaverse\n",
    "# llm:\n",
    "#   sambaverse_model_name: \"Mistral/Mistral-7B-Instruct-v0.2\"\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load the config.yaml\n",
    "CONFIG_PATH = os.path.join(current_dir, \"config.yaml\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "\n",
    "# Since Embedding Models are only available on SambaStudio and not SambaVerse we create a local Hugging Face Embeddings Object\n",
    "# In the SambaStudio examples later we utilise an Embeddings Models hosted on SambaStudio\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings,collection_name='sambaverse_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaverseEndpoint(\n",
    "    sambaverse_model_name=llm_info[\"sambaverse_model_name\"],\n",
    "    sambaverse_api_key=os.getenv(\"SAMBAVERSE_API_KEY\"),\n",
    "    model_kwargs={\n",
    "        \"do_sample\": False,\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"process_prompt\": True,\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"How can you use langsmith for testing\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234570",
   "metadata": {},
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with a named expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1234571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_community.document_loaders.web_base:fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Answer: Based on the provided context, LangSmith is a language processing tool that can be used within applications. However, the context does not provide specific information on how to integrate LangSmith into applications. For more detailed instructions, you may want to refer to LangSmith's official documentation or contact their support team for assistance.\n"
     ]
    }
   ],
   "source": [
    "from use_CoE_model import SambaNovaEmbeddingModel, SambaNovaEndpoint, create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "# Load the config.yaml\n",
    "CONFIG_PATH = os.path.join(current_dir, \"config.yaml\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings,collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaNovaEndpoint(\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Tell me how I can use langsmith within applications\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234572",
   "metadata": {},
   "source": [
    "## Example 3: Using SambaStudio to call CoE with Routing\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with routing. The script will automatically determine the appropriate expert based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1234573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_community.document_loaders.web_base:fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router expert response: {'data': [{'stop_reason': 'end_of_text', 'completion': ' Based on the information provided, I would classify the message \"Tell me how I can use langsmith for testing\" as \"Code Generation\".\\n\\n<<detected category>>: Code Generation\\n\\nThe message is asking for information on how to use a tool called \"langsmith\" for testing, which is a programming concept related to writing and debugging code. Therefore, it falls under the category of \"Code Generation\".', 'total_tokens_count': 400.0, 'prompt': '{\"conversation_id\": \"sambaverse-conversation-id\", \"messages\": [{\"message_id\": 0, \"role\": \"user\", \"content\": \"Tell me how I can use langsmith for testing\"}], \"prompt\": \"<s>[INST] \\\\n\\\\nA message can be classified as only one of the following categories: \\'finance\\',  \\'economics\\',  \\'maths\\',  \\'code generation\\', \\'legal\\', \\'medical\\', \\'history\\' or \\'None of the above\\'.  \\\\n\\\\nExamples for few of these categories are given below:\\\\n- \\'code generation\\': Write a python program\\\\n- \\'code generation\\': Debug the following code\\\\n- \\'None of the above\\': Who are you?\\\\n- \\'None of the above\\': What are you?\\\\n- \\'None of the above\\': Where are you?\\\\n\\\\nBased on the above categories, classify this message: \\\\n\\\\nTell me how I can use langsmith for testing\\\\n\\\\nAlways remember the following instructions while classifying the given statement:\\\\n- Think carefully and if you are not highly certain then classify the  given statement as \\'None of the above\\'\\\\n- Always begin your response by putting the classified category of the given statement after  \\'<<detected category>>:\\'\\\\n- Explain you answer\\\\n\\\\n[/INST]\\\\n\"}', 'logprobs': {'top_logprobs': None, 'text_offset': None}, 'tokens': ['', 'Based', 'on', 'the', 'information', 'provided', ',', 'I', 'would', 'class', 'ify', 'the', 'message', '\"', 'T', 'ell', 'me', 'how', 'I', 'can', 'use', 'lang', 'sm', 'ith', 'for', 'testing', '\"', 'as', '\"', 'Code', 'Generation', '\".', '\\n', '\\n', '<<', 'det', 'ected', 'category', '>>', ':', 'Code', 'Generation', '\\n', '\\n', 'The', 'message', 'is', 'asking', 'for', 'information', 'on', 'how', 'to', 'use', 'a', 'tool', 'called', '\"', 'lang', 'sm', 'ith', '\"', 'for', 'testing', ',', 'which', 'is', 'a', 'programming', 'concept', 'related', 'to', 'writing', 'and', 'debugging', 'code', '.', 'Therefore', ',', 'it', 'falls', 'under', 'the', 'category', 'of', '\"', 'Code', 'Generation', '\".']}]}\n",
      "Routing Named Expert: Code expert\n",
      "Named expert Model Name: deepseek-llm-67b-chat\n",
      "Response:  my LLM application.\n",
      "\n",
      "    Answer: To use LangSmith for testing your LLM application, you can follow these steps:\n",
      "\n",
      "    1. Familiarize yourself with LangSmith: Start by reading the User Guide to learn about the workflows LangSmith supports at each stage of the LLM application lifecycle. This will give you a better understanding of how LangSmith can be used for testing your LLM application.\n",
      "\n",
      "    2. Explore the Evaluation capabilities: In the provided context, there is a section on Evaluation. Learn about the evaluation capabilities of LangSmith, which will help you assess the performance of your LLM application.\n",
      "\n",
      "    3. Utilize the Prompt Hub: The Prompt Hub is a prompt management tool built into LangSmith. Learn about it to understand how you can use it to manage and test your LLM application's prompts.\n",
      "\n",
      "    4. Review Additional Resources: The provided context mentions several additional resources, such as the LangSmith Cookbook, LangChain Python documentation, and LangChain JS documentation. Review these resources to gain more insights into using LangSmith for testing your LLM application.\n",
      "\n",
      "    5. Join the Discord community: Engage with the LangChain community on Discord to discuss your questions, share experiences, and learn from others who are using LangSmith for testing their LLM applications.\n",
      "\n",
      "By following these steps, you can effectively use LangSmith for testing your LLM application and ensure its performance and functionality meet your requirements.\n"
     ]
    }
   ],
   "source": [
    "from use_CoE_model import SambaNovaEmbeddingModel, SambaNovaEndpoint, create_stuff_documents_chain, create_retrieval_chain, get_expert, get_expert_val\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   coe_routing: true\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings,collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "user_query = \"Tell me how I can use langsmith for testing\"\n",
    "\n",
    "# Get the expert by calling SambaStudio with a custom prompt workflow\n",
    "expert_response = get_expert(user_query,use_requests=True)\n",
    "print(f\"Router expert response: {expert_response}\")\n",
    "\n",
    "# Extract the expert name from the response\n",
    "expert = get_expert_val(expert_response)\n",
    "print(f\"Routing Named Expert: {expert}\")\n",
    "\n",
    "# Look up the model name based on the expert\n",
    "named_expert = {\n",
    "    \"Finance expert\": \"finance-chat\",\n",
    "    \"Math expert\": \"deepseek-llm-67b-chat\",\n",
    "    \"Code expert\": \"deepseek-llm-67b-chat\",\n",
    "    \"Medical expert\": \"medicine-chat\",\n",
    "    \"Legal expert\": \"law-chat\",\n",
    "    \"Generalist\": \"Mistral-7B-Instruct-v0.2\",\n",
    "}[expert]\n",
    "print(f\"Named expert Model Name: {named_expert}\")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaNovaEndpoint(\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": named_expert,\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(f\"Response: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234574",
   "metadata": {},
   "source": [
    "In each example, we walk through the following steps:\n",
    "\n",
    "1. Update the `config.yaml` file with the appropriate API information and LLM parameters.\n",
    "2. Create a `SambaNovaEmbeddingModel` object for embeddings.\n",
    "3. Load documents from a URL and split them into chunks.\n",
    "4. Create a vector database using Chroma.\n",
    "5. Define the prompt template.\n",
    "6. Set up the language model based on the example configuration.\n",
    "7. Create the document chain and retrieval chain.\n",
    "8. Invoke the retrieval chain with the user query.\n",
    "9. Print the response.\n",
    "\n",
    "For Example 3, we additionally:\n",
    "- Call `get_expert()` to determine the appropriate expert based on the user query.\n",
    "- Extract the expert name using `get_expert_val()`.\n",
    "- Look up the model name based on the expert.\n",
    "\n",
    "Feel free to explore and experiment with different configurations and queries to see how the CoE models respond!\n",
    "\n",
    "If you have any questions or need further assistance, please don't hesitate to ask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
