{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Generate training files and train the model in SambaStudio\n",
    "\n",
    "This notebook is a guide of how to convert your jsonl files to hdf5 files that are the files you will need to upload in order to start a training job in Sambastudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative data preparation package dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [generative Data preparation package]() is a package developed by Sambanova systems used to prepare your data to be used as dataset in your SambaStudio Environment, this package is included as git submodule in the AI Starter kit, so for instalations you should onlY execute teh following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/jorgep/Documents/ask_public_own/ai-starter-kit/utils/fine_tuning/generative_data_prep\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alive-progress in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.1.5)\n",
      "Requirement already satisfied: gitpython in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.1.43)\n",
      "Requirement already satisfied: h5py in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.11.0)\n",
      "Requirement already satisfied: jsonlines in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (4.0.0)\n",
      "Requirement already satisfied: numpy in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (1.26.4)\n",
      "Requirement already satisfied: psutil in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (6.0.0)\n",
      "Requirement already satisfied: pydantic in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (6.0.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.2.0)\n",
      "Requirement already satisfied: tabulate in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.9.0)\n",
      "Requirement already satisfied: torch>=2.3 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (4.66.4)\n",
      "Requirement already satisfied: transformers in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (4.41.2)\n",
      "Requirement already satisfied: types-tabulate in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.9.0.20240106)\n",
      "Requirement already satisfied: filelock in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (1.13.0)\n",
      "Requirement already satisfied: networkx in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2024.5.0)\n",
      "Requirement already satisfied: about-time==4.2.1 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from alive-progress->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (4.2.1)\n",
      "Requirement already satisfied: grapheme==0.6.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from alive-progress->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.6.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from gitpython->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (4.0.11)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from jsonlines->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (23.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from pydantic->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from pydantic->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (0.4.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from jinja2->torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from requests->transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from requests->transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from requests->transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from requests->transformers->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages (from sympy->torch>=2.3->sambanova-generative-data-prep==0.1.dev66+g3b4779c) (1.3.0)\n",
      "Building wheels for collected packages: sambanova-generative-data-prep\n",
      "  Building wheel for sambanova-generative-data-prep (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sambanova-generative-data-prep: filename=sambanova_generative_data_prep-0.1.dev66+g3b4779c-py3-none-any.whl size=86027 sha256=68fdaca373d2bf47c26899b3ec35d75316f2c88ed1f4e0b61785938be5076565\n",
      "  Stored in directory: /private/var/folders/p4/y0q2kh796nx_k_yzfhxs57f00000gp/T/pip-ephem-wheel-cache-1sggcu80/wheels/df/4f/fb/02896327c293054033c9dc14409c1f07ee004b71bd3d093709\n",
      "Successfully built sambanova-generative-data-prep\n",
      "Installing collected packages: sambanova-generative-data-prep\n",
      "  Attempting uninstall: sambanova-generative-data-prep\n",
      "    Found existing installation: sambanova-generative-data-prep 0.1.dev66+g3b4779c\n",
      "    Uninstalling sambanova-generative-data-prep-0.1.dev66+g3b4779c:\n",
      "      Successfully uninstalled sambanova-generative-data-prep-0.1.dev66+g3b4779c\n",
      "Successfully installed sambanova-generative-data-prep-0.1.dev66+g3b4779c\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "generative_data_prep_dir = os.path.join(repo_dir, \"utils\", \"fine_tuning\", \"generative_data_prep\")\n",
    "! pip install $generative_data_prep_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative data preparation usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dataset generation you will need to specify the tokenizer of the model you want to train setting its model id in the `TOKENIZER` variable\n",
    "- [Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-hf): meta-llama/Llama-2-7b-hf\n",
    "- [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B): meta-llama/Meta-Llama-3-8B\n",
    "- [Mixtral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2): mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "> You will need to request access to the models to get the tokenizers in their HuggingFace spaces  [Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-hf), [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B), [Mixtral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain with squad-smol-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(kit_dir, \"data\" , \"pre-training\", \"pretrain-squad-smol-sql.jsonl\")\n",
    "OUTPUT_PATH = os.path.join(kit_dir, \"data\", \"output\", \"pretrain-squad-smol-sql\")\n",
    "TOKENIZER = \"meta-llama/Llama-2-7b-hf\"  # set with the model id\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "WARNING: /Users/jorgep/Documents/ask_public_own/ai-starter-kit/fine_tuning_sql/data/output/pretrain-squad-smol-sql already exists, new files will be written here.\n",
      "/Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "--------------------------------------------------------------------------------\n",
      "Size of input jsonl file is: 0.15 GB (148.6 MB)\n",
      "--------------------------------------------------------------------------------\n",
      "Running tokenization jobs locally, There are 8 processes working on it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|██████████████████████████████████████▏⚠︎| (!) 9546/10016 [95%] in 40.1s (237.91/s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Tokenization is complete, the output dataset is located at: /Users/jorgep/Documents/ask_public_own/ai-starter-kit/fine_tuning_sql/data/output/pretrain-squad-smol-sql\n",
      "--------------------------------------------------------------------------------\n",
      "Balancing hdf5 files to ensure they have the same number of sequences.\n",
      "------------------------------------Metrics------------------------------------\n",
      "╒════════════════════════════╤══════════╕\n",
      "│ Sequences                  │ 17934    │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Articles                   │ 10000    │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Dataset Tokens             │ 73457664 │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Prompt Tokens              │ 0        │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Completion Tokens          │ 73398229 │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Padding Tokens             │ 59435    │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Average Completion Length  │ 7339.82  │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Average Prompt Length      │ 0.0      │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Data Utilization           │ 100.00%  │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Dropped From Packing       │ 0.00%    │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Dropped From All Prompt    │ 0.00%    │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Sequence Utilization       │ 99.92%   │\n",
      "├────────────────────────────┼──────────┤\n",
      "│ Seq Completion Utilization │ 99.92%   │\n",
      "╘════════════════════════════╧══════════╛\n",
      "------------------------------------Complete------------------------------------\n",
      "Elapsed time: 0:01:31\n",
      "--------------------------------------------------------------------------------\n",
      "When training, please adhere to the dataset requirements provided below:\n",
      "    Max sequence length == 4096\n",
      "    Model vocabulary size == 32000\n",
      "    Batch size <= 560\n",
      "    Number of RDUs (data parallel workers) <= 32\n",
      "    Do eval must be False\n"
     ]
    }
   ],
   "source": [
    "%run -m generative_data_prep pipeline \\\n",
    "--input_file_path=$INPUT_PATH \\\n",
    "--output_path=$OUTPUT_PATH \\\n",
    "--pretrained_tokenizer=$TOKENIZER \\\n",
    "--max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "--shuffle=on_RAM \\\n",
    "--keep_split_jsonls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain with the stack dedup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(kit_dir, \"data\" , \"pre-training\", \"pretrain-the-stack-dedup.jsonl\")\n",
    "OUTPUT_PATH = os.path.join(kit_dir, \"data\", \"output\", \"pretrain-the-stack-dedup\")\n",
    "TOKENIZER = \"meta-llama/Llama-2-7b-hf\" \n",
    "MAX_SEQ_LENGTH = 4096\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m generative_data_prep pipeline \\\n",
    "--input_file_path=$INPUT_PATH \\\n",
    "--output_path=$OUTPUT_PATH \\\n",
    "--pretrained_tokenizer=$TOKENIZER \\\n",
    "--max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "--shuffle=on_RAM \\\n",
    "--keep_split_jsonls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune with the NSText2SQL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(kit_dir, \"data\" , \"fine-tuning\", \"fine-tune-nstext2sql.jsonl\")\n",
    "OUTPUT_PATH = os.path.join(kit_dir, \"data\", \"output\", \"fine-tune-nstext2sq\")\n",
    "TOKENIZER = \"meta-llama/Llama-2-7b-hf\" \n",
    "MAX_SEQ_LENGTH = 4096\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "WARNING: /Users/jorgep/Documents/ask_public_own/ai-starter-kit/fine_tuning_sql/data/output/fine-tune-nstext2sq already exists, new files will be written here.\n",
      "/Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "--------------------------------------------------------------------------------\n",
      "Size of input jsonl file is: 0.4 GB (408.08 MB)\n",
      "--------------------------------------------------------------------------------\n",
      "Running tokenization jobs locally, There are 8 processes working on it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|██████████████████████████████████████▍⚠︎| (!) 277156/289312 [96%] in 1:35.1 (2913.15/s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Tokenization is complete, the output dataset is located at: /Users/jorgep/Documents/ask_public_own/ai-starter-kit/fine_tuning_sql/data/output/fine-tune-nstext2sq\n",
      "--------------------------------------------------------------------------------\n",
      "Balancing hdf5 files to ensure they have the same number of sequences.\n",
      "------------------------------------Metrics------------------------------------\n",
      "╒════════════════════════════╤═══════════╕\n",
      "│ Sequences                  │ 35882     │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Articles                   │ 289288    │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Dataset Tokens             │ 146972672 │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Prompt Tokens              │ 125607979 │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Completion Tokens          │ 21300079  │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Padding Tokens             │ 64614     │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Average Completion Length  │ 73.63     │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Average Prompt Length      │ 434.2     │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Data Utilization           │ 100.00%   │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Dropped From Packing       │ 0.00%     │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Dropped From All Prompt    │ 0.00%     │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Sequence Utilization       │ 99.96%    │\n",
      "├────────────────────────────┼───────────┤\n",
      "│ Seq Completion Utilization │ 14.49%    │\n",
      "╘════════════════════════════╧═══════════╛\n",
      "------------------------------------Complete------------------------------------\n",
      "Elapsed time: 1:13:18\n",
      "--------------------------------------------------------------------------------\n",
      "When training, please adhere to the dataset requirements provided below:\n",
      "    Max sequence length == 4096\n",
      "    Model vocabulary size == 32000\n",
      "    Batch size <= 1121\n",
      "    Number of RDUs (data parallel workers) <= 32\n",
      "    Do eval must be False\n"
     ]
    }
   ],
   "source": [
    "%run -m generative_data_prep pipeline \\\n",
    "--input_file_path=$INPUT_PATH \\\n",
    "--output_path=$OUTPUT_PATH \\\n",
    "--pretrained_tokenizer=$TOKENIZER \\\n",
    "--max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "--shuffle=on_RAM \\\n",
    "--keep_split_jsonls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> find more details of the data prepariton proces in the [Generative data preparation Readme](../../utils/fine_tuning/generative_data_prep/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
