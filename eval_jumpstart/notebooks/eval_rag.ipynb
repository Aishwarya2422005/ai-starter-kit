{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the repo dir /Users/kwasia/Documents/Projects/ai-starter-kit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "print(f\"This is the repo dir {repo_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwasia/.pyenv/versions/3.11.3/envs/adi-rag-eval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.eval.rag_eval import (\n",
    "    RAGEvaluator,\n",
    "    RAGEvalConfig,\n",
    "    load_pipeline,\n",
    "    load_eval_dataframe,\n",
    ")\n",
    "from langchain_community.llms.sambanova import SambaStudio, Sambaverse\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config_path = \"../../utils/eval/config.yaml\"\n",
    "config = RAGEvalConfig(config_yaml_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf: ('llama38b', {'name': 'llama38b', 'model_kwargs': {'select_expert': 'Meta-Llama-3-8B-Instruct', 'process_prompt': False, 'max_tokens_to_generate': 512}})\n",
      "llm_name: llama38b\n",
      "llm_config: {'sambastudio_base_url': None, 'sambastudio_project_id': None, 'sambastudio_endpoint_id': None, 'sambastudio_api_key': None, 'model_kwargs': {'select_expert': 'Meta-Llama-3-8B-Instruct', 'process_prompt': False, 'max_tokens_to_generate': 512}}\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for SambaStudio\nsambastudio_base_url\n  none is not an allowed value (type=type_error.none.not_allowed)\nsambastudio_project_id\n  none is not an allowed value (type=type_error.none.not_allowed)\nsambastudio_endpoint_id\n  none is not an allowed value (type=type_error.none.not_allowed)\nsambastudio_api_key\n  none is not an allowed value (type=type_error.none.not_allowed)\n__root__\n  Did not find sambastudio_base_url, please add an environment variable `SAMBASTUDIO_BASE_URL` which contains it, or pass `sambastudio_base_url` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm_name)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm_config)\n\u001b[0;32m---> 24\u001b[0m     eval_llm \u001b[38;5;241m=\u001b[39m \u001b[43mSambaStudio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     eval_llms\u001b[38;5;241m.\u001b[39mappend((llm_name, eval_llm))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_llms:\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_llms)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/adi-rag-eval/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for SambaStudio\nsambastudio_base_url\n  none is not an allowed value (type=type_error.none.not_allowed)\nsambastudio_project_id\n  none is not an allowed value (type=type_error.none.not_allowed)\nsambastudio_endpoint_id\n  none is not an allowed value (type=type_error.none.not_allowed)\nsambastudio_api_key\n  none is not an allowed value (type=type_error.none.not_allowed)\n__root__\n  Did not find sambastudio_base_url, please add an environment variable `SAMBASTUDIO_BASE_URL` which contains it, or pass `sambastudio_base_url` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "# Create evaluator\n",
    "eval_llms = []\n",
    "for conf in config.eval_llm_configs:\n",
    "    print(\"conf:\", conf)\n",
    "    llm_name, llm_config = config.get_llm_config(conf)\n",
    "    print(\"llm_name:\", llm_name)\n",
    "    print(\"llm_config:\", llm_config)\n",
    "    eval_llm = SambaStudio(**llm_config)\n",
    "    eval_llms.append((llm_name, eval_llm))\n",
    "\n",
    "print(\"eval_llms:\", eval_llms)\n",
    "\n",
    "eval_embeddings = HuggingFaceInstructEmbeddings(model_name=config.embedding_model_name)\n",
    "evaluator = RAGEvaluator(eval_llms, eval_embeddings, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:11<00:00,  3.79s/it]\n",
      "Evaluating: 100%|██████████| 3/3 [00:50<00:00, 16.91s/it]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkwasi-ankomah\u001b[0m (\u001b[33mai-solutions-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kwasia/Documents/Projects/ai-starter-kit/eval_jumpstart/notebooks/wandb/run-20240529_020720-yl9j7ona</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval/runs/yl9j7ona' target=\"_blank\">test-run-1</a></strong> to <a href='https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval' target=\"_blank\">https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval/runs/yl9j7ona' target=\"_blank\">https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval/runs/yl9j7ona</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-run-1</strong> at: <a href='https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval/runs/yl9j7ona' target=\"_blank\">https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval/runs/yl9j7ona</a><br/> View project at: <a href='https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval' target=\"_blank\">https://sambanova.wandb.io/ai-solutions-team/rag-pipeline-eval</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240529_020720-yl9j7ona/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results 1: {'eval_llm1': {'answer_relevancy': 0.9160, 'answer_correctness': 0.9409, 'answer_similarity': 0.7636}, 'eval_llm2': {'answer_relevancy': 0.9961, 'answer_correctness': 0.2561, 'answer_similarity': 0.7636}}\n"
     ]
    }
   ],
   "source": [
    "# Use Case 1: CSV file with pre-generated answers, no context\n",
    "eval_df = pd.read_csv(\"../data/res.csv\")\n",
    "results1 = evaluator.evaluate(eval_df)\n",
    "print(\"Results 1:\", results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 2: CSV file with pre-generated answers and context\n",
    "eval_df = pd.read_csv(\"eval_set_with_answers_and_context.csv\")\n",
    "results2 = evaluator.evaluate(eval_df)\n",
    "print(\"Results 2:\", results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 3: CSV file without answers, generate with pipelines, no context\n",
    "eval_df = pd.read_csv(\"eval_set.csv\")\n",
    "pipelines = [\n",
    "    load_pipeline(\n",
    "        SambaStudio(\n",
    "            sambastudio_base_url=conf[\"base_url\"],\n",
    "            sambastudio_project_id=conf[\"project_id\"],\n",
    "            sambastudio_endpoint_id=conf[\"endpoint_id\"],\n",
    "            sambastudio_api_key=conf[\"api_key\"],\n",
    "            **conf[\"model_kwargs\"]\n",
    "        ),\n",
    "        config,\n",
    "    )\n",
    "    for conf in config.llm_configs\n",
    "]\n",
    "results3 = evaluator.evaluate(eval_df, pipelines)\n",
    "print(\"Results 3:\", results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 4: CSV file without answers, generate with pipelines, with context from vector DB\n",
    "eval_df = pd.read_csv(\"eval_set.csv\")\n",
    "pipelines = [\n",
    "    load_pipeline(\n",
    "        SambaStudio(\n",
    "            sambastudio_base_url=conf[\"base_url\"],\n",
    "            sambastudio_project_id=conf[\"project_id\"],\n",
    "            sambastudio_endpoint_id=conf[\"endpoint_id\"],\n",
    "            sambastudio_api_key=conf[\"api_key\"],\n",
    "            **conf[\"model_kwargs\"]\n",
    "        ),\n",
    "        config,\n",
    "    )\n",
    "    for conf in config.llm_configs\n",
    "]\n",
    "results4 = evaluator.evaluate(eval_df, pipelines)\n",
    "print(\"Results 4:\", results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 5: Evaluate on HF dataset\n",
    "config.config[\"eval_dataset\"][\"hf_dataset_name\"] = \"squad_v2\"\n",
    "eval_df = load_eval_dataframe(config)\n",
    "pipelines = [\n",
    "    load_pipeline(\n",
    "        SambaStudio(\n",
    "            sambastudio_base_url=conf[\"base_url\"],\n",
    "            sambastudio_project_id=conf[\"project_id\"],\n",
    "            sambastudio_endpoint_id=conf[\"endpoint_id\"],\n",
    "            sambastudio_api_key=conf[\"api_key\"],\n",
    "            **conf[\"model_kwargs\"]\n",
    "        ),\n",
    "        config,\n",
    "    )\n",
    "    for conf in config.llm_configs\n",
    "]\n",
    "results5 = evaluator.evaluate(eval_df, pipelines)\n",
    "print(\"Results 5:\", results5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adi-rag-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
