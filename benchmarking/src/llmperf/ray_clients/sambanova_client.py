import os

llmperf_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
src_dir = os.path.dirname(llmperf_dir)
benchmarking_dir = os.path.dirname(src_dir)
repo_dir = os.path.dirname(benchmarking_dir)

os.environ["PYTHONPATH"] = llmperf_dir + ":" +  src_dir + ":" + benchmarking_dir + ":" + repo_dir + ":" + os.environ.get("PYTHONPATH", "")

from llmperf.ray_llm_client import LLMClient
from llmperf.models import RequestConfig
from llmperf import common_metrics

import ray
import json
from typing import Any, Dict
import time
import requests
from transformers import LlamaTokenizerFast

@ray.remote
class SambaNovaLLMClient(LLMClient):

    def __init__(self):
        self.tokenizer = LlamaTokenizerFast.from_pretrained(
            "hf-internal-testing/llama-tokenizer"
        )
        
        

    def llm_request(self, request_config: RequestConfig) -> Dict[str, Any]:
        """Make a single completion request to a LLM API
        Returns:
            Metrics about the performance charateristics of the request.
            The text generated by the request to the LLM API.
            The request_config used to make the request. This is mainly for logging purposes.
        """

        
        base_url = os.environ.get("BASE_URL")
        project_id = os.environ.get("PROJECT_ID")
        endpoint_id = os.environ.get("ENDPOINT_ID")
        api_key = os.environ.get("API_KEY")

        time_to_new_tokens = []
        new_tokens_recieved = []
        ttft = 0
        prompt_len = 0
        generated_text = ""
        total_request_time = 0
        error_msg = ""
        error_response_code = ""

        metrics = {}
        metrics[common_metrics.ERROR_CODE] = None
        metrics[common_metrics.ERROR_MSG] = ""

        try:
            # Define the URL for the request
            if "COE" in request_config.model:
                path = f"/predict/generic/stream/{project_id}/{endpoint_id}"
            else:
                path = f"/predict/nlp/stream/{project_id}/{endpoint_id}"
            API_BASE_PATH = "/api"
            url = f"{base_url}{API_BASE_PATH}{path}"
            # Define the headers
            headers = {"key": api_key}
            # data
            data = self._get_data(request_config)

            # Make the POST request
            start_time = time.monotonic()
            with requests.post(
                url, headers=headers, json=data, stream=True
            ) as response:
                if response.status_code != 200:
                    error_msg = response.text
                    error_response_code = response.status_code
                    response.raise_for_status()

                if "COE" in request_config.model:
                    for chunk_orig in response.iter_lines(chunk_size=None):
                        chunk = chunk_orig.strip()
                        data = json.loads(chunk)
                        completion = data["result"]["status"]["complete"]
                        if completion is False:
                            continue
                    # Run the calculations
                    total_request_time = data["result"]["responses"][0]["total_latency"]

                    # TTFT: Total time to first token
                    ttft = data["result"]["responses"][0]["time_to_first_token"]
                    # print(data["result"]["responses"][0]["prompt_tokens_count"])
                    prompt_len = data["result"]["responses"][0]["prompt_tokens_count"]
                    generated_text = data["result"]["responses"][0]["completion"]
                else:
                    for chunk_orig in response.iter_lines(chunk_size=None):
                        chunk = chunk_orig.strip()
                        stem = b"data: "
                        if (len(chunk) <= len(stem)) or (chunk[: len(stem)] != stem):
                            continue
                        chunk = chunk[len(stem) :]
                        data = json.loads(chunk)
                        delta = data["stream_token"]

                        if delta:
                            # Tokenize the cumulative text
                            new_tokens_recieved.append(delta)
                            if not ttft:
                                ttft = time.monotonic() - start_time
                                time_to_new_tokens.append(ttft)
                            else:
                                time_to_new_tokens.append(
                                    time.monotonic() - most_recent_received_token_time
                                )
                            most_recent_received_token_time = time.monotonic()
                            generated_text += delta

                    # Run the calculations
                    total_request_time = time.monotonic() - start_time

                    # TTFT: Total time to first token
                    ttft = self._get_ttft(time_to_new_tokens, new_tokens_recieved)
                    # print(len(self.tokenizer.encode(request_config.prompt[0])))
                    prompt_len = len(self.tokenizer.encode(request_config.prompt[0]))
        except Exception as e:
            metrics[common_metrics.ERROR_MSG] = error_msg
            metrics[common_metrics.ERROR_CODE] = error_response_code
            print(f"Warning Or Error: {e}")

        metrics[common_metrics.INTER_TOKEN_LAT] = (
            total_request_time  # token_benchmark divides by token_count
        )
        metrics[common_metrics.TTFT] = ttft
        metrics[common_metrics.E2E_LAT] = total_request_time - ttft
        metrics[common_metrics.NUM_INPUT_TOKENS] = prompt_len
        return metrics, generated_text, request_config

    def _get_ttft(self, time_to_new_tokens, new_tokens_recieved):
        new_token_counts = [
            len(
                self.tokenizer.encode(
                    t.rstrip() if len(t) > 1 else t, add_special_tokens=False
                )
            )
            for t in new_tokens_recieved
        ]

        time_per_token = [
            tim / tok for tok, tim in zip(new_token_counts, time_to_new_tokens)
        ]
        avg_time_per_token = sum(time_per_token[1:-1]) / len(time_per_token[1:-1])
        # Compute ttft = time_to_get_first_set_of_tokens - num_first_set_of_tokens * avg_time_per_token
        ttft = time_to_new_tokens[0] - (new_token_counts[0] - 1) * avg_time_per_token
        return ttft

    def _get_data(self, request_config):
        prompt = request_config.prompt
        prompt, _ = prompt
        if isinstance(prompt, str) and "COE" not in request_config.model:
            prompt = [prompt]

        sampling_params = request_config.sampling_params
        if "COE" in request_config.model:
            sampling_params["select_expert"] = request_config.model.split("/")[-1]
        tuning_params_dict = {
            k: {"type": type(v).__name__, "value": str(v)}
            for k, v in (sampling_params.items())
        }
        tuning_params = json.dumps(tuning_params_dict)
        if "COE" in request_config.model:
            data = {"instance": prompt, "params": json.loads(tuning_params)}
        else:
            data = {"inputs": prompt, "params": json.loads(tuning_params)}
        return data


if __name__ == "__main__":

    # ray.init(local_mode=True)
    prompt = "Test this."
    client = SambaNovaLLMClient.remote()
    request_config = RequestConfig(
        prompt=(prompt, 10),
        model="COE/llama-2-7b-chat-hf",
        sampling_params={
            "temperature": 0.2,
            "max_tokens_to_generate": 256,
            "top_k": 40,
            "top_p": 0.95,
            "process_prompt": False,
        },
    )
    ray.get(client.llm_request.remote(request_config))