import sys

sys.path.append("./src")

from llmperf.ray_llm_client import LLMClient
from llmperf.models import RequestConfig
import ray
import os
import json
from typing import Any, Dict
from llmperf import common_metrics
import time
import requests
from transformers import LlamaTokenizerFast
import logging
from dotenv import load_dotenv
import warnings
from math import isclose

warnings.filterwarnings("ignore")


@ray.remote
class SambaNovaLLMClient(LLMClient):
    """SambaNova LLM Client Completions API."""

    def __init__(self):
        self.tokenizer = LlamaTokenizerFast.from_pretrained(
            "hf-internal-testing/llama-tokenizer"
        )
        self.get_token_length = lambda text: len(self.tokenizer.encode(text))

    def llm_request(self, request_config: RequestConfig) -> tuple:
        """Makes a single completion request to a LLM API

        Args:
            request_config (RequestConfig): config options including user's prompt and LLM parameters

        Returns:
            tuple: Metrics about the performance charateristics of the request.
            The text generated by the request to the LLM API.
            The request_config used to make the request. This is mainly for logging purposes.
        """
        base_url = os.environ.get("BASE_URL")
        project_id = os.environ.get("PROJECT_ID")
        endpoint_id = os.environ.get("ENDPOINT_ID")
        api_key = os.environ.get("API_KEY")
        # api_key = os.environ.get("SAMBAVERSE_API_KEY")
        # print(f"client api_key: {api_key}", flush=True)

        ttft = 0
        generated_text = ""
        total_request_time = 0
        error_msg = ""
        error_response_code = ""

        metrics = {}
        metrics[common_metrics.ERROR_CODE] = None
        metrics[common_metrics.ERROR_MSG] = ""

        try:
            # Define the URL for the request
            url = self._get_url(request_config, base_url, project_id, endpoint_id)

            # Define the headers
            headers = {"key": api_key}

            # Get data
            data = self._get_data(request_config)

            prompt_len = self.get_token_length(request_config.prompt[0])
            # Make the POST request
            if request_config.mode == "stream":
                total_request_time, ttft, generated_text, output_data = (
                    self._get_total_request_time_and_ttft(
                        url, headers, data, stream=True
                    )
                )
                metrics = self.populate_server_metrics(output_data, metrics)

        except Exception as e:
            metrics[common_metrics.ERROR_MSG] = error_msg
            metrics[common_metrics.ERROR_CODE] = error_response_code
            print(f"Warning Or Error: {e}")

        num_output_tokens = self.get_token_length(generated_text)
        metrics[common_metrics.NUM_INPUT_TOKENS] = (
            prompt_len
            if metrics[common_metrics.NUM_INPUT_TOKENS_SERVER] is None
            else metrics[common_metrics.NUM_INPUT_TOKENS_SERVER]
        )
        metrics[common_metrics.NUM_OUTPUT_TOKENS] = (
            num_output_tokens
            if metrics[common_metrics.NUM_OUTPUT_TOKENS_SERVER] is None
            else metrics[common_metrics.NUM_OUTPUT_TOKENS_SERVER]
        )
        metrics[common_metrics.NUM_TOTAL_TOKENS] = (
            prompt_len + num_output_tokens
            if metrics[common_metrics.NUM_TOTAL_TOKENS_SERVER] is None
            else metrics[common_metrics.NUM_TOTAL_TOKENS_SERVER]
        )
        metrics[common_metrics.TTFT] = ttft
        metrics[common_metrics.E2E_LAT] = total_request_time

        metrics[common_metrics.REQ_OUTPUT_THROUGHPUT] = (
            metrics[common_metrics.NUM_OUTPUT_TOKENS] / (total_request_time - ttft)
            if not isclose(ttft, total_request_time, abs_tol=1e-8)
            else None
        )
        return metrics, generated_text, request_config

    def _get_url(
        self,
        request_config: RequestConfig,
        base_url: str,
        project_id: str,
        endpoint_id: str,
    ) -> str:
        """Builds url for API

        Args:
            request_config (RequestConfig): config options with LLM mode
            base_url (str): base url for API
            project_id (str): project ID
            endpoint_id (str): endpoint ID

        Returns:
            str: url needed for API
        """

        if request_config.mode == "stream":
            path = f"/predict/generic/stream/{project_id}/{endpoint_id}"
        else:
            path = f"/predict/generic/{project_id}/{endpoint_id}"

        API_BASE_PATH = "/api"
        url = f"{base_url}{API_BASE_PATH}{path}"
        return url

    def _get_data(self, request_config: RequestConfig) -> dict:
        """Gets data structure needed for API based on request_config

        Args:
            request_config (RequestConfig): contains LLM params

        Returns:
            dict: data structure needed for API
        """

        prompt = request_config.prompt
        prompt, _ = prompt
        # if isinstance(prompt, str):
        #     prompt = [prompt]
        sampling_params = request_config.sampling_params
        if "COE" in request_config.model:
            sampling_params["select_expert"] = request_config.model.split("/")[-1]
            sampling_params["process_prompt"] = False
        tuning_params_dict = {
            k: {"type": type(v).__name__, "value": str(v)}
            for k, v in (sampling_params.items())
        }
        tuning_params = json.dumps(tuning_params_dict)
        if request_config.mode == "stream":
            data = {"instance": prompt, "params": json.loads(tuning_params)}
        else:
            data = {"instances": [prompt], "params": json.loads(tuning_params)}
        return data

    def _get_total_request_time_and_ttft(
        self, url: str, headers: str, input_data: dict, stream: bool
    ) -> tuple:
        """Gets total time of a request

        Args:
            url (str): URL of the request
            headers (str): headers of the request
            input_data (dict): data of the request
            stream (bool): stream option

        Returns:
            tuple: tuple containing request time, LLM generated text and LLM params
        """

        start_time = chunk_start_time = time.monotonic()
        total_request_time = 0
        ttft = 0
        generated_text = ""
        chunks_received = []
        chunks_timings = []

        with requests.post(
            url, headers=headers, json=input_data, stream=stream
        ) as response:
            if response.status_code != 200:
                response.raise_for_status()
            for chunk_orig in response.iter_lines(chunk_size=None):
                chunk = chunk_orig.strip()
                data = json.loads(chunk)

                ##TODO: Non-streaming case
                if stream is False:
                    generated_text = data["predictions"][0]["completion"]
                    break

                completion = data["result"]["responses"][0]["is_last_response"]
                chunks_timings.append(time.monotonic() - chunk_start_time)
                chunk_start_time = time.monotonic()
                if completion is False:
                    chunks_received.append(
                        data["result"]["responses"][0]["stream_token"]
                    )
                    continue
                else:
                    generated_text = data["result"]["responses"][0]["completion"]
                    break
        total_request_time = time.monotonic() - start_time
        if len(chunks_received) <= 1:
            ttft = total_request_time
        else:
            total_tokens_received_after_first_chunk = sum(
                self.get_token_length(c) for c in chunks_received[1:]
            )
            total_time_to_receive_tokens_after_first_chunk = sum(chunks_timings[1:])
            total_tokens_in_first_chunk = self.get_token_length(chunks_received[0])
            tpot = (
                total_time_to_receive_tokens_after_first_chunk
                / total_tokens_received_after_first_chunk
            )
            ttft = chunks_timings[0] - (total_tokens_in_first_chunk - 1) * tpot

        return total_request_time, ttft, generated_text, data

    def populate_server_metrics(self, output_data: dict, metrics: dict) -> dict:
        """Parse output data to metrics dictionary structure

        Args:
            output_data (dict): output data with performance metrics
            metrics (dict): metrics dictionary

        Returns:
            dict: updated metrics dictionary
        """
        metrics[common_metrics.NUM_INPUT_TOKENS_SERVER] = output_data["result"][
            "responses"
        ][0].get("prompt_tokens_count")
        metrics[common_metrics.NUM_OUTPUT_TOKENS_SERVER] = output_data["result"][
            "responses"
        ][0].get("completion_tokens_count")
        metrics[common_metrics.NUM_TOTAL_TOKENS_SERVER] = output_data["result"][
            "responses"
        ][0].get("total_tokens_count")
        metrics[common_metrics.TTFT_SERVER] = output_data["result"]["responses"][0].get(
            "time_to_first_token"
        )
        metrics[common_metrics.E2E_LAT_SERVER] = output_data["result"]["responses"][
            0
        ].get("total_latency")
        metrics[common_metrics.REQ_OUTPUT_THROUGHPUT_SERVER] = output_data["result"][
            "responses"
        ][0].get("completion_tokens_after_first_per_sec")
        # metrics[common_metrics.REQ_OUTPUT_THROUGHPUT_AFTER_FIRST_SERVER] = output_data[
        #     "result"
        # ]["responses"][0].get("completion_tokens_after_first_per_sec")
        return metrics


if __name__ == "__main__":

    # load env variables
    load_dotenv("../.env", override=True)
    env_vars = dict(os.environ)

    # init ray
    ray.init(
        local_mode=True,
        runtime_env={"env_vars": env_vars},
        log_to_driver=True,
        logging_level=logging.ERROR,
    )

    # prompt = "Short test."
    prompt = "This is a test example, so tell me about anything"
    client = SambaNovaLLMClient.remote()
    request_config = RequestConfig(
        prompt=(prompt, 10),
        # model="COE/Meta-Llama-3-8B-Instruct",
        model="COE/llama-2-7b-chat-hf",
        # model="llama-2-7b-chat-hf",
        # model="Llama-7B-DynamicBatching",
        sampling_params={
            # "do_sample": False,
            "max_tokens_to_generate": 250,
            # "top_k": 40,
            # "top_p": 0.95,
            # "process_prompt": "False",
        },
        mode="stream",
        llm_api="sambastudio",
        num_concurrent_requests=1,
    )

    metrics, generated_text, request_config = ray.get(
        client.llm_request.remote(request_config)
    )
    print(f"Metrics collected: {metrics}")
    # print(f'Completion text: {generated_text}')
    print(f"Request config: {request_config}")
