{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56950450",
   "metadata": {},
   "source": [
    "# Token Benchmark Example Analysis\n",
    "The following is an example of the analysis that can be done on individual responses that are saved when running `token_benchmark_ray.py` with the flag `--results-dir` which enables the saving of all responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfe98a-e81b-4089-9506-97a652993b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666608c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_metrics_df(valid_df):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    final_df[\"number_input_tokens\"] = valid_df[\"number_input_tokens\"]\n",
    "    final_df[\"number_output_tokens\"] = valid_df[\"number_output_tokens\"]\n",
    "    final_df[\"number_total_tokens\"]  = valid_df[\"number_total_tokens\"]\n",
    "    final_df[\"concurrent_user\"] = valid_df[\"concurrent_user\"]\n",
    "    \n",
    "    # server metrics\n",
    "    final_df[\"ttft_server_s\"] = valid_df[\"ttft_server_s\"]\n",
    "    final_df[\"end_to_end_latency_server_s\"] = valid_df[\"end_to_end_latency_server_s\"]\n",
    "    final_df[\"generation_throughput_server\"] = valid_df[\"request_output_throughput_server_token_per_s\"]\n",
    "    final_df[\"batch_size_used\"] = valid_df[\"batch_size_used\"]\n",
    "    final_df[\"total_tokens_per_sec_server\"] = valid_df[\"total_tokens_per_sec_server\"]\n",
    "    \n",
    "    # client metrics\n",
    "    final_df[\"ttft_s\"] = valid_df[\"ttft_s\"]\n",
    "    final_df[\"end_to_end_latency_s\"] = valid_df[\"end_to_end_latency_s\"]\n",
    "    final_df[\"generation_throughput\"] = valid_df[\"request_output_throughput_token_per_s\"]\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921bde7",
   "metadata": {},
   "source": [
    "# Metrics across concurrent workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_current_users = [1,4,8,16]\n",
    "mode='stream'\n",
    "num_current_users = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7abe9-ed9e-466c-b034-577489aaf98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to the individual responses json file\n",
    "df = pd.DataFrame()\n",
    "for concurrent_user in num_current_users:\n",
    "    df_user = pd.read_json(f'../data/results/llmperf/COE-llama-2-7b-chat-hf_1024_1024_{concurrent_user}_{mode}_individual_responses.json')\n",
    "    # df_user = pd.read_json(f'../data/results/llmperf/COE-llama-2-7b-chat-hf_150_150_{concurrent_user}_{mode}_individual_responses.json')\n",
    "    # df_user = pd.read_json(f'sambanova-Llama-2-7b-chat_150_150_{concurrent_user}_individual_responses.json')\n",
    "    df_user['concurrent_user'] = concurrent_user\n",
    "    df = pd.concat([df,df_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df[(df[\"error_code\"] != \"\")]\n",
    "final_df = get_final_metrics_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71430074",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,ncols=1,figsize=(8,12))\n",
    "sns.scatterplot(data=final_df, x=\"number_input_tokens\", y=\"ttft_s\", hue=\"concurrent_user\", ax=ax[0]).set_title(\"Number of Input Tokens vs. TTFT\")\n",
    "sns.scatterplot(data=final_df, x=\"number_output_tokens\", y=\"generation_throughput\", hue=\"concurrent_user\", ax=ax[1]).set_title(\"Number of output Tokens vs. Throughput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14de79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,ncols=1,figsize=(8,12))\n",
    "sns.boxplot(data=final_df, x=\"ttft_s\", hue=\"concurrent_user\", ax=ax[0])\n",
    "sns.boxplot(data=final_df, x=\"generation_throughput\", hue=\"concurrent_user\", ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8040dca",
   "metadata": {},
   "source": [
    "# Server vs client metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94343f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concurrent_user = 1\n",
    "mode = 'stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d648eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f'../data/results/llmperf/COE-llama-2-7b-chat-hf_1024_1024_{concurrent_user}_{mode}_individual_responses.json')\n",
    "df['concurrent_user'] = concurrent_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df[(df[\"error_code\"] != \"\")]\n",
    "final_df = get_final_metrics_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server = final_df[['ttft_server_s','number_input_tokens', 'number_total_tokens', 'generation_throughput_server','number_output_tokens', 'end_to_end_latency_server_s']].copy()\n",
    "df_server = df_server.rename(columns = {'ttft_server_s': 'ttft', 'generation_throughput_server': 'generation_throughput', 'end_to_end_latency_server_s': 'e2e_latency'})\n",
    "df_server['type'] = 'Server side'               \n",
    "\n",
    "df_client = final_df[['ttft_s','number_input_tokens', 'number_total_tokens', 'generation_throughput','number_output_tokens', 'end_to_end_latency_s']].copy()\n",
    "df_client = df_client.rename(columns = {'ttft_s': 'ttft', 'end_to_end_latency_s': 'e2e_latency'})\n",
    "df_client['type'] = 'Client side'               \n",
    "\n",
    "df_ttft_throughput_latency = pd.concat([df_server, df_client], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b48b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=3,ncols=1,figsize=(8,20))\n",
    "sns.scatterplot(data=df_ttft_throughput_latency, x=\"number_input_tokens\", y=\"ttft\", hue=\"type\", ax=ax[0], alpha=0.5).set_title(\"Number of Input Tokens vs. TTFT\")\n",
    "sns.scatterplot(data=df_ttft_throughput_latency, x=\"number_output_tokens\", y=\"generation_throughput\", hue=\"type\", ax=ax[1], alpha=0.5).set_title(\"Number of output Tokens vs. Throughput\");\n",
    "sns.scatterplot(data=df_ttft_throughput_latency, x=\"number_output_tokens\", y=\"e2e_latency\", hue=\"type\", ax=ax[2], alpha=0.5).set_title(\"Number of output tokens vs Latency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=3,ncols=1,figsize=(8,12))\n",
    "sns.boxplot(data=df_ttft_throughput_latency, x=\"ttft\", y=\"type\", ax=ax[0])\n",
    "sns.boxplot(data=df_ttft_throughput_latency, x=\"e2e_latency\", y=\"type\", ax=ax[1])\n",
    "sns.boxplot(data=df_ttft_throughput_latency, x=\"generation_throughput\", y=\"type\", ax=ax[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965ba7b",
   "metadata": {},
   "source": [
    "## Show multiple distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad85cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_distributions(models, data_path, input_tokens=1000, output_tokens=1000, num_concurrent_workers = 1, mode='stream'):\n",
    "    \n",
    "    fig = plt.figure(layout='constrained', figsize=(40, 25))    \n",
    "    subfigs = fig.subfigures(nrows=len(models))\n",
    "    \n",
    "    df_user = pd.read_json(f\"{data_path}{model.replace('/','-').replace('.','-')}_{input_tokens}_{output_tokens}_{num_concurrent_workers}_stream_individual_responses.json\")\n",
    "    df_user['concurrent_user'] = num_concurrent_workers\n",
    "    final_df = get_final_metrics_df(df_user)\n",
    "    \n",
    "    if mode == 'batch':\n",
    "        \n",
    "        for idx, model in enumerate(models):\n",
    "            \n",
    "            df_server = final_df[['ttft_server_s','number_input_tokens', 'number_total_tokens', 'generation_throughput_server','number_output_tokens', 'end_to_end_latency_server_s', 'batch_size_used', 'total_tokens_per_sec_server']].copy()\n",
    "            df_server = df_server.rename(columns = {'ttft_server_s': 'ttft', 'generation_throughput_server': 'generation_throughput', 'end_to_end_latency_server_s': 'e2e_latency', 'total_tokens_per_sec_server': 'total_tokens_per_sec'})\n",
    "            df_server['type'] = 'Server side'               \n",
    "\n",
    "            df_client = final_df[['ttft_s','number_input_tokens', 'number_total_tokens', 'generation_throughput','number_output_tokens', 'end_to_end_latency_s']].copy()\n",
    "            df_client = df_client.rename(columns = {'ttft_s': 'ttft', 'end_to_end_latency_s': 'e2e_latency'})\n",
    "            df_client['batch_size_used'] = None\n",
    "            df_client['total_tokens_per_sec'] = None\n",
    "            df_client['type'] = 'Client side'               \n",
    "\n",
    "            df_ttft_throughput_latency = pd.concat([df_server, df_client], ignore_index=True)\n",
    "            \n",
    "            ax = subfigs[idx].subplots(1, 6)\n",
    "            subfigs[idx].suptitle(f'{model}', fontsize='x-large')\n",
    "\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"number_input_tokens\", y=\"type\", ax=ax[0])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"number_output_tokens\", y=\"type\", ax=ax[1])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"ttft\", y=\"type\", ax=ax[2])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"generation_throughput\", y=\"type\", ax=ax[3])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"e2e_latency\", y=\"type\", ax=ax[4])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"total_tokens_per_sec\", y=\"type\", hue=\"batch_size_used\", ax=ax[5])\n",
    "\n",
    "    else:\n",
    "    \n",
    "        for idx, model in enumerate(models):\n",
    "            \n",
    "            df_server = final_df[['ttft_server_s','number_input_tokens', 'number_total_tokens', 'generation_throughput_server','number_output_tokens', 'end_to_end_latency_server_s']].copy()\n",
    "            df_server = df_server.rename(columns = {'ttft_server_s': 'ttft', 'generation_throughput_server': 'generation_throughput', 'end_to_end_latency_server_s': 'e2e_latency'})\n",
    "            df_server['type'] = 'Server side'               \n",
    "\n",
    "            df_client = final_df[['ttft_s','number_input_tokens', 'number_total_tokens', 'generation_throughput','number_output_tokens', 'end_to_end_latency_s']].copy()\n",
    "            df_client = df_client.rename(columns = {'ttft_s': 'ttft', 'end_to_end_latency_s': 'e2e_latency'})\n",
    "            df_client['type'] = 'Client side'               \n",
    "\n",
    "            df_ttft_throughput_latency = pd.concat([df_server, df_client], ignore_index=True)\n",
    "            \n",
    "            ax = subfigs[idx].subplots(1, 5)\n",
    "            subfigs[idx].suptitle(f'{model}', fontsize='x-large')\n",
    "\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"number_input_tokens\", y=\"type\", ax=ax[0])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"number_output_tokens\", y=\"type\", ax=ax[1])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"ttft\", y=\"type\", ax=ax[2])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"generation_throughput\", y=\"type\", ax=ax[3])\n",
    "            sns.boxplot(data=df_ttft_throughput_latency, x=\"e2e_latency\", y=\"type\", ax=ax[4])\n",
    "        \n",
    "    fig.suptitle(f\"No Output Tokens, TTFT, Throughput, E2E Latency distributions\", fontsize='xx-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['COE/Meta-Llama-3-8B-Instruct-dybs_v1','COE/Meta-Llama-3-8B-Instruct-dybs_v2','COE/Meta-Llama-3-8B-Instruct-dybs_v3']\n",
    "show_distributions(models,  data_path='../data/results/llmperf/debug_final_test/', input_tokens=1000, output_tokens=1000, num_concurrent_workers=50, mode='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783337bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['COE/Meta-Llama-3-8B-Instruct_v1','COE/Meta-Llama-3-8B-Instruct_v2','COE/Meta-Llama-3-8B-Instruct_v3']\n",
    "show_distributions(models,  data_path='../data/results/llmperf/debug_final_test/', input_tokens=1000, output_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c107f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['COE/llama-2-7b-chat-hf_v1','COE/llama-2-7b-chat-hf_v2','COE/llama-2-7b-chat-hf_v3']\n",
    "show_distributions(models,  data_path='../data/results/llmperf/debug_final_test/', input_tokens=1000, output_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ac909",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['COE/llama-2-13b-chat-hf_v1','COE/llama-2-13b-chat-hf_v2','COE/llama-2-13b-chat-hf_v3']\n",
    "show_distributions(models,  data_path='../data/results/llmperf/debug_final_test/', input_tokens=1000, output_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['COE/Mistral-7B-Instruct-v0-2_v1','COE/Mistral-7B-Instruct-v0-2_v2','COE/Mistral-7B-Instruct-v0-2_v3']\n",
    "show_distributions(models,  data_path='../data/results/llmperf/debug_final_test/', input_tokens=1000, output_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b40a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4abed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
